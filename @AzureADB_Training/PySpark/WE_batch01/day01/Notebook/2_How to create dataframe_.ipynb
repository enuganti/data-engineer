{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "153d82f0-345b-43bc-a177-8caa99c1c28d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**1) Create a DataFrame from a List of Tuples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0b61317-44b3-4ce8-ab38-fff0f85937b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method createDataFrame in module pyspark.sql.connect.session:\n\ncreateDataFrame(data: Union[ForwardRef('pd.DataFrame'), ForwardRef('np.ndarray'), ForwardRef('pa.Table'), Iterable[Any]], schema: Union[pyspark.sql.types.AtomicType, pyspark.sql.types.StructType, str, List[str], Tuple[str, ...], NoneType] = None, samplingRatio: Optional[float] = None, verifySchema: Optional[bool] = None) -> 'ParentDataFrame' method of pyspark.sql.connect.session.SparkSession instance\n    Creates a :class:`DataFrame` from an :class:`RDD`, a list, a :class:`pandas.DataFrame`,\n    a :class:`numpy.ndarray`, or a :class:`pyarrow.Table`.\n\n    .. versionadded:: 2.0.0\n\n    .. versionchanged:: 3.4.0\n        Supports Spark Connect.\n\n    .. versionchanged:: 4.0.0\n        Supports :class:`pyarrow.Table`.\n\n    Parameters\n    ----------\n    data : :class:`RDD` or iterable\n        an RDD of any kind of SQL data representation (:class:`Row`,\n        :class:`tuple`, ``int``, ``boolean``, ``dict``, etc.), or :class:`list`,\n        :class:`pandas.DataFrame`, :class:`numpy.ndarray`, or :class:`pyarrow.Table`.\n    schema : :class:`pyspark.sql.types.DataType`, str or list, optional\n        a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n        column names, default is None. The data type string format equals to\n        :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n        omit the ``struct<>``.\n\n        When ``schema`` is a list of column names, the type of each column\n        will be inferred from ``data``.\n\n        When ``schema`` is ``None``, it will try to infer the schema (column names and types)\n        from ``data``, which should be an RDD of either :class:`Row`,\n        :class:`namedtuple`, or :class:`dict`.\n\n        When ``schema`` is :class:`pyspark.sql.types.DataType` or a datatype string, it must\n        match the real data, or an exception will be thrown at runtime. If the given schema is\n        not :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n        :class:`pyspark.sql.types.StructType` as its only field, and the field name will be\n        \"value\". Each record will also be wrapped into a tuple, which can be converted to row\n        later.\n    samplingRatio : float, optional\n        the sample ratio of rows used for inferring. The first few rows will be used\n        if ``samplingRatio`` is ``None``. This option is effective only when the input is\n        :class:`RDD`.\n    verifySchema : bool, optional\n        verify data types of every row against schema. Enabled by default.\n        When the input is :class:`pyarrow.Table` or when the input class is\n        :class:`pandas.DataFrame` and `spark.sql.execution.arrow.pyspark.enabled` is enabled,\n        this option is not effective. It follows Arrow type coercion. This option is not\n        supported with Spark Connect.\n\n        .. versionadded:: 2.1.0\n\n    Returns\n    -------\n    :class:`DataFrame`\n\n    Notes\n    -----\n    Usage with `spark.sql.execution.arrow.pyspark.enabled=True` is experimental.\n\n    Examples\n    --------\n    Create a DataFrame from a list of tuples.\n\n    >>> spark.createDataFrame([('Alice', 1)]).show()\n    +-----+---+\n    |   _1| _2|\n    +-----+---+\n    |Alice|  1|\n    +-----+---+\n\n    Create a DataFrame from a list of dictionaries.\n\n    >>> d = [{'name': 'Alice', 'age': 1}]\n    >>> spark.createDataFrame(d).show()\n    +---+-----+\n    |age| name|\n    +---+-----+\n    |  1|Alice|\n    +---+-----+\n\n    Create a DataFrame with column names specified.\n\n    >>> spark.createDataFrame([('Alice', 1)], ['name', 'age']).show()\n    +-----+---+\n    | name|age|\n    +-----+---+\n    |Alice|  1|\n    +-----+---+\n\n    Create a DataFrame with the explicit schema specified.\n\n    >>> from pyspark.sql.types import *\n    >>> schema = StructType([\n    ...    StructField(\"name\", StringType(), True),\n    ...    StructField(\"age\", IntegerType(), True)])\n    >>> spark.createDataFrame([('Alice', 1)], schema).show()\n    +-----+---+\n    | name|age|\n    +-----+---+\n    |Alice|  1|\n    +-----+---+\n\n    Create a DataFrame with the schema in DDL formatted string.\n\n    >>> spark.createDataFrame([('Alice', 1)], \"name: string, age: int\").show()\n    +-----+---+\n    | name|age|\n    +-----+---+\n    |Alice|  1|\n    +-----+---+\n\n    Create an empty DataFrame.\n    When initializing an empty DataFrame in PySpark, it's mandatory to specify its schema,\n    as the DataFrame lacks data from which the schema can be inferred.\n\n    >>> spark.createDataFrame([], \"name: string, age: int\").show()\n    +----+---+\n    |name|age|\n    +----+---+\n    +----+---+\n\n    Create a DataFrame from Row objects.\n\n    >>> from pyspark.sql import Row\n    >>> Person = Row('name', 'age')\n    >>> df = spark.createDataFrame([Person(\"Alice\", 1)])\n    >>> df.show()\n    +-----+---+\n    | name|age|\n    +-----+---+\n    |Alice|  1|\n    +-----+---+\n\n    Create a DataFrame from a pandas DataFrame.\n\n    >>> spark.createDataFrame(df.toPandas()).show()  # doctest: +SKIP\n    +-----+---+\n    | name|age|\n    +-----+---+\n    |Alice|  1|\n    +-----+---+\n\n    >>> pdf = pandas.DataFrame([[1, 2]])  # doctest: +SKIP\n    >>> spark.createDataFrame(pdf).show()  # doctest: +SKIP\n    +---+---+\n    |  0|  1|\n    +---+---+\n    |  1|  2|\n    +---+---+\n\n    Create a DataFrame from a PyArrow Table.\n\n    >>> spark.createDataFrame(df.toArrow()).show()  # doctest: +SKIP\n    +-----+---+\n    | name|age|\n    +-----+---+\n    |Alice|  1|\n    +-----+---+\n\n    >>> table = pyarrow.table({'0': [1], '1': [2]})  # doctest: +SKIP\n    >>> spark.createDataFrame(table).show()  # doctest: +SKIP\n    +---+---+\n    |  0|  1|\n    +---+---+\n    |  1|  2|\n    +---+---+\n\n"
     ]
    }
   ],
   "source": [
    "help(spark.createDataFrame)\n",
    "# spark.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0059b1c1-c968-4cb5-b884-f5eb6e7aec6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+----------+----------+\n| id|  name|salary|     phone|       dob|\n+---+------+------+----------+----------+\n|  1|Naresh|  10.5|9980133778|2025-01-03|\n|  2|Harish|  12.5|9980133778|2024-01-03|\n|  3|Harish|  12.5|9980133778|2024-01-03|\n+---+------+------+----------+----------+\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>salary</th><th>phone</th><th>dob</th></tr></thead><tbody><tr><td>1</td><td>Naresh</td><td>10.5</td><td>9980133778</td><td>2025-01-03</td></tr><tr><td>2</td><td>Harish</td><td>12.5</td><td>9980133778</td><td>2024-01-03</td></tr><tr><td>3</td><td>Harish</td><td>12.5</td><td>9980133778</td><td>2024-01-03</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Naresh",
         10.5,
         9980133778,
         "2025-01-03"
        ],
        [
         2,
         "Harish",
         12.5,
         9980133778,
         "2024-01-03"
        ],
        [
         3,
         "Harish",
         12.5,
         9980133778,
         "2024-01-03"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "phone",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "dob",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- salary: double (nullable = true)\n |-- phone: long (nullable = true)\n |-- dob: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "data =[(1, \"Naresh\", 10.5, 9980133778, \"2025-01-03\"),\n",
    "       (2, \"Harish\", 12.5, 9980133778, \"2024-01-03\"),\n",
    "       (3, \"Harish\", 12.5, 9980133778, \"2024-01-03\")]\n",
    "       \n",
    "schema = [\"id\", \"name\", \"salary\", \"phone\", \"dob\"]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()\n",
    "display(df)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "974fb4a5-4314-4973-99d0-9d343379034f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Spark automatically **infers data types**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78858143-046a-46e9-aa32-5ec88185b618",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+----------+----------+\n| id|  name|salary|     phone|       dob|\n+---+------+------+----------+----------+\n|  1|Naresh|  10.5|9980133778|2025-01-03|\n|  2|Harish|  12.5|9980133778|2024-01-03|\n|  3|Harish|  12.5|9980133778|2024-01-03|\n|  3|Harish|  data|9980133778|2024-01-03|\n+---+------+------+----------+----------+\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>salary</th><th>phone</th><th>dob</th></tr></thead><tbody><tr><td>1</td><td>Naresh</td><td>10.5</td><td>9980133778</td><td>2025-01-03</td></tr><tr><td>2</td><td>Harish</td><td>12.5</td><td>9980133778</td><td>2024-01-03</td></tr><tr><td>3</td><td>Harish</td><td>12.5</td><td>9980133778</td><td>2024-01-03</td></tr><tr><td>3</td><td>Harish</td><td>data</td><td>9980133778</td><td>2024-01-03</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Naresh",
         "10.5",
         9980133778,
         "2025-01-03"
        ],
        [
         2,
         "Harish",
         "12.5",
         9980133778,
         "2024-01-03"
        ],
        [
         3,
         "Harish",
         "12.5",
         9980133778,
         "2024-01-03"
        ],
        [
         3,
         "Harish",
         "data",
         9980133778,
         "2024-01-03"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "phone",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "dob",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- salary: string (nullable = true)\n |-- phone: long (nullable = true)\n |-- dob: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "data =[(1, \"Naresh\", 10.5, 9980133778, \"2025-01-03\"),\n",
    "       (2, \"Harish\", 12.5, 9980133778, \"2024-01-03\"),\n",
    "       (3, \"Harish\", 12.5, 9980133778, \"2024-01-03\"),\n",
    "       (3, \"Harish\", \"data\", 9980133778, \"2024-01-03\")]\n",
    "       \n",
    "schema = [\"id\", \"name\", \"salary\", \"phone\", \"dob\"]\n",
    "\n",
    "df1 = spark.createDataFrame(data, schema)\n",
    "df1.show()\n",
    "display(df1)\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eca26190-9145-4bcf-afc9-c10dfd55a9d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+----------+----------+\n| id|  name|salary|     phone|       dob|\n+---+------+------+----------+----------+\n|  1|Naresh|  10.5|9980133778|2025-01-03|\n|  2|Harish|  12.5|9980133778|2024-01-03|\n|  3|Harish|  12.5|9980133778|2024-01-03|\n+---+------+------+----------+----------+\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>salary</th><th>phone</th><th>dob</th></tr></thead><tbody><tr><td>1</td><td>Naresh</td><td>10.5</td><td>9980133778</td><td>2025-01-03</td></tr><tr><td>2</td><td>Harish</td><td>12.5</td><td>9980133778</td><td>2024-01-03</td></tr><tr><td>3</td><td>Harish</td><td>12.5</td><td>9980133778</td><td>2024-01-03</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Naresh",
         10.5,
         9980133778,
         "2025-01-03"
        ],
        [
         2,
         "Harish",
         12.5,
         9980133778,
         "2024-01-03"
        ],
        [
         3,
         "Harish",
         12.5,
         9980133778,
         "2024-01-03"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "phone",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "dob",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: integer (nullable = false)\n |-- name: string (nullable = true)\n |-- salary: double (nullable = true)\n |-- phone: long (nullable = true)\n |-- dob: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, DoubleType, LongType\n",
    "\n",
    "data =[(1, \"Naresh\", 10.5, 9980133778, \"2025-01-03\"),\n",
    "       (2, \"Harish\", 12.5, 9980133778, \"2024-01-03\"),\n",
    "       (3, \"Harish\", 12.5, 9980133778, \"2024-01-03\")]\n",
    "       \n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"salary\", DoubleType(), True),\n",
    "    StructField(\"phone\", LongType(), True),\n",
    "    StructField(\"dob\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()\n",
    "display(df)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "304e6467-5c6a-46e3-9eed-3a860cef82af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**2) Create a DataFrame from a List of Lists**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76cf1f28-35ba-4af1-b159-e1d9e32498cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+----------+----------+\n| id|  name|salary|     phone|       dob|\n+---+------+------+----------+----------+\n|  1|Naresh|  10.5|9980133778|2025-01-03|\n|  2|Harish|  12.5|9980133778|2024-01-03|\n|  3|Harish|  12.5|9980133778|2024-01-03|\n+---+------+------+----------+----------+\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>salary</th><th>phone</th><th>dob</th></tr></thead><tbody><tr><td>1</td><td>Naresh</td><td>10.5</td><td>9980133778</td><td>2025-01-03</td></tr><tr><td>2</td><td>Harish</td><td>12.5</td><td>9980133778</td><td>2024-01-03</td></tr><tr><td>3</td><td>Harish</td><td>12.5</td><td>9980133778</td><td>2024-01-03</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Naresh",
         10.5,
         9980133778,
         "2025-01-03"
        ],
        [
         2,
         "Harish",
         12.5,
         9980133778,
         "2024-01-03"
        ],
        [
         3,
         "Harish",
         12.5,
         9980133778,
         "2024-01-03"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "phone",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "dob",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: integer (nullable = false)\n |-- name: string (nullable = true)\n |-- salary: double (nullable = true)\n |-- phone: long (nullable = true)\n |-- dob: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, DoubleType, LongType\n",
    "\n",
    "data =[[1, \"Naresh\", 10.5, 9980133778, \"2025-01-03\"],\n",
    "       [2, \"Harish\", 12.5, 9980133778, \"2024-01-03\"],\n",
    "       [3, \"Harish\", 12.5, 9980133778, \"2024-01-03\"]]\n",
    "       \n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"salary\", DoubleType(), True),\n",
    "    StructField(\"phone\", LongType(), True),\n",
    "    StructField(\"dob\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_list_list = spark.createDataFrame(data, schema)\n",
    "df_list_list.show()\n",
    "display(df_list_list)\n",
    "df_list_list.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9517b10-d3f7-4f2d-ae00-ff3b3ef1a6b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**3) Create a DataFrame using dictionary**\n",
    "- Useful when data comes as **JSON-like objects**.\n",
    "- **Column order** is **not guaranteed**.\n",
    "\n",
    "- Syntax: \n",
    "  - spark.createDataFrame(data)\n",
    "  - **Schema** is **inferred automatically**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48155407-21c0-48d9-b17e-71b9a141634b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>dob</th><th>id</th><th>name</th><th>phone</th><th>salary</th></tr></thead><tbody><tr><td>2025-01-03</td><td>1</td><td>John</td><td>9980133778</td><td>10.5</td></tr><tr><td>2024-01-03</td><td>2</td><td>Jane</td><td>9980133778</td><td>12.5</td></tr><tr><td>2024-01-03</td><td>3</td><td>Jane</td><td>9980133778</td><td>12.5</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2025-01-03",
         1,
         "John",
         9980133778,
         10.5
        ],
        [
         "2024-01-03",
         2,
         "Jane",
         9980133778,
         12.5
        ],
        [
         "2024-01-03",
         3,
         "Jane",
         9980133778,
         12.5
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "dob",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "phone",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [\n",
    "    {'id': 1, 'name': 'John', 'salary': 10.5, 'phone': 9980133778, 'dob': '2025-01-03'},\n",
    "    {'id': 2, 'name': 'Jane', 'salary': 12.5, 'phone': 9980133778, 'dob': '2024-01-03'},\n",
    "    {'id': 3, 'name': 'Jane', 'salary': 12.5, 'phone': 9980133778, 'dob': '2024-01-03'}\n",
    "]\n",
    "\n",
    "df_dict = spark.createDataFrame(data)\n",
    "display(df_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d77b9c36-088f-4402-89cb-f23268077533",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**4) Create a DataFrame with an Explicit Schema**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a447a4e-e4e5-412d-b285-bf66f0ee019b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+----------+----------+\n| id|  name|salary|     phone|       dob|\n+---+------+------+----------+----------+\n|  1|Naresh|  10.5|9980133778|2025-01-03|\n|  2|Harish|  12.5|9980133778|2024-01-03|\n|  3|Harish|  12.5|9980133778|2024-01-03|\n+---+------+------+----------+----------+\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>salary</th><th>phone</th><th>dob</th></tr></thead><tbody><tr><td>1</td><td>Naresh</td><td>10.5</td><td>9980133778</td><td>2025-01-03</td></tr><tr><td>2</td><td>Harish</td><td>12.5</td><td>9980133778</td><td>2024-01-03</td></tr><tr><td>3</td><td>Harish</td><td>12.5</td><td>9980133778</td><td>2024-01-03</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Naresh",
         10.5,
         9980133778,
         "2025-01-03"
        ],
        [
         2,
         "Harish",
         12.5,
         9980133778,
         "2024-01-03"
        ],
        [
         3,
         "Harish",
         12.5,
         9980133778,
         "2024-01-03"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "phone",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "dob",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: integer (nullable = false)\n |-- name: string (nullable = true)\n |-- salary: double (nullable = true)\n |-- phone: long (nullable = true)\n |-- dob: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, DoubleType, LongType\n",
    "\n",
    "data =[[1, \"Naresh\", 10.5, 9980133778, \"2025-01-03\"],\n",
    "       [2, \"Harish\", 12.5, 9980133778, \"2024-01-03\"],\n",
    "       [3, \"Harish\", 12.5, 9980133778, \"2024-01-03\"]]\n",
    "       \n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"salary\", DoubleType(), True),\n",
    "    StructField(\"phone\", LongType(), True),\n",
    "    StructField(\"dob\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_list_list = spark.createDataFrame(data, schema)\n",
    "df_list_list.show()\n",
    "display(df_list_list)\n",
    "df_list_list.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0fc8497-6b32-40d6-bf90-7c0c3765ea7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**5) How to create dataframe with NULL values?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62fb2e9a-7c97-48a8-9155-912af270b951",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+----------+----------+\n| id|  name|salary|     phone|       dob|\n+---+------+------+----------+----------+\n|  1|Naresh|  10.5|9980133778|2025-01-03|\n|  2|Harish|  NULL|9980133778|2024-01-03|\n|  3|Harish|  12.5|      NULL|2024-01-03|\n|  3|Harish|  12.5|9980133778|2024-01-03|\n|  3|Harish|  12.5|9980133778|      NULL|\n|  3|  NULL|  12.5|9980133778|2024-01-03|\n|  3|Harish|  12.5|9980133778|2024-01-03|\n+---+------+------+----------+----------+\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>salary</th><th>phone</th><th>dob</th></tr></thead><tbody><tr><td>1</td><td>Naresh</td><td>10.5</td><td>9980133778</td><td>2025-01-03</td></tr><tr><td>2</td><td>Harish</td><td>null</td><td>9980133778</td><td>2024-01-03</td></tr><tr><td>3</td><td>Harish</td><td>12.5</td><td>null</td><td>2024-01-03</td></tr><tr><td>3</td><td>Harish</td><td>12.5</td><td>9980133778</td><td>2024-01-03</td></tr><tr><td>3</td><td>Harish</td><td>12.5</td><td>9980133778</td><td>null</td></tr><tr><td>3</td><td>null</td><td>12.5</td><td>9980133778</td><td>2024-01-03</td></tr><tr><td>3</td><td>Harish</td><td>12.5</td><td>9980133778</td><td>2024-01-03</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Naresh",
         10.5,
         9980133778,
         "2025-01-03"
        ],
        [
         2,
         "Harish",
         null,
         9980133778,
         "2024-01-03"
        ],
        [
         3,
         "Harish",
         12.5,
         null,
         "2024-01-03"
        ],
        [
         3,
         "Harish",
         12.5,
         9980133778,
         "2024-01-03"
        ],
        [
         3,
         "Harish",
         12.5,
         9980133778,
         null
        ],
        [
         3,
         null,
         12.5,
         9980133778,
         "2024-01-03"
        ],
        [
         3,
         "Harish",
         12.5,
         9980133778,
         "2024-01-03"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "phone",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "dob",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: integer (nullable = false)\n |-- name: string (nullable = true)\n |-- salary: double (nullable = true)\n |-- phone: long (nullable = true)\n |-- dob: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, DoubleType, LongType\n",
    "\n",
    "data =[[1, \"Naresh\", 10.5, 9980133778, \"2025-01-03\"],\n",
    "       [2, \"Harish\", None, 9980133778, \"2024-01-03\"],\n",
    "       [3, \"Harish\", 12.5, None, \"2024-01-03\"],\n",
    "       [3, \"Harish\", 12.5, 9980133778, \"2024-01-03\"],\n",
    "       [3, \"Harish\", 12.5, 9980133778, None],\n",
    "       [3, None, 12.5, 9980133778, \"2024-01-03\"],\n",
    "       [3, \"Harish\", 12.5, 9980133778, \"2024-01-03\"]]\n",
    "       \n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"salary\", DoubleType(), True),\n",
    "    StructField(\"phone\", LongType(), True),\n",
    "    StructField(\"dob\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_null = spark.createDataFrame(data, schema)\n",
    "df_null.show()\n",
    "display(df_null)\n",
    "df_null.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a62829a6-833f-403c-9035-d67b3e5a2ce6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPySparkValueError\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-6167684495832718>, line 19\u001B[0m\n",
       "\u001B[1;32m      3\u001B[0m data \u001B[38;5;241m=\u001B[39m[[\u001B[38;5;241m1\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNaresh\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m10.5\u001B[39m, \u001B[38;5;241m9980133778\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m2025-01-03\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n",
       "\u001B[1;32m      4\u001B[0m        [\u001B[38;5;241m2\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHarish\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m9980133778\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m2024-01-03\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n",
       "\u001B[1;32m      5\u001B[0m        [\u001B[38;5;241m3\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHarish\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m12.5\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m2024-01-03\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m      8\u001B[0m        [\u001B[38;5;241m3\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m12.5\u001B[39m, \u001B[38;5;241m9980133778\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m2024-01-03\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n",
       "\u001B[1;32m      9\u001B[0m        [\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHarish\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m12.5\u001B[39m, \u001B[38;5;241m9980133778\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m2024-01-03\u001B[39m\u001B[38;5;124m\"\u001B[39m]]\n",
       "\u001B[1;32m     11\u001B[0m schema \u001B[38;5;241m=\u001B[39m StructType([\n",
       "\u001B[1;32m     12\u001B[0m     StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m\"\u001B[39m, IntegerType(), \u001B[38;5;28;01mFalse\u001B[39;00m),\n",
       "\u001B[1;32m     13\u001B[0m     StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m\"\u001B[39m, StringType(), \u001B[38;5;28;01mFalse\u001B[39;00m),\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m     16\u001B[0m     StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdob\u001B[39m\u001B[38;5;124m\"\u001B[39m, StringType(), \u001B[38;5;28;01mTrue\u001B[39;00m)\n",
       "\u001B[1;32m     17\u001B[0m ])\n",
       "\u001B[0;32m---> 19\u001B[0m df_null_01 \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mcreateDataFrame(data, schema)\n",
       "\u001B[1;32m     20\u001B[0m df_null_01\u001B[38;5;241m.\u001B[39mshow()\n",
       "\u001B[1;32m     21\u001B[0m display(df_null_01)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/session.py:813\u001B[0m, in \u001B[0;36mSparkSession.createDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n",
       "\u001B[1;32m    806\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconversion\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n",
       "\u001B[1;32m    807\u001B[0m         LocalDataToArrowConversion,\n",
       "\u001B[1;32m    808\u001B[0m     )\n",
       "\u001B[1;32m    810\u001B[0m     \u001B[38;5;66;03m# Spark Connect will try its best to build the Arrow table with the\u001B[39;00m\n",
       "\u001B[1;32m    811\u001B[0m     \u001B[38;5;66;03m# inferred schema in the client side, and then rename the columns and\u001B[39;00m\n",
       "\u001B[1;32m    812\u001B[0m     \u001B[38;5;66;03m# cast the datatypes in the server side.\u001B[39;00m\n",
       "\u001B[0;32m--> 813\u001B[0m     _table \u001B[38;5;241m=\u001B[39m LocalDataToArrowConversion\u001B[38;5;241m.\u001B[39mconvert(_data, _schema, prefers_large_types)\n",
       "\u001B[1;32m    815\u001B[0m \u001B[38;5;66;03m# TODO: Beside the validation on number of columns, we should also check\u001B[39;00m\n",
       "\u001B[1;32m    816\u001B[0m \u001B[38;5;66;03m# whether the Arrow Schema is compatible with the user provided Schema.\u001B[39;00m\n",
       "\u001B[1;32m    817\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _num_cols \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m _num_cols \u001B[38;5;241m!=\u001B[39m _table\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m]:\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/conversion.py:472\u001B[0m, in \u001B[0;36mLocalDataToArrowConversion.convert\u001B[0;34m(data, schema, use_large_var_types)\u001B[0m\n",
       "\u001B[1;32m    463\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m len_column_names \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
       "\u001B[1;32m    464\u001B[0m     column_convs \u001B[38;5;241m=\u001B[39m [\n",
       "\u001B[1;32m    465\u001B[0m         LocalDataToArrowConversion\u001B[38;5;241m.\u001B[39m_create_converter(\n",
       "\u001B[1;32m    466\u001B[0m             field\u001B[38;5;241m.\u001B[39mdataType, field\u001B[38;5;241m.\u001B[39mnullable, none_on_identity\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m    467\u001B[0m         )\n",
       "\u001B[1;32m    468\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m field \u001B[38;5;129;01min\u001B[39;00m schema\u001B[38;5;241m.\u001B[39mfields\n",
       "\u001B[1;32m    469\u001B[0m     ]\n",
       "\u001B[1;32m    471\u001B[0m     pylist \u001B[38;5;241m=\u001B[39m [\n",
       "\u001B[0;32m--> 472\u001B[0m         [conv(row[i]) \u001B[38;5;28;01mfor\u001B[39;00m row \u001B[38;5;129;01min\u001B[39;00m rows] \u001B[38;5;28;01mif\u001B[39;00m conv \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m [row[i] \u001B[38;5;28;01mfor\u001B[39;00m row \u001B[38;5;129;01min\u001B[39;00m rows]\n",
       "\u001B[1;32m    473\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m i, conv \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(column_convs)\n",
       "\u001B[1;32m    474\u001B[0m     ]\n",
       "\u001B[1;32m    476\u001B[0m     pa_schema \u001B[38;5;241m=\u001B[39m to_arrow_schema(\n",
       "\u001B[1;32m    477\u001B[0m         StructType(\n",
       "\u001B[1;32m    478\u001B[0m             [\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    485\u001B[0m         prefers_large_types\u001B[38;5;241m=\u001B[39muse_large_var_types,\n",
       "\u001B[1;32m    486\u001B[0m     )\n",
       "\u001B[1;32m    488\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m pa\u001B[38;5;241m.\u001B[39mTable\u001B[38;5;241m.\u001B[39mfrom_arrays(pylist, schema\u001B[38;5;241m=\u001B[39mpa_schema)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/conversion.py:408\u001B[0m, in \u001B[0;36mLocalDataToArrowConversion._create_converter.<locals>.convert_other\u001B[0;34m(value)\u001B[0m\n",
       "\u001B[1;32m    406\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mconvert_other\u001B[39m(value: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n",
       "\u001B[1;32m    407\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[0;32m--> 408\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m PySparkValueError(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdataType\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must not be None\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m    409\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m value\n",
       "\n",
       "\u001B[0;31mPySparkValueError\u001B[0m: input for IntegerType() must not be None"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "PySparkValueError",
        "evalue": "input for IntegerType() must not be None"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>PySparkValueError</span>: input for IntegerType() must not be None"
       },
       "removedWidgets": [],
       "sqlProps": {
        "breakingChangeInfo": null,
        "errorClass": null,
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "pysparkSummary": "",
        "sqlState": null,
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mPySparkValueError\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m<command-6167684495832718>, line 19\u001B[0m\n\u001B[1;32m      3\u001B[0m data \u001B[38;5;241m=\u001B[39m[[\u001B[38;5;241m1\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNaresh\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m10.5\u001B[39m, \u001B[38;5;241m9980133778\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m2025-01-03\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m      4\u001B[0m        [\u001B[38;5;241m2\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHarish\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m9980133778\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m2024-01-03\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m      5\u001B[0m        [\u001B[38;5;241m3\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHarish\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m12.5\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m2024-01-03\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      8\u001B[0m        [\u001B[38;5;241m3\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m12.5\u001B[39m, \u001B[38;5;241m9980133778\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m2024-01-03\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m      9\u001B[0m        [\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHarish\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m12.5\u001B[39m, \u001B[38;5;241m9980133778\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m2024-01-03\u001B[39m\u001B[38;5;124m\"\u001B[39m]]\n\u001B[1;32m     11\u001B[0m schema \u001B[38;5;241m=\u001B[39m StructType([\n\u001B[1;32m     12\u001B[0m     StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m\"\u001B[39m, IntegerType(), \u001B[38;5;28;01mFalse\u001B[39;00m),\n\u001B[1;32m     13\u001B[0m     StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m\"\u001B[39m, StringType(), \u001B[38;5;28;01mFalse\u001B[39;00m),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     16\u001B[0m     StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdob\u001B[39m\u001B[38;5;124m\"\u001B[39m, StringType(), \u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     17\u001B[0m ])\n\u001B[0;32m---> 19\u001B[0m df_null_01 \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mcreateDataFrame(data, schema)\n\u001B[1;32m     20\u001B[0m df_null_01\u001B[38;5;241m.\u001B[39mshow()\n\u001B[1;32m     21\u001B[0m display(df_null_01)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/session.py:813\u001B[0m, in \u001B[0;36mSparkSession.createDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m    806\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconversion\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m    807\u001B[0m         LocalDataToArrowConversion,\n\u001B[1;32m    808\u001B[0m     )\n\u001B[1;32m    810\u001B[0m     \u001B[38;5;66;03m# Spark Connect will try its best to build the Arrow table with the\u001B[39;00m\n\u001B[1;32m    811\u001B[0m     \u001B[38;5;66;03m# inferred schema in the client side, and then rename the columns and\u001B[39;00m\n\u001B[1;32m    812\u001B[0m     \u001B[38;5;66;03m# cast the datatypes in the server side.\u001B[39;00m\n\u001B[0;32m--> 813\u001B[0m     _table \u001B[38;5;241m=\u001B[39m LocalDataToArrowConversion\u001B[38;5;241m.\u001B[39mconvert(_data, _schema, prefers_large_types)\n\u001B[1;32m    815\u001B[0m \u001B[38;5;66;03m# TODO: Beside the validation on number of columns, we should also check\u001B[39;00m\n\u001B[1;32m    816\u001B[0m \u001B[38;5;66;03m# whether the Arrow Schema is compatible with the user provided Schema.\u001B[39;00m\n\u001B[1;32m    817\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _num_cols \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m _num_cols \u001B[38;5;241m!=\u001B[39m _table\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m]:\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/conversion.py:472\u001B[0m, in \u001B[0;36mLocalDataToArrowConversion.convert\u001B[0;34m(data, schema, use_large_var_types)\u001B[0m\n\u001B[1;32m    463\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m len_column_names \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    464\u001B[0m     column_convs \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    465\u001B[0m         LocalDataToArrowConversion\u001B[38;5;241m.\u001B[39m_create_converter(\n\u001B[1;32m    466\u001B[0m             field\u001B[38;5;241m.\u001B[39mdataType, field\u001B[38;5;241m.\u001B[39mnullable, none_on_identity\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    467\u001B[0m         )\n\u001B[1;32m    468\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m field \u001B[38;5;129;01min\u001B[39;00m schema\u001B[38;5;241m.\u001B[39mfields\n\u001B[1;32m    469\u001B[0m     ]\n\u001B[1;32m    471\u001B[0m     pylist \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m--> 472\u001B[0m         [conv(row[i]) \u001B[38;5;28;01mfor\u001B[39;00m row \u001B[38;5;129;01min\u001B[39;00m rows] \u001B[38;5;28;01mif\u001B[39;00m conv \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m [row[i] \u001B[38;5;28;01mfor\u001B[39;00m row \u001B[38;5;129;01min\u001B[39;00m rows]\n\u001B[1;32m    473\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m i, conv \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(column_convs)\n\u001B[1;32m    474\u001B[0m     ]\n\u001B[1;32m    476\u001B[0m     pa_schema \u001B[38;5;241m=\u001B[39m to_arrow_schema(\n\u001B[1;32m    477\u001B[0m         StructType(\n\u001B[1;32m    478\u001B[0m             [\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    485\u001B[0m         prefers_large_types\u001B[38;5;241m=\u001B[39muse_large_var_types,\n\u001B[1;32m    486\u001B[0m     )\n\u001B[1;32m    488\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m pa\u001B[38;5;241m.\u001B[39mTable\u001B[38;5;241m.\u001B[39mfrom_arrays(pylist, schema\u001B[38;5;241m=\u001B[39mpa_schema)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/conversion.py:408\u001B[0m, in \u001B[0;36mLocalDataToArrowConversion._create_converter.<locals>.convert_other\u001B[0;34m(value)\u001B[0m\n\u001B[1;32m    406\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mconvert_other\u001B[39m(value: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    407\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 408\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m PySparkValueError(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdataType\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must not be None\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    409\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m value\n",
        "\u001B[0;31mPySparkValueError\u001B[0m: input for IntegerType() must not be None"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, DoubleType, LongType\n",
    "\n",
    "data =[[1, \"Naresh\", 10.5, 9980133778, \"2025-01-03\"],\n",
    "       [2, \"Harish\", None, 9980133778, \"2024-01-03\"],\n",
    "       [3, \"Harish\", 12.5, None, \"2024-01-03\"],\n",
    "       [3, \"Harish\", 12.5, 9980133778, \"2024-01-03\"],\n",
    "       [3, \"Harish\", 12.5, 9980133778, None],\n",
    "       [3, None, 12.5, 9980133778, \"2024-01-03\"],\n",
    "       [None, \"Harish\", 12.5, 9980133778, \"2024-01-03\"]]\n",
    "       \n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"salary\", DoubleType(), True),\n",
    "    StructField(\"phone\", LongType(), True),\n",
    "    StructField(\"dob\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_null_01 = spark.createDataFrame(data, schema)\n",
    "df_null_01.show()\n",
    "display(df_null_01)\n",
    "df_null_01.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1d18a8f-3c0d-47f9-9386-1f1fabd0b8b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- df.show() Vs df.display()\n",
    "- df.printSchema()\n",
    "- df.columns\n",
    "- df.count()\n",
    "- df.schema\n",
    "- df.dtypes\n",
    "- dict(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f00dcfc7-aa94-4416-be54-ab1bacaeedea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['id', 'name', 'salary', 'phone', 'dob']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "434507f9-01cd-42f0-b193-c1c09fbd45b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5bb0d884-c1c0-460e-93fe-89ba0cf59d58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**6) Create Empty DataFrame without Schema (no columns)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30e26bd5-6131-4382-9df6-68f507c4f4f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df3 = spark.createDataFrame([], StructType([]))\n",
    "display(df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf526d69-c2fd-481f-a1ee-1f8b0bd56237",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**7) Create Empty DataFrame with Schema**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98a894bf-4222-4e5b-9456-e9e8c5f72d0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create Schema\n",
    "schema = StructType([\n",
    "  StructField('FirstName', StringType(), True),\n",
    "  StructField('LastName', IntegerType(), True),\n",
    "  StructField('Records', IntegerType(), True),\n",
    "  StructField('Product_Type', StringType(), True),\n",
    "  StructField('transaction_date', StringType(), True),\n",
    "  StructField('current_timestamp', StringType(), True),\n",
    "  StructField('Category', StringType(), True)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65bbd021-0b6a-4b3a-9669-62f02c86fc9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>FirstName</th><th>LastName</th><th>Records</th><th>Product_Type</th><th>transaction_date</th><th>current_timestamp</th><th>Category</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "FirstName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "LastName",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "Records",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "Product_Type",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "transaction_date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "current_timestamp",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Category",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create empty DataFrame directly.\n",
    "emp_df = spark.createDataFrame([], schema)\n",
    "display(emp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4970bb43-17e1-4049-9c6b-f9f18246d077",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**8) Creating empty DataFrame with NULL's**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fca65c4-ca68-4704-9a9b-42878d6e420c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>Name</th><th>Age</th><th>Sales</th></tr></thead><tbody><tr><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>null</td><td>null</td><td>null</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         null,
         null,
         null,
         null
        ],
        [
         null,
         null,
         null,
         null
        ],
        [
         null,
         null,
         null,
         null
        ],
        [
         null,
         null,
         null,
         null
        ],
        [
         null,
         null,
         null,
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Age",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "Sales",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Age\", IntegerType(), True),\n",
    "    StructField(\"Sales\", IntegerType(), True),\n",
    "])\n",
    "\n",
    "# Just placeholders with all nulls\n",
    "empty_data = [(None, None, None, None),\n",
    "              (None, None, None, None),\n",
    "              (None, None, None, None),\n",
    "              (None, None, None, None),\n",
    "              (None, None, None, None)]\n",
    "\n",
    "df_empty = spark.createDataFrame(empty_data, schema)\n",
    "# display(df_empty)\n",
    "df_empty.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1098865-054c-4572-aa72-fa8cca5a28d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_empty.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fedc063a-cd61-48ff-b663-6a718cf71b38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "StructType([StructField('id', IntegerType(), False), StructField('name', StringType(), True), StructField('salary', DoubleType(), True), StructField('phone', LongType(), True), StructField('dob', StringType(), True)])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b854c78-322e-4060-92d0-b81c74d4ad5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('id', 'int'),\n",
       " ('name', 'string'),\n",
       " ('salary', 'double'),\n",
       " ('phone', 'bigint'),\n",
       " ('dob', 'string')]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9245d4e-571a-4968-a971-0079d4227500",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'id': 'int',\n",
       " 'name': 'string',\n",
       " 'salary': 'double',\n",
       " 'phone': 'bigint',\n",
       " 'dob': 'string'}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(df.dtypes)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2_How to create dataframe?",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}