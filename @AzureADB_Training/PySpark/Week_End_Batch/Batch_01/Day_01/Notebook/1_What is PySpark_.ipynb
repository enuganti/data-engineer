{"cells":[{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"7dfbe313-d238-4a58-81ec-fbc0ee91f7c1","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"HJ2IhzybKnGd"},"source":["- **PySpark** means using **Apache Spark** with **Python**."]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"a38b329f-eaa7-4c30-a1b1-6c2a5d4a7690","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"8c51sm1YKnGg"},"source":["**1) What is Spark?**\n","\n","- Apache Spark is a **fast, distributed data processing engine**.\n","- It is used to:\n","  - Process **very large amounts of data**.\n","  - Written mainly in **Scala**.\n","  - Runs on a **cluster (multiple machines)**.\n","  - **Executes jobs in parallel**.\n","  - Do **data analysis, ETL, and machine learning**.\n","  - Spark is **much faster** than older tools like **Hadoop MapReduce** because it **works in memory**."]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"f539d5d7-9b5c-46ed-a146-18cb8ee7d0e0","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"WvNb0LV6KnGg"},"source":["**Spark (Scala)**\n","\n","     val df = spark.read.csv(\"data.csv\")\n","     df.filter($\"age\" > 25).show()"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"42c6800f-c694-4957-8a8e-a9bf55a60547","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"JjkakLcAKnGh"},"source":["**2) What is PySpark?**\n","- **PySpark = Python + Spark**\n","- It is the **Python API** for **Apache Spark**.\n","- Acts as a bridge **between Python and Spark**.\n","- It allows you to write **Spark** programs using **Python** instead of **Scala or Java**.\n","- PySpark is **not a separate engine**, it is an **interface**."]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"6a7aaff8-a315-4ecf-a3f4-a313912a9f09","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"qttySW90KnGh"},"source":["**PySpark (Python)**\n","\n","     df = spark.read.csv(\"data.csv\")\n","     df.filter(df.age > 25).show()"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"37a488c9-f791-4367-bc40-7bd2d07ad368","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"e3wzaySFKnGi"},"source":["**Simple definition**\n","- PySpark is a tool that lets you process **big data** using **Python**, powered by **Apache Spark**."]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"b844822b-8d04-4dad-9994-c4d6425a6b7c","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"IFMeX243KnGi"},"source":["**3) Why do we need PySpark?**\n","\n","- Imagine you have:\n","  - **Millions or billions of records**\n","  - **Data too big for Pandas**\n","  - **Need fast processing**\n","\n","=> **Pandas** works on **one machine**.\n","\n","=> **PySpark** works on **many machines** together."]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"2b5bfc5c-379e-4a4e-aee0-03eca9b1523c","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"SXIEytf0KnGi"},"source":["**Pandas (single machine)**\n","\n","     import pandas as pd\n","     df = pd.read_csv(\"big_file.csv\")\n","\n","- Can **crash** if data is **too large**."]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"d077475b-9dbf-413a-9f7a-79506021ad18","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"e-yBKb7YKnGj"},"source":["**PySpark (distributed)**\n","\n","     df = spark.read.csv(\"big_file.csv\", header=True)\n","\n","- **Handles huge data**.\n","- Uses **cluster resources**."]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"c23e3312-56c7-468f-990b-6424ee4616dd","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"v0Y33fS7KnGj"},"source":["**4) Where is PySpark used?**\n","\n","- PySpark is **widely used** in:\n","  - **Data Engineering**\n","  - **ETL pipelines**\n","  - **Big Data processing**\n","  - **Data Analytics**\n","  - **Machine Learning (Spark MLlib)**\n","  - **Streaming (Spark Structured Streaming)**"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"b54ef835-4e8f-4efe-92ba-f5103a57fe27","showTitle":false,"tableResultSettingsMap":{},"title":""},"id":"VfqLBH9uKnGj"},"source":["**5) When should you use PySpark?**\n","- **Use PySpark when:**\n","  - Data size is very large\n","  - Processing needs speed & scalability\n","  - Data is distributed\n","  - Pandas becomes slow or crashes\n","\n","**6) When NOT to use PySpark?**\n","- **Avoid PySpark when:**\n","  - Data is small\n","  - Simple analysis\n","  - Pandas is enough"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"computePreferences":null,"dashboards":[],"environmentMetadata":{"base_environment":"","environment_version":"4"},"inputWidgetPreferences":null,"language":"python","notebookMetadata":{"pythonIndentUnit":4},"notebookName":"1_What is PySpark?","widgets":{}},"language_info":{"name":"python"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}