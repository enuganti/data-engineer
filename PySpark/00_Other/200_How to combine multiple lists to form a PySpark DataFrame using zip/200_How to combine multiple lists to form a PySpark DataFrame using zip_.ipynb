{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19c4ef62-3a35-4f9e-b295-23c7ae8f1f86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1) zip()\n",
    "2) zipWithIndex()\n",
    "3) zipWithUniqueId()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "663eac62-c329-4390-bf15-ebc82436205a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**zip**\n",
    "\n",
    "- Python zip() is a **built-in** function that takes **zero or more iterable objects** as **arguments** (e.g. **lists, tuples, or sets**) and **aggregates** them in the form of a series of **tuples**.\n",
    "\n",
    "- zip() is primarily used for **combining two datasets element-wise**.\n",
    "\n",
    "- When creating a PySpark DataFrame from **multiple lists**, ensure that the **lists are aligned correctly**. Each list represents a **column**, and their **lengths should be the same** to avoid data misalignment.\n",
    "\n",
    "- The **zip** function is commonly used to **combine multiple lists element-wise**.\n",
    "\n",
    "- It creates **tuples**, with **each tuple** containing values from corresponding positions in the input lists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2de826d2-628c-4d63-a2cf-0e3bdcdf8207",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Syntax**\n",
    "\n",
    "     # Syntax of zip() function\n",
    "     zip(iterator1, iterator2, ...)\n",
    "\n",
    "**parameters:**\n",
    "- It takes **iterable** objects as its **arguments**.\n",
    "\n",
    "**Return value:**\n",
    "- It returns a **zip object** which is the **iterable object** of **tuples**.\n",
    "- If **no argument** is passed into **zip()**, it will return the **empty iterator**.\n",
    "- If we pass **one argument**, it will return the **iterable of tuples** where **each tuple** has a **single element**.\n",
    "- If we pass **more than two iterables**, it will return an **iterable of tuples** where **each tuple** contains elements of **all passed iterables**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42798408-7d8f-49fb-91dc-9fd54f9ce09a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**1) zip() Function without Arguments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f497f05-64f6-4783-963a-246518a84a8e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Returns an empty list"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Initialize two lists\n",
    "subjects1 = [\"Java\",\"Python\",\"PHP\"]\n",
    "subjects2 = ['C#','CPP','C']\n",
    "\n",
    "# zip() function with out arguments\n",
    "final = zip()\n",
    "print(list(final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77ec94d2-c1d6-49ba-9026-43a0ef4089d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**2) zip() with Single Iterable as Argument**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c281c43d-3cb0-46df-aa13-7d1941f0c57e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Java',), ('Python',), ('PHP',)]\n"
     ]
    }
   ],
   "source": [
    "subjects1 = [\"Java\", \"Python\", \"PHP\"]\n",
    "# Passing single iterable into zip()\n",
    "final = zip(subjects1)\n",
    "print(list(final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "673afa4d-bf5a-4888-9cf1-08f795a32eef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**3) zip() with Two Iterable as Argument**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff335840-81e9-4286-8ece-c44e16e9b69e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Example 01"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List1 : ['Java', 'Python', 'PHP']\nList2 : ['C#', 'CPP', 'C']\n\nZip Lists : [('Java', 'C#'), ('Python', 'CPP'), ('PHP', 'C')]\n"
     ]
    }
   ],
   "source": [
    "# Initialize two lists\n",
    "subjects1 = [\"Java\", \"Python\", \"PHP\"]\n",
    "subjects2 = ['C#','CPP','C']\n",
    "print(\"List1 :\", subjects1)\n",
    "print(\"List2 :\", subjects2)\n",
    "\n",
    "# Zip two lists\n",
    "final = zip(subjects1, subjects2)\n",
    "print(\"\\nZip Lists :\", list(final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaab4fa0-2231-463e-9b36-7013d0e78f40",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Example 02"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('Adarsh', 30, 'India'),\n",
       " ('Bibin', 25, 'SriLanka'),\n",
       " ('Chetan', 35, 'Nepal'),\n",
       " ('Damini', 28, 'US'),\n",
       " ('Kennedy', 29, 'UK')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1 = ['Adarsh', 'Bibin', 'Chetan', 'Damini', 'Kennedy']\n",
    "list2 = [30, 25, 35, 28, 29]\n",
    "list3 = ['India', 'SriLanka', 'Nepal', 'US', 'UK']\n",
    "\n",
    "rows = list(zip(list1, list2, list3))\n",
    "rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b26478db-dd65-4626-8a41-888a8344549b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- **zip()** takes corresponding **elements from each list** and groups them into **tuples**.\n",
    "- **list(zip(...))** converts the **zipped** result into a **list of those tuples**.\n",
    "\n",
    "      [\n",
    "       ('Adarsh', 30, 'India'),\n",
    "       ('Bibin', 25, 'SriLanka'),\n",
    "       ('Chetan', 35, 'Nepal'),\n",
    "       ('Damini', 28, 'US'),\n",
    "       ('Kennedy', 29, 'UK')\n",
    "      ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9e06376-7b19-46d9-8d32-580eb86bc03b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Name</th><th>Age</th><th>City</th></tr></thead><tbody><tr><td>Adarsh</td><td>30</td><td>India</td></tr><tr><td>Bibin</td><td>25</td><td>SriLanka</td></tr><tr><td>Chetan</td><td>35</td><td>Nepal</td></tr><tr><td>Damini</td><td>28</td><td>US</td></tr><tr><td>Kennedy</td><td>29</td><td>UK</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Adarsh",
         30,
         "India"
        ],
        [
         "Bibin",
         25,
         "SriLanka"
        ],
        [
         "Chetan",
         35,
         "Nepal"
        ],
        [
         "Damini",
         28,
         "US"
        ],
        [
         "Kennedy",
         29,
         "UK"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Age",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "City",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "schema = StructType([StructField(\"Name\", StringType(), True),\n",
    "                     StructField(\"Age\", IntegerType(), True),\n",
    "                     StructField(\"City\", StringType(), True)])\n",
    "\n",
    "df = spark.createDataFrame(rows, schema)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4a11f25-4cfc-4fdd-8a4d-355480655c79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**4) Pass Multiple Iterables into Python zip()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f0983fb-8fb7-433c-bc53-c587e776eec3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List1 : ['Java', 'Python', 'PHP']\nList2 : ['C#', 'CPP', 'C']\nlist3 : ['.net', 'pyspark', 'scala']\n\nZip Lists : [('Java', 'C#', '.net'), ('Python', 'CPP', 'pyspark'), ('PHP', 'C', 'scala')]\n"
     ]
    }
   ],
   "source": [
    "# Initialize multiple lists\n",
    "subjects1 = [\"Java\", \"Python\", \"PHP\"]\n",
    "subjects2 = ['C#','CPP','C']\n",
    "subjects3 = ['.net','pyspark','scala']\n",
    "print(\"List1 :\", subjects1)\n",
    "print(\"List2 :\", subjects2)\n",
    "print(\"list3 :\", subjects3)\n",
    "\n",
    "# Zip multiple lists\n",
    "final = zip(subjects1, subjects2, subjects3)\n",
    "print(\"\\nZip Lists :\", list(final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1c3e720-8231-45eb-9ecb-e9605efb5780",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**5) Pass Unequal Lengths of Iterables**\n",
    "- when we pass the **unequal length** of iterables into **zip()** function, it will return the iterable of **tuples** having **same length** of **least passed iterable**.\n",
    "- Here, **“html”** is the **extra element**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41e3a5c7-1081-4a0e-acba-6a89608b5287",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List1 : ['Java', 'Python', 'PHP', 'html']\nList2 : ['C#', 'CPP', 'C']\nzip unequal lists: [('Java', 'C#'), ('Python', 'CPP'), ('PHP', 'C')]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the unequal lengths of list\n",
    "subjects1 = [\"Java\", \"Python\", \"PHP\", \"html\"]\n",
    "subjects2 = ['C#','CPP','C']\n",
    "print(\"List1 :\", subjects1)\n",
    "print(\"List2 :\", subjects2)\n",
    "\n",
    "# Zip the unequal lists\n",
    "final = zip(subjects1, subjects2)\n",
    "print(\"zip unequal lists:\", list(final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec5a2b49-813a-43e8-b072-a764850bf482",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List1: ['Java', 'Python', 'PHP', 'html']\nlist2: ['C#', 'CPP', 'C']\nJava   C#\nPython   CPP\nPHP   C\n"
     ]
    }
   ],
   "source": [
    "# Traversing Parallelly\n",
    "print(\"List1:\", subjects1)\n",
    "print(\"list2:\", subjects2)\n",
    "\n",
    "for i, j in zip(subjects1, subjects2):\n",
    "    print(i,\" \",j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d2a4829-10ed-44e6-ba34-532c2acf47ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**6) Unzipping the Iterables**\n",
    "- **unpack operator (*)** is used to **unzip** the **iterable objects**. If we pass the unpacking operator inside the zip, then iterators will be unzipped.\n",
    "\n",
    "**Syntax:**\n",
    "\n",
    "     # zip() with unpack operator\n",
    "     zip(*zipped_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d322b159-6cec-49f0-a55e-f23b06a866a8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "unpack operator (*)"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List1: ('Java', 'Python', 'PHP')\nList2: ('C#', 'CPP', 'C')\n"
     ]
    }
   ],
   "source": [
    "# Initialize the lists\n",
    "subjects1 = [\"Java\", \"Python\", \"PHP\", \"html\"]\n",
    "subjects2 = ['C#','CPP','C']\n",
    "\n",
    "final = zip(subjects1, subjects2)\n",
    "final1 = list(final)\n",
    "\n",
    "# Unzipping the zipped object\n",
    "subjects1,subjects2 = zip(*final1)\n",
    "print(\"List1:\", subjects1)\n",
    "print(\"List2:\", subjects2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ab2d201-c378-48a6-9dc2-8548bb525fc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**7) Zip iterable Objects into Python Dictionary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a87b845-eee4-45c3-af37-ecec9c67db99",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "dictionary"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List1: ['course', 'fee', 'duration']\nList2: ['Python', '4000', '45 days']\n\nGet the dictionary using zip(): {'course': 'Python', 'fee': '4000', 'duration': '45 days'}\n"
     ]
    }
   ],
   "source": [
    "# Initialize the lists\n",
    "keys = [\"course\", \"fee\", \"duration\"]\n",
    "values = ['Python','4000','45 days']\n",
    "print(\"List1:\", keys)\n",
    "print(\"List2:\", values)\n",
    "\n",
    "# Use zip() to convert the dictionary\n",
    "final = dict(zip(keys, values))\n",
    "print(\"\\nGet the dictionary using zip():\", final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec59bbde-d1b1-461f-8da8-2743711b6578",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**8) Using zip() with RDDs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1070e76-ee43-47c1-b229-ef0375f1f0de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd')]\n"
     ]
    }
   ],
   "source": [
    "# Create two RDDs\n",
    "rdd1 = spark.sparkContext.parallelize([1, 2, 3, 4])\n",
    "rdd2 = spark.sparkContext.parallelize([\"a\", \"b\", \"c\", \"d\"])\n",
    "\n",
    "# Zip the RDDs\n",
    "zipped_rdd = rdd1.zip(rdd2)\n",
    "\n",
    "# Collect and display results\n",
    "print(zipped_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d402defd-64cf-476c-b603-4e30cdf7789b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- **zip()** combines the **two RDDs element-wise**.\n",
    "- The result is an **RDD of tuples**:\n",
    "  - **Each tuple** contains **one element** from **rdd1** and the **corresponding element** from **rdd2**.\n",
    "\n",
    "        [(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd')]\n",
    "\n",
    "  - **Note:** Both **RDDs** must have the **same number of elements** and be partitioned the same way for zip() to work correctly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "779385f3-6278-4886-b65f-cd14aa2197bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**9) Zipping RDDs with Different Data Types**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2e12e7c-eaaf-4104-b38c-5bb5e9895831",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((1001, 'Admin'), True), ((1002, 'Sales'), False), ((1003, 'Marketing'), True), ((1004, 'HR'), False), ((1005, 'Finance'), False), ((1006, 'Maintenance'), True)]\n"
     ]
    }
   ],
   "source": [
    "# Create RDDs\n",
    "rdd_numbers = spark.sparkContext.parallelize([1001, 1002, 1003, 1004, 1005, 1006])\n",
    "rdd_strings = spark.sparkContext.parallelize([\"Admin\", \"Sales\", \"Marketing\", \"HR\", \"Finance\", \"Maintenance\"])\n",
    "rdd_booleans = spark.sparkContext.parallelize([True, False, True, False, False, True])\n",
    "\n",
    "# Zip RDDs\n",
    "zipped_rdd = rdd_numbers.zip(rdd_strings).zip(rdd_booleans)\n",
    "print(zipped_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96cb7c58-712a-4a00-84f1-08fb8f63d492",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1001, 'Admin', True), (1002, 'Sales', False), (1003, 'Marketing', True), (1004, 'HR', False), (1005, 'Finance', False), (1006, 'Maintenance', True)]\n"
     ]
    }
   ],
   "source": [
    "# Flatten the tuples and display\n",
    "flattened_rdd = zipped_rdd.map(lambda x: (x[0][0], x[0][1], x[1]))\n",
    "print(flattened_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1bc3f16-6e96-421f-8049-eda2b7e6d734",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "     zipped_rdd = rdd_numbers.zip(rdd_strings).zip(rdd_booleans)\n",
    "\n",
    "- This is a **two-step zip** process:\n",
    "\n",
    "  - **First:** rdd_numbers.zip(rdd_strings)\n",
    "    - Combines the **first and second** RDDs **element-wise** into **tuples** like:\n",
    "\n",
    "          (1001, \"Admin\"), (1002, \"Sales\"), ...\n",
    "\n",
    "  - Then: **.zip(rdd_booleans)**\n",
    "\n",
    "    - Each result from the first zip is now zipped with a boolean value:\n",
    "\n",
    "          ((1001, \"Admin\"), True), ((1002, \"Sales\"), False), ...\n",
    "\n",
    "**Flattening the Tuples:**\n",
    "\n",
    "      flattened_rdd = zipped_rdd.map(lambda x: (x[0][0], x[0][1], x[1]))\n",
    "\n",
    "      x = ((1001, 'Admin'), True)\n",
    "      \n",
    "      (1001, 'Admin', True)\n",
    "\n",
    "**Collecting and Printing the Results**\n",
    "\n",
    "      print(flattened_rdd.collect())\n",
    "\n",
    "**Output**\n",
    "\n",
    "     [\n",
    "       (1001, 'Admin', True),\n",
    "       (1002, 'Sales', False),\n",
    "       (1003, 'Marketing', True),\n",
    "       (1004, 'HR', False),\n",
    "       (1005, 'Finance', False),\n",
    "       (1006, 'Maintenance', True)\n",
    "     ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e25af9a3-dc79-4943-90aa-4eb2f964bd67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**10) Using zip() for Advanced RDD Transformations**\n",
    "\n",
    "- To add **row numbers or indices** to your RDD.\n",
    "\n",
    "- **.zipWithIndex()** assigns a **sequential index (starting from 0)** to **each element** in the RDD.\n",
    "\n",
    "- It returns a **new RDD of tuples**: each element paired with its corresponding index.\n",
    "\n",
    "- Unlike **.zipWithUniqueId()**, the indices are **guaranteed to be 0-based and sequential**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a72bbdcb-df07-4c68-a13c-3446b5815d0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('kamal', 0), ('Bobby', 1), ('Senthil', 2), ('Dravid', 3)]\n"
     ]
    }
   ],
   "source": [
    "# Create an RDD\n",
    "rdd = spark.sparkContext.parallelize([\"kamal\", \"Bobby\", \"Senthil\", \"Dravid\"])\n",
    "\n",
    "# Zip with index\n",
    "zipped_with_index = rdd.zipWithIndex()\n",
    "\n",
    "# Collect and display results\n",
    "print(zipped_with_index.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3cfd712a-b48d-4a06-b21d-9bca85fe3591",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**5) Using zipWithUniqueId for Unique Identifiers**\n",
    "\n",
    "- **.zipWithUniqueId()** pairs each element of the RDD with a **unique long integer ID**.\n",
    "\n",
    "- The IDs are **monotonically increasing** and unique, but they are **not guaranteed** to be **sequential** (i.e., **not always 0, 1, 2, 3...**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6c704f6-6ba0-405a-bb31-da261004aa18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('kamal', 1), ('Bobby', 3), ('Senthil', 4), ('Dravid', 6), ('Shobha', 7)]\n"
     ]
    }
   ],
   "source": [
    "# Create an RDD\n",
    "rdd = spark.sparkContext.parallelize([\"kamal\", \"Bobby\", \"Senthil\", \"Dravid\", \"Shobha\"])\n",
    "\n",
    "# Zip with unique ID\n",
    "zipped_with_unique_id = rdd.zipWithUniqueId()\n",
    "\n",
    "# Collect and display results\n",
    "print(zipped_with_unique_id.collect())"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "200_How to combine multiple lists to form a PySpark DataFrame using zip?",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}