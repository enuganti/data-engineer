{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1346b39-35e9-49a8-af85-3977fa17954f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Explode: \n",
    "   - Explode function is used to **convert** collection columns **(List, Array & Map) to rows**.\n",
    "   - When an **array** is passed to explode function, it creates a **new row for each element in array**.\n",
    "   - When a **map** is passed, it creates **two new columns one for key and one for value** and each element in map split into the rows. If the **array or map is NULL**, that **row is eliminated**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4ad069e-65fa-4f96-8bf3-4b195b672864",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, explode\n",
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c8fef14-d174-4160-b266-127e0fc639d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function explode in module pyspark.sql.functions:\n\nexplode(col: 'ColumnOrName') -> pyspark.sql.column.Column\n    Returns a new row for each element in the given array or map.\n    Uses the default column name `col` for elements in the array and\n    `key` and `value` for elements in the map unless specified otherwise.\n    \n    .. versionadded:: 1.4.0\n    \n    .. versionchanged:: 3.4.0\n        Support Spark Connect.\n    \n    Parameters\n    ----------\n    col : :class:`~pyspark.sql.Column` or str\n        target column to work on.\n    \n    Returns\n    -------\n    :class:`~pyspark.sql.Column`\n        one row per array item or map key value.\n    \n    See Also\n    --------\n    :meth:`pyspark.functions.posexplode`\n    :meth:`pyspark.functions.explode_outer`\n    :meth:`pyspark.functions.posexplode_outer`\n    \n    Examples\n    --------\n    >>> from pyspark.sql import Row\n    >>> eDF = spark.createDataFrame([Row(a=1, intlist=[1,2,3], mapfield={\"a\": \"b\"})])\n    >>> eDF.select(explode(eDF.intlist).alias(\"anInt\")).collect()\n    [Row(anInt=1), Row(anInt=2), Row(anInt=3)]\n    \n    >>> eDF.select(explode(eDF.mapfield).alias(\"key\", \"value\")).show()\n    +---+-----+\n    |key|value|\n    +---+-----+\n    |  a|    b|\n    +---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "help(explode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d3ff58e-0a85-449f-b7bc-bc00c2f13b5d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 1) Create dataframe with array column\n",
    "- Explode(Array Type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2364645-92e0-4573-85f2-55f5b3649072",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------------------------+\n|id |Name    |skills                      |\n+---+--------+----------------------------+\n|1  |Suresh  |[.net, Python, Spark, Azure]|\n|2  |Ramya   |[java, PySpark, AWS]        |\n|3  |Apurba  |[C, SAP, Mainframes]        |\n|4  |Pranitha|[COBOL, DEVOPS]             |\n|5  |Sowmya  |[ABAP]                      |\n+---+--------+----------------------------+\n\nroot\n |-- id: long (nullable = true)\n |-- Name: string (nullable = true)\n |-- skills: array (nullable = true)\n |    |-- element: string (containsNull = true)\n\nNumber of Rows: 5\n"
     ]
    }
   ],
   "source": [
    "data = [(1, \"Suresh\", [\".net\", \"Python\", \"Spark\", \"Azure\"]),\n",
    "        (2, \"Ramya\", [\"java\", \"PySpark\", \"AWS\"]),\n",
    "        (3, \"Apurba\", [\"C\", \"SAP\", \"Mainframes\"]),\n",
    "        (4, \"Pranitha\", [\"COBOL\", \"DEVOPS\"]),\n",
    "        (5, \"Sowmya\", [\"ABAP\"])]\n",
    "\n",
    "schema = [\"id\", \"Name\", \"skills\"]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "df.show(truncate=False)\n",
    "df.printSchema()\n",
    "print(\"Number of Rows:\", df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72c4bf22-6f0c-4aa6-851b-2c270707973e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>Name</th><th>skills</th><th>New-Skills</th></tr></thead><tbody><tr><td>1</td><td>Suresh</td><td>List(.net, Python, Spark, Azure)</td><td>.net</td></tr><tr><td>1</td><td>Suresh</td><td>List(.net, Python, Spark, Azure)</td><td>Python</td></tr><tr><td>1</td><td>Suresh</td><td>List(.net, Python, Spark, Azure)</td><td>Spark</td></tr><tr><td>1</td><td>Suresh</td><td>List(.net, Python, Spark, Azure)</td><td>Azure</td></tr><tr><td>2</td><td>Ramya</td><td>List(java, PySpark, AWS)</td><td>java</td></tr><tr><td>2</td><td>Ramya</td><td>List(java, PySpark, AWS)</td><td>PySpark</td></tr><tr><td>2</td><td>Ramya</td><td>List(java, PySpark, AWS)</td><td>AWS</td></tr><tr><td>3</td><td>Apurba</td><td>List(C, SAP, Mainframes)</td><td>C</td></tr><tr><td>3</td><td>Apurba</td><td>List(C, SAP, Mainframes)</td><td>SAP</td></tr><tr><td>3</td><td>Apurba</td><td>List(C, SAP, Mainframes)</td><td>Mainframes</td></tr><tr><td>4</td><td>Pranitha</td><td>List(COBOL, DEVOPS)</td><td>COBOL</td></tr><tr><td>4</td><td>Pranitha</td><td>List(COBOL, DEVOPS)</td><td>DEVOPS</td></tr><tr><td>5</td><td>Sowmya</td><td>List(ABAP)</td><td>ABAP</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Suresh",
         [
          ".net",
          "Python",
          "Spark",
          "Azure"
         ],
         ".net"
        ],
        [
         1,
         "Suresh",
         [
          ".net",
          "Python",
          "Spark",
          "Azure"
         ],
         "Python"
        ],
        [
         1,
         "Suresh",
         [
          ".net",
          "Python",
          "Spark",
          "Azure"
         ],
         "Spark"
        ],
        [
         1,
         "Suresh",
         [
          ".net",
          "Python",
          "Spark",
          "Azure"
         ],
         "Azure"
        ],
        [
         2,
         "Ramya",
         [
          "java",
          "PySpark",
          "AWS"
         ],
         "java"
        ],
        [
         2,
         "Ramya",
         [
          "java",
          "PySpark",
          "AWS"
         ],
         "PySpark"
        ],
        [
         2,
         "Ramya",
         [
          "java",
          "PySpark",
          "AWS"
         ],
         "AWS"
        ],
        [
         3,
         "Apurba",
         [
          "C",
          "SAP",
          "Mainframes"
         ],
         "C"
        ],
        [
         3,
         "Apurba",
         [
          "C",
          "SAP",
          "Mainframes"
         ],
         "SAP"
        ],
        [
         3,
         "Apurba",
         [
          "C",
          "SAP",
          "Mainframes"
         ],
         "Mainframes"
        ],
        [
         4,
         "Pranitha",
         [
          "COBOL",
          "DEVOPS"
         ],
         "COBOL"
        ],
        [
         4,
         "Pranitha",
         [
          "COBOL",
          "DEVOPS"
         ],
         "DEVOPS"
        ],
        [
         5,
         "Sowmya",
         [
          "ABAP"
         ],
         "ABAP"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "skills",
         "type": "{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "New-Skills",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: long (nullable = true)\n |-- Name: string (nullable = true)\n |-- skills: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- New-Skills: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"New-Skills\", explode(col('skills')))\n",
    "display(df)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33709f5e-c42c-42a5-afd4-20e762fbe152",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------------------------+\n|id |Name    |skills                      |\n+---+--------+----------------------------+\n|1  |Suresh  |[.net, Python, Spark, Azure]|\n|2  |Ramya   |[java, PySpark, AWS]        |\n|3  |Rakesh  |[ADF, SQL, null, GCC]       |\n|4  |Apurba  |[C, SAP, Mainframes]        |\n|5  |Pranitha|[COBOL, DEVOPS]             |\n|6  |Sowmya  |[ABAP]                      |\n|7  |Anand   |null                        |\n|8  |Sourabh |[]                          |\n+---+--------+----------------------------+\n\nroot\n |-- id: long (nullable = true)\n |-- Name: string (nullable = true)\n |-- skills: array (nullable = true)\n |    |-- element: string (containsNull = true)\n\nNumber of Rows: 8\n"
     ]
    }
   ],
   "source": [
    "data1 = [(1, \"Suresh\", [\".net\", \"Python\", \"Spark\", \"Azure\"]),\n",
    "         (2, \"Ramya\", [\"java\", \"PySpark\", \"AWS\"]),\n",
    "         (3, \"Rakesh\", [\"ADF\", \"SQL\", None, \"GCC\"]),\n",
    "         (4, \"Apurba\", [\"C\", \"SAP\", \"Mainframes\"]),\n",
    "         (5, \"Pranitha\", [\"COBOL\", \"DEVOPS\"]),\n",
    "         (6, \"Sowmya\", [\"ABAP\"]),\n",
    "         (7, \"Anand\", None),\n",
    "         (8, \"Sourabh\", [])]\n",
    "\n",
    "schema1 = [\"id\", \"Name\", \"skills\"]\n",
    "\n",
    "df1 = spark.createDataFrame(data=data1, schema=schema1)\n",
    "df1.show(truncate=False)\n",
    "df1.printSchema()\n",
    "print(\"Number of Rows:\", df1.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff22c399-c3e8-4f53-aa56-6011eb14d3c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>Name</th><th>skills</th><th>New-Skills</th></tr></thead><tbody><tr><td>1</td><td>Suresh</td><td>List(.net, Python, Spark, Azure)</td><td>.net</td></tr><tr><td>1</td><td>Suresh</td><td>List(.net, Python, Spark, Azure)</td><td>Python</td></tr><tr><td>1</td><td>Suresh</td><td>List(.net, Python, Spark, Azure)</td><td>Spark</td></tr><tr><td>1</td><td>Suresh</td><td>List(.net, Python, Spark, Azure)</td><td>Azure</td></tr><tr><td>2</td><td>Ramya</td><td>List(java, PySpark, AWS)</td><td>java</td></tr><tr><td>2</td><td>Ramya</td><td>List(java, PySpark, AWS)</td><td>PySpark</td></tr><tr><td>2</td><td>Ramya</td><td>List(java, PySpark, AWS)</td><td>AWS</td></tr><tr><td>3</td><td>Rakesh</td><td>List(ADF, SQL, null, GCC)</td><td>ADF</td></tr><tr><td>3</td><td>Rakesh</td><td>List(ADF, SQL, null, GCC)</td><td>SQL</td></tr><tr><td>3</td><td>Rakesh</td><td>List(ADF, SQL, null, GCC)</td><td>null</td></tr><tr><td>3</td><td>Rakesh</td><td>List(ADF, SQL, null, GCC)</td><td>GCC</td></tr><tr><td>4</td><td>Apurba</td><td>List(C, SAP, Mainframes)</td><td>C</td></tr><tr><td>4</td><td>Apurba</td><td>List(C, SAP, Mainframes)</td><td>SAP</td></tr><tr><td>4</td><td>Apurba</td><td>List(C, SAP, Mainframes)</td><td>Mainframes</td></tr><tr><td>5</td><td>Pranitha</td><td>List(COBOL, DEVOPS)</td><td>COBOL</td></tr><tr><td>5</td><td>Pranitha</td><td>List(COBOL, DEVOPS)</td><td>DEVOPS</td></tr><tr><td>6</td><td>Sowmya</td><td>List(ABAP)</td><td>ABAP</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Suresh",
         [
          ".net",
          "Python",
          "Spark",
          "Azure"
         ],
         ".net"
        ],
        [
         1,
         "Suresh",
         [
          ".net",
          "Python",
          "Spark",
          "Azure"
         ],
         "Python"
        ],
        [
         1,
         "Suresh",
         [
          ".net",
          "Python",
          "Spark",
          "Azure"
         ],
         "Spark"
        ],
        [
         1,
         "Suresh",
         [
          ".net",
          "Python",
          "Spark",
          "Azure"
         ],
         "Azure"
        ],
        [
         2,
         "Ramya",
         [
          "java",
          "PySpark",
          "AWS"
         ],
         "java"
        ],
        [
         2,
         "Ramya",
         [
          "java",
          "PySpark",
          "AWS"
         ],
         "PySpark"
        ],
        [
         2,
         "Ramya",
         [
          "java",
          "PySpark",
          "AWS"
         ],
         "AWS"
        ],
        [
         3,
         "Rakesh",
         [
          "ADF",
          "SQL",
          null,
          "GCC"
         ],
         "ADF"
        ],
        [
         3,
         "Rakesh",
         [
          "ADF",
          "SQL",
          null,
          "GCC"
         ],
         "SQL"
        ],
        [
         3,
         "Rakesh",
         [
          "ADF",
          "SQL",
          null,
          "GCC"
         ],
         null
        ],
        [
         3,
         "Rakesh",
         [
          "ADF",
          "SQL",
          null,
          "GCC"
         ],
         "GCC"
        ],
        [
         4,
         "Apurba",
         [
          "C",
          "SAP",
          "Mainframes"
         ],
         "C"
        ],
        [
         4,
         "Apurba",
         [
          "C",
          "SAP",
          "Mainframes"
         ],
         "SAP"
        ],
        [
         4,
         "Apurba",
         [
          "C",
          "SAP",
          "Mainframes"
         ],
         "Mainframes"
        ],
        [
         5,
         "Pranitha",
         [
          "COBOL",
          "DEVOPS"
         ],
         "COBOL"
        ],
        [
         5,
         "Pranitha",
         [
          "COBOL",
          "DEVOPS"
         ],
         "DEVOPS"
        ],
        [
         6,
         "Sowmya",
         [
          "ABAP"
         ],
         "ABAP"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "skills",
         "type": "{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "New-Skills",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: long (nullable = true)\n |-- Name: string (nullable = true)\n |-- skills: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- New-Skills: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df1 = df1.withColumn(\"New-Skills\", explode(col('skills')))\n",
    "display(df1)\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "209b59ec-8798-47e9-9dd8-bef19fcdc8e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+\n| id|  Name|New-Skills|\n+---+------+----------+\n|  1|Suresh|      .net|\n|  1|Suresh|    Python|\n|  1|Suresh|     Spark|\n|  1|Suresh|     Azure|\n|  1|Suresh|      .net|\n|  1|Suresh|    Python|\n|  1|Suresh|     Spark|\n|  1|Suresh|     Azure|\n|  1|Suresh|      .net|\n|  1|Suresh|    Python|\n|  1|Suresh|     Spark|\n|  1|Suresh|     Azure|\n|  1|Suresh|      .net|\n|  1|Suresh|    Python|\n|  1|Suresh|     Spark|\n|  1|Suresh|     Azure|\n|  2| Ramya|      java|\n|  2| Ramya|   PySpark|\n|  2| Ramya|       AWS|\n|  2| Ramya|      java|\n+---+------+----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df1.withColumn(\"New-Skills\", explode(col('skills'))).select(\"id\", \"Name\", \"New-Skills\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97757bff-65ae-484b-afd7-44a372258306",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+\n|id |Name  |New-Skills|\n+---+------+----------+\n|1  |Suresh|.net      |\n|1  |Suresh|Python    |\n|1  |Suresh|Spark     |\n|1  |Suresh|Azure     |\n|1  |Suresh|.net      |\n|1  |Suresh|Python    |\n|1  |Suresh|Spark     |\n|1  |Suresh|Azure     |\n|1  |Suresh|.net      |\n|1  |Suresh|Python    |\n|1  |Suresh|Spark     |\n|1  |Suresh|Azure     |\n|1  |Suresh|.net      |\n|1  |Suresh|Python    |\n|1  |Suresh|Spark     |\n|1  |Suresh|Azure     |\n|2  |Ramya |java      |\n|2  |Ramya |PySpark   |\n|2  |Ramya |AWS       |\n|2  |Ramya |java      |\n+---+------+----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# using select method\n",
    "df1.select(\"id\", \"Name\", explode(col(\"skills\")).alias(\"New-Skills\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5783502-56bb-41e6-bff8-d23d04a8abfe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 2) Create dataframe with map column\n",
    "- Explode(MapType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bd4afac-f9e2-412c-b5b1-c41128b6d1da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>EmpName</th><th>EmpDetails</th></tr></thead><tbody><tr><td>Sam</td><td>Map(Office -> EcoSpace, Technology -> Azure, Car -> Baleno, Bike -> Honda)</td></tr><tr><td>Krishna</td><td>Map(Office -> Bharathi, Technology -> AWS, Car -> Santro, Bike -> RoyalEnfield)</td></tr><tr><td>Arijit</td><td>Map(Office -> EcoWorld, Car -> Etios, Bike -> BMW)</td></tr><tr><td>Swamy</td><td>Map(Car -> Swift, Bike -> TVS)</td></tr><tr><td>Senthil</td><td>null</td></tr><tr><td>Anand</td><td>Map()</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Sam",
         {
          "Bike": "Honda",
          "Car": "Baleno",
          "Office": "EcoSpace",
          "Technology": "Azure"
         }
        ],
        [
         "Krishna",
         {
          "Bike": "RoyalEnfield",
          "Car": "Santro",
          "Office": "Bharathi",
          "Technology": "AWS"
         }
        ],
        [
         "Arijit",
         {
          "Bike": "BMW",
          "Car": "Etios",
          "Office": "EcoWorld"
         }
        ],
        [
         "Swamy",
         {
          "Bike": "TVS",
          "Car": "Swift"
         }
        ],
        [
         "Senthil",
         null
        ],
        [
         "Anand",
         {}
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "EmpName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "EmpDetails",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------------------------------------------------------------------+\n|EmpName|EmpDetails                                                                  |\n+-------+----------------------------------------------------------------------------+\n|Sam    |{Office -> EcoSpace, Technology -> Azure, Car -> Baleno, Bike -> Honda}     |\n|Krishna|{Office -> Bharathi, Technology -> AWS, Car -> Santro, Bike -> RoyalEnfield}|\n|Arijit |{Office -> EcoWorld, Car -> Etios, Bike -> BMW}                             |\n|Swamy  |{Car -> Swift, Bike -> TVS}                                                 |\n|Senthil|null                                                                        |\n|Anand  |{}                                                                          |\n+-------+----------------------------------------------------------------------------+\n\nroot\n |-- EmpName: string (nullable = true)\n |-- EmpDetails: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\n"
     ]
    }
   ],
   "source": [
    "data2 = [('Sam', {'Car':'Baleno', 'Bike':'Honda', 'Office':'EcoSpace', 'Technology':'Azure'}),\n",
    "         ('Krishna', {'Car':'Santro', 'Bike':'RoyalEnfield', 'Office':'Bharathi', 'Technology':'AWS'}),\n",
    "         ('Arijit', {'Car':'Etios', 'Bike':'BMW', 'Office':'EcoWorld'}),\n",
    "         ('Swamy', {'Car':'Swift', 'Bike':'TVS'}),\n",
    "         ('Senthil', None),\n",
    "         (\"Anand\", {})]\n",
    "\n",
    "schema2 = ['EmpName', 'EmpDetails']\n",
    "\n",
    "df2 = spark.createDataFrame(data=data2, schema=schema2)\n",
    "display(df2)\n",
    "df2.show(truncate=False)\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21b10764-8b78-4979-9e98-711587b34c1b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>EmpName</th><th>EmpDetails</th><th>key</th><th>value</th></tr></thead><tbody><tr><td>Sam</td><td>Map(Office -> EcoSpace, Technology -> Azure, Car -> Baleno, Bike -> Honda)</td><td>Office</td><td>EcoSpace</td></tr><tr><td>Sam</td><td>Map(Office -> EcoSpace, Technology -> Azure, Car -> Baleno, Bike -> Honda)</td><td>Technology</td><td>Azure</td></tr><tr><td>Sam</td><td>Map(Office -> EcoSpace, Technology -> Azure, Car -> Baleno, Bike -> Honda)</td><td>Car</td><td>Baleno</td></tr><tr><td>Sam</td><td>Map(Office -> EcoSpace, Technology -> Azure, Car -> Baleno, Bike -> Honda)</td><td>Bike</td><td>Honda</td></tr><tr><td>Krishna</td><td>Map(Office -> Bharathi, Technology -> AWS, Car -> Santro, Bike -> RoyalEnfield)</td><td>Office</td><td>Bharathi</td></tr><tr><td>Krishna</td><td>Map(Office -> Bharathi, Technology -> AWS, Car -> Santro, Bike -> RoyalEnfield)</td><td>Technology</td><td>AWS</td></tr><tr><td>Krishna</td><td>Map(Office -> Bharathi, Technology -> AWS, Car -> Santro, Bike -> RoyalEnfield)</td><td>Car</td><td>Santro</td></tr><tr><td>Krishna</td><td>Map(Office -> Bharathi, Technology -> AWS, Car -> Santro, Bike -> RoyalEnfield)</td><td>Bike</td><td>RoyalEnfield</td></tr><tr><td>Arijit</td><td>Map(Office -> EcoWorld, Car -> Etios, Bike -> BMW)</td><td>Office</td><td>EcoWorld</td></tr><tr><td>Arijit</td><td>Map(Office -> EcoWorld, Car -> Etios, Bike -> BMW)</td><td>Car</td><td>Etios</td></tr><tr><td>Arijit</td><td>Map(Office -> EcoWorld, Car -> Etios, Bike -> BMW)</td><td>Bike</td><td>BMW</td></tr><tr><td>Swamy</td><td>Map(Car -> Swift, Bike -> TVS)</td><td>Car</td><td>Swift</td></tr><tr><td>Swamy</td><td>Map(Car -> Swift, Bike -> TVS)</td><td>Bike</td><td>TVS</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Sam",
         {
          "Bike": "Honda",
          "Car": "Baleno",
          "Office": "EcoSpace",
          "Technology": "Azure"
         },
         "Office",
         "EcoSpace"
        ],
        [
         "Sam",
         {
          "Bike": "Honda",
          "Car": "Baleno",
          "Office": "EcoSpace",
          "Technology": "Azure"
         },
         "Technology",
         "Azure"
        ],
        [
         "Sam",
         {
          "Bike": "Honda",
          "Car": "Baleno",
          "Office": "EcoSpace",
          "Technology": "Azure"
         },
         "Car",
         "Baleno"
        ],
        [
         "Sam",
         {
          "Bike": "Honda",
          "Car": "Baleno",
          "Office": "EcoSpace",
          "Technology": "Azure"
         },
         "Bike",
         "Honda"
        ],
        [
         "Krishna",
         {
          "Bike": "RoyalEnfield",
          "Car": "Santro",
          "Office": "Bharathi",
          "Technology": "AWS"
         },
         "Office",
         "Bharathi"
        ],
        [
         "Krishna",
         {
          "Bike": "RoyalEnfield",
          "Car": "Santro",
          "Office": "Bharathi",
          "Technology": "AWS"
         },
         "Technology",
         "AWS"
        ],
        [
         "Krishna",
         {
          "Bike": "RoyalEnfield",
          "Car": "Santro",
          "Office": "Bharathi",
          "Technology": "AWS"
         },
         "Car",
         "Santro"
        ],
        [
         "Krishna",
         {
          "Bike": "RoyalEnfield",
          "Car": "Santro",
          "Office": "Bharathi",
          "Technology": "AWS"
         },
         "Bike",
         "RoyalEnfield"
        ],
        [
         "Arijit",
         {
          "Bike": "BMW",
          "Car": "Etios",
          "Office": "EcoWorld"
         },
         "Office",
         "EcoWorld"
        ],
        [
         "Arijit",
         {
          "Bike": "BMW",
          "Car": "Etios",
          "Office": "EcoWorld"
         },
         "Car",
         "Etios"
        ],
        [
         "Arijit",
         {
          "Bike": "BMW",
          "Car": "Etios",
          "Office": "EcoWorld"
         },
         "Bike",
         "BMW"
        ],
        [
         "Swamy",
         {
          "Bike": "TVS",
          "Car": "Swift"
         },
         "Car",
         "Swift"
        ],
        [
         "Swamy",
         {
          "Bike": "TVS",
          "Car": "Swift"
         },
         "Bike",
         "TVS"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "EmpName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "EmpDetails",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "key",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "value",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- EmpName: string (nullable = true)\n |-- EmpDetails: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n |-- key: string (nullable = false)\n |-- value: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df2 = df2.select(df2.EmpName, df2.EmpDetails, explode(df2.EmpDetails))\n",
    "display(df2)\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c8e8315-2d90-44fe-8bb4-01101a98ac0e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "11_Databricks_Pyspark: Explode on Array & Map Types",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
