from pyspark.sql.functions import from_unixtime, unix_timestamp, to_timestamp

df = df.select("*", to_timestamp(from_unixtime(col("Data.Party_Input_Timestamp"),'yyyy-MM-dd HH:mm:ss')).alias('party_input_timestamp'),
                    to_timestamp(from_unixtime(col("Data.Party_Last_Update_Timestamp"),'yyyy-MM-dd HH:mm:ss')).alias('party_last_update_timestamp')
              )

display(df)



// Convert Unix timestamp to timestamp
df3=df2.select(
    from_unixtime(col("timestamp_1")).alias("timestamp_1"),
    from_unixtime(col("timestamp_2"),"MM-dd-yyyy HH:mm:ss").alias("timestamp_2"),
    from_unixtime(col("timestamp_3"),"MM-dd-yyyy").alias("timestamp_3"),
    from_unixtime(col("timestamp_4")).alias("timestamp_4")
  )
df3.printSchema()
df3.show(truncate=False)




from pyspark.sql import SparkSession
from pyspark.sql.functions import current_timestamp

# Initialize a SparkSession
spark = SparkSession.builder.appName("UnixTimeAndTimestamps").getOrCreate()

# Create a DataFrame with current timestamp
df = spark.createDataFrame([], "timestamp timestamp")
df = df.withColumn("current_timestamp", current_timestamp())

df.show(truncate=False)



# Converting Timestamps to Unix Time
from pyspark.sql.functions import unix_timestamp

# Adding a new column with Unix time
df_with_unix_time = df.withColumn("unix_time", unix_timestamp("current_timestamp"))

df_with_unix_time.show(truncate=False)


# Converting Unix Time to a Human-Readable Format
from pyspark.sql.functions import from_unixtime

# Adding a new column with human-readable timestamp
df_human_readable = df_with_unix_time.withColumn("timestamp", from_unixtime("unix_time"))

df_human_readable.show(truncate=False)


# Converting Timestamps to Unix Time with Time Zone Consideration

# Specifying a particular time zone
df_with_unix_time_tz = df.withColumn("unix_time_tz", unix_timestamp("current_timestamp", "America/New_York"))

df_with_unix_time_tz.show(truncate=False)
