{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b426e6c-25eb-4be2-b5d5-9366a93d7edf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### **Creating MapType**\n",
    "\n",
    "- The PySpark **Map Type** datatype is used to represent the map **key-value pair** similar to the python Dictionary **(Dict)**.\n",
    " \n",
    "- A **MapType** column in a Spark DataFrame is one that contains **complex data** in the form of **key-value pairs**, with keys and values each having defined **data types**.\n",
    "\n",
    "- In this example, we use **StringType** for **both key and value** and specify **False** to indicate that the **map is not nullable**.\n",
    "\n",
    "- The **\"keyType\" and \"valueType\"** can be any type that further extends the DataType class for e.g the **StringType, IntegerType, ArrayType, MapType, StructType (struct)** etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad935402-8a1a-419b-8769-28e8a263caae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### **Why Choose PySpark MapType Dict in Databricks?**\n",
    "\n",
    "- PySpark **MapType Dict** allows for the **compact storage** of **key-value pairs** within a **single column**, reducing the overall storage footprint. This becomes crucial when dealing with **large datasets** in a Databricks environment.\n",
    "\n",
    "- The MapType Dict provides a **flexible schema, accommodating dynamic and evolving data structures**. This is particularly beneficial in scenarios where the data **schema might change over time**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb6e4fa4-df78-4663-8d0b-07979fd75b38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### **Syntax**\n",
    "\n",
    "     MapType(keyType, valueType, valueContainsNull=True)\n",
    "\n",
    "     from pyspark.sql.types import StringType, IntegerType, MapType\n",
    "     \n",
    "     MapType(StringType(), IntegerType(), valueContainsNull=True)     \n",
    "     MapType(StringType(), IntegerType())\n",
    "     MapType(StringType(), IntegerType(), True)\n",
    "     MapType(StringType(), StringType(), True)\n",
    "\n",
    "**Parameters**\n",
    "- **keyType:**\n",
    "  - This is the **data type** of the **keys**.\n",
    "  - The **keys** in a **MapType** are **not allowed** to be **None or NULL**.\n",
    "- **valueType:**\n",
    "  - This is the **data type** of the **values**.\n",
    "- **valueContainsNull:** \n",
    "  - This is a **boolean** value indicating whether the values can be **NULL or None**.\n",
    "  - The **default** value is **True**, which indicates that the values can be **NULL**.\n",
    "  - Specify **False** to indicate that the **map is not nullable**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d2172f7-3057-4ff4-90b6-45a0838a5ca7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, col, expr\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import StringType, IntegerType, StructType, StructField, MapType, ArrayType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "978b9250-29fb-4d0c-ae81-b8f470328c79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class MapType in module pyspark.sql.types:\n\nclass MapType(DataType)\n |  MapType(keyType: pyspark.sql.types.DataType, valueType: pyspark.sql.types.DataType, valueContainsNull: bool = True)\n |  \n |  Map data type.\n |  \n |  Parameters\n |  ----------\n |  keyType : :class:`DataType`\n |      :class:`DataType` of the keys in the map.\n |  valueType : :class:`DataType`\n |      :class:`DataType` of the values in the map.\n |  valueContainsNull : bool, optional\n |      indicates whether values can contain null (None) values.\n |  \n |  Notes\n |  -----\n |  Keys in a map data type are not allowed to be null (None).\n |  \n |  Examples\n |  --------\n |  >>> from pyspark.sql.types import IntegerType, FloatType, MapType, StringType\n |  \n |  The below example demonstrates how to create class:`MapType`:\n |  \n |  >>> map_type = MapType(StringType(), IntegerType())\n |  \n |  The values of the map can contain null (``None``) values by default:\n |  \n |  >>> (MapType(StringType(), IntegerType())\n |  ...        == MapType(StringType(), IntegerType(), True))\n |  True\n |  >>> (MapType(StringType(), IntegerType(), False)\n |  ...        == MapType(StringType(), FloatType()))\n |  False\n |  \n |  Method resolution order:\n |      MapType\n |      DataType\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __init__(self, keyType: pyspark.sql.types.DataType, valueType: pyspark.sql.types.DataType, valueContainsNull: bool = True)\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  __repr__(self) -> str\n |      Return repr(self).\n |  \n |  fromInternal(self, obj: Dict[~T, Optional[~U]]) -> Dict[~T, Optional[~U]]\n |      Converts an internal SQL object into a native Python object.\n |  \n |  jsonValue(self) -> Dict[str, Any]\n |  \n |  needConversion(self) -> bool\n |      Does this type needs conversion between Python object and internal SQL object.\n |      \n |      This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n |  \n |  simpleString(self) -> str\n |  \n |  toInternal(self, obj: Dict[~T, Optional[~U]]) -> Dict[~T, Optional[~U]]\n |      Converts a Python object into an internal SQL object.\n |  \n |  toNullable(self) -> 'MapType'\n |      Returns the same data type but set all nullability fields are true\n |      (`StructField.nullable`, `ArrayType.containsNull`, and `MapType.valueContainsNull`).\n |      \n |      .. versionadded:: 4.0.0\n |      \n |      Returns\n |      -------\n |      :class:`MapType`\n |      \n |      Examples\n |      --------\n |      Example 1: Simple nullability conversion\n |      \n |      >>> MapType(IntegerType(), StringType(), valueContainsNull=False).toNullable()\n |      MapType(IntegerType(), StringType(), True)\n |      \n |      Example 2: Nested nullability conversion\n |      \n |      >>> MapType(\n |      ...     StringType(),\n |      ...     MapType(\n |      ...         IntegerType(),\n |      ...         ArrayType(IntegerType(), containsNull=False),\n |      ...         valueContainsNull=False\n |      ...     ),\n |      ...     valueContainsNull=False\n |      ... ).toNullable()\n |      MapType(StringType(), MapType(IntegerType(), ArrayType(IntegerType(), True), True), True)\n |  \n |  ----------------------------------------------------------------------\n |  Class methods defined here:\n |  \n |  fromJson(json: Dict[str, Any], fieldPath: str = '', collationsMap: Optional[Dict[str, str]] = None) -> 'MapType' from builtins.type\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from DataType:\n |  \n |  __eq__(self, other: Any) -> bool\n |      Return self==value.\n |  \n |  __hash__(self) -> int\n |      Return hash(self).\n |  \n |  __ne__(self, other: Any) -> bool\n |      Return self!=value.\n |  \n |  json(self) -> str\n |  \n |  ----------------------------------------------------------------------\n |  Class methods inherited from DataType:\n |  \n |  fromDDL(ddl: str) -> 'DataType' from builtins.type\n |      Creates :class:`DataType` for a given DDL-formatted string.\n |      \n |      .. versionadded:: 4.0.0\n |      \n |      Parameters\n |      ----------\n |      ddl : str\n |          DDL-formatted string representation of types, e.g.\n |          :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct\n |          type can omit the ``struct<>`` for the compatibility reason with\n |          ``spark.createDataFrame`` and Python UDFs.\n |      \n |      Returns\n |      -------\n |      :class:`DataType`\n |      \n |      Examples\n |      --------\n |      Create a StructType by the corresponding DDL formatted string.\n |      \n |      >>> from pyspark.sql.types import DataType\n |      >>> DataType.fromDDL(\"b string, a int\")\n |      StructType([StructField('b', StringType(), True), StructField('a', IntegerType(), True)])\n |      \n |      Create a single DataType by the corresponding DDL formatted string.\n |      \n |      >>> DataType.fromDDL(\"decimal(10,10)\")\n |      DecimalType(10,10)\n |      \n |      Create a StructType by the legacy string format.\n |      \n |      >>> DataType.fromDDL(\"b: string, a: int\")\n |      StructType([StructField('b', StringType(), True), StructField('a', IntegerType(), True)])\n |  \n |  typeName() -> str from builtins.type\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from DataType:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n\n"
     ]
    }
   ],
   "source": [
    "help(MapType)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3fb4cb1-226a-468e-942d-6746a8f82fa8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### **1) Creating a DataFrame with MapType Schema**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d5caa9c-6ee4-4220-9084-4515f8fcf0f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**key: StringType()**\n",
    "\n",
    "**value: IntegerType()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f3bf1ed-cd93-4c46-be07-b8b8a5b15de3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "key: string, value: integer"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Name</th><th>City</th><th>Properties</th></tr></thead><tbody><tr><td>Naresh</td><td>Bangalore</td><td>Map(Exp -> 5, Age -> 25, emp_id -> 768954)</td></tr><tr><td>Harish</td><td>Chennai</td><td>Map(Exp -> 2, Age -> 30, emp_id -> 768956)</td></tr><tr><td>Prem</td><td>Hyderabad</td><td>Map(Exp -> 8, Age -> 28, emp_id -> 798954)</td></tr><tr><td>Prabhav</td><td>kochin</td><td>Map(Exp -> 6, Age -> 35, emp_id -> 788956)</td></tr><tr><td>Hari</td><td>Nasik</td><td>Map(Exp -> 9, Age -> 21, emp_id -> 769954)</td></tr><tr><td>Druv</td><td>Delhi</td><td>Map(Exp -> 4, Age -> 36, emp_id -> 768946)</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Naresh",
         "Bangalore",
         {
          "Age": 25,
          "Exp": 5,
          "emp_id": 768954
         }
        ],
        [
         "Harish",
         "Chennai",
         {
          "Age": 30,
          "Exp": 2,
          "emp_id": 768956
         }
        ],
        [
         "Prem",
         "Hyderabad",
         {
          "Age": 28,
          "Exp": 8,
          "emp_id": 798954
         }
        ],
        [
         "Prabhav",
         "kochin",
         {
          "Age": 35,
          "Exp": 6,
          "emp_id": 788956
         }
        ],
        [
         "Hari",
         "Nasik",
         {
          "Age": 21,
          "Exp": 9,
          "emp_id": 769954
         }
        ],
        [
         "Druv",
         "Delhi",
         {
          "Age": 36,
          "Exp": 4,
          "emp_id": 768946
         }
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "City",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Properties",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"integer\",\"valueContainsNull\":true}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sample DataFrame with a StringType column containing JSON strings\n",
    "data = [(\"Naresh\", \"Bangalore\", {\"Age\": 25, \"emp_id\": 768954, \"Exp\": 5}), \n",
    "        (\"Harish\", \"Chennai\", {\"Age\": 30, \"emp_id\": 768956, \"Exp\": 2}),\n",
    "        (\"Prem\", \"Hyderabad\", {\"Age\": 28, \"emp_id\": 798954, \"Exp\": 8}), \n",
    "        (\"Prabhav\", \"kochin\", {\"Age\": 35, \"emp_id\": 788956, \"Exp\": 6}),\n",
    "        (\"Hari\", \"Nasik\", {\"Age\": 21, \"emp_id\": 769954, \"Exp\": 9}), \n",
    "        (\"Druv\", \"Delhi\", {\"Age\": 36, \"emp_id\": 768946, \"Exp\": 4}),\n",
    "        ]\n",
    "\n",
    "# Define the schema for the MapType column\n",
    "map_schema = StructType([\n",
    "  StructField(\"Name\", StringType(), True),\n",
    "  StructField(\"City\", StringType(), True),\n",
    "  StructField(\"Properties\", MapType(StringType(), IntegerType()))])\n",
    "\n",
    "# Convert the StringType column to a MapType column\n",
    "df_map_int = spark.createDataFrame(data, map_schema)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "display(df_map_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95587ec3-a8b0-48f1-86b9-181720cecced",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**key : StringType()**\n",
    "\n",
    "**value: StringType()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf33a1e8-3d2d-4748-85c3-cbf31887cd1b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "key: string, value: string"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Name</th><th>City</th><th>Properties</th></tr></thead><tbody><tr><td>Naresh</td><td>Bangalore</td><td>Map(Designation -> DE, Domain -> Gas, Branch -> IT)</td></tr><tr><td>Harish</td><td>Chennai</td><td>Map(Designation -> DE, Domain -> DS, Branch -> CSC)</td></tr><tr><td>Prem</td><td>Hyderabad</td><td>Map(Designation -> DE, Domain -> Trade, Branch -> EEE)</td></tr><tr><td>Prabhav</td><td>kochin</td><td>Map(Designation -> DE, Domain -> Sales, Branch -> AI)</td></tr><tr><td>Hari</td><td>Nasik</td><td>Map(Designation -> DE, Domain -> TELE, Branch -> ECE)</td></tr><tr><td>Druv</td><td>Delhi</td><td>Map(Designation -> DE, Domain -> BANKING, Branch -> IT)</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Naresh",
         "Bangalore",
         {
          "Branch": "IT",
          "Designation": "DE",
          "Domain": "Gas"
         }
        ],
        [
         "Harish",
         "Chennai",
         {
          "Branch": "CSC",
          "Designation": "DE",
          "Domain": "DS"
         }
        ],
        [
         "Prem",
         "Hyderabad",
         {
          "Branch": "EEE",
          "Designation": "DE",
          "Domain": "Trade"
         }
        ],
        [
         "Prabhav",
         "kochin",
         {
          "Branch": "AI",
          "Designation": "DE",
          "Domain": "Sales"
         }
        ],
        [
         "Hari",
         "Nasik",
         {
          "Branch": "ECE",
          "Designation": "DE",
          "Domain": "TELE"
         }
        ],
        [
         "Druv",
         "Delhi",
         {
          "Branch": "IT",
          "Designation": "DE",
          "Domain": "BANKING"
         }
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "City",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Properties",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sample DataFrame with a StringType column containing JSON strings\n",
    "data = [(\"Naresh\", \"Bangalore\", {\"Domain\": \"Gas\", \"Branch\": \"IT\", \"Designation\": \"DE\"}), \n",
    "        (\"Harish\", \"Chennai\", {\"Domain\": \"DS\", \"Branch\": \"CSC\", \"Designation\": \"DE\"}),\n",
    "        (\"Prem\", \"Hyderabad\", {\"Domain\": \"Trade\", \"Branch\": \"EEE\", \"Designation\": \"DE\"}), \n",
    "        (\"Prabhav\", \"kochin\", {\"Domain\": \"Sales\", \"Branch\": \"AI\", \"Designation\": \"DE\"}),\n",
    "        (\"Hari\", \"Nasik\", {\"Domain\": \"TELE\", \"Branch\": \"ECE\", \"Designation\": \"DE\"}), \n",
    "        (\"Druv\", \"Delhi\", {\"Domain\": \"BANKING\", \"Branch\": \"IT\", \"Designation\": \"DE\"}),\n",
    "        ]\n",
    "\n",
    "# Define the schema for the MapType column\n",
    "map_schema = StructType([\n",
    "  StructField(\"Name\", StringType(), True),\n",
    "  StructField(\"City\", StringType(), True),\n",
    "  StructField(\"Properties\", MapType(StringType(), StringType()))])\n",
    "\n",
    "# Convert the StringType column to a MapType column\n",
    "df_map_str = spark.createDataFrame(data, map_schema)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "display(df_map_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ded65c5-77cd-4809-9fcb-df5361dbc66c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**key : StringType()**\n",
    "\n",
    "**value: StringType()**\n",
    "\n",
    "**valueContainsNull=True**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8412b422-1dee-45d9-9a81-374f12e13f2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Name</th><th>City</th><th>Properties</th></tr></thead><tbody><tr><td>Naresh</td><td>Bangalore</td><td>Map(Designation -> DE, Domain -> Gas, Branch -> IT)</td></tr><tr><td>Harish</td><td>Chennai</td><td>Map(Designation -> null, Domain -> DS, Branch -> CSC)</td></tr><tr><td>Prem</td><td>Hyderabad</td><td>Map(Designation -> DE, Domain -> Trade, Branch -> EEE)</td></tr><tr><td>Prabhav</td><td>kochin</td><td>Map(Designation -> DE, Domain -> Sales, Branch -> null)</td></tr><tr><td>Hari</td><td>Nasik</td><td>Map(Designation -> DE, Domain -> TELE, Branch -> ECE)</td></tr><tr><td>Druv</td><td>Delhi</td><td>Map(Designation -> DE, Domain -> null, Branch -> IT)</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Naresh",
         "Bangalore",
         {
          "Branch": "IT",
          "Designation": "DE",
          "Domain": "Gas"
         }
        ],
        [
         "Harish",
         "Chennai",
         {
          "Branch": "CSC",
          "Designation": null,
          "Domain": "DS"
         }
        ],
        [
         "Prem",
         "Hyderabad",
         {
          "Branch": "EEE",
          "Designation": "DE",
          "Domain": "Trade"
         }
        ],
        [
         "Prabhav",
         "kochin",
         {
          "Branch": null,
          "Designation": "DE",
          "Domain": "Sales"
         }
        ],
        [
         "Hari",
         "Nasik",
         {
          "Branch": "ECE",
          "Designation": "DE",
          "Domain": "TELE"
         }
        ],
        [
         "Druv",
         "Delhi",
         {
          "Branch": "IT",
          "Designation": "DE",
          "Domain": null
         }
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "City",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Properties",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sample DataFrame with a StringType column containing JSON strings\n",
    "data = [(\"Naresh\", \"Bangalore\", {\"Domain\": \"Gas\", \"Branch\": \"IT\", \"Designation\": \"DE\"}), \n",
    "        (\"Harish\", \"Chennai\", {\"Domain\": \"DS\", \"Branch\": \"CSC\", \"Designation\": None}),\n",
    "        (\"Prem\", \"Hyderabad\", {\"Domain\": \"Trade\", \"Branch\": \"EEE\", \"Designation\": \"DE\"}), \n",
    "        (\"Prabhav\", \"kochin\", {\"Domain\": \"Sales\", \"Branch\": None, \"Designation\": \"DE\"}),\n",
    "        (\"Hari\", \"Nasik\", {\"Domain\": \"TELE\", \"Branch\": \"ECE\", \"Designation\": \"DE\"}), \n",
    "        (\"Druv\", \"Delhi\", {\"Domain\": None, \"Branch\": \"IT\", \"Designation\": \"DE\"}),\n",
    "        ]\n",
    "\n",
    "# Define the schema for the MapType column\n",
    "map_schema = StructType([\n",
    "  StructField(\"Name\", StringType(), True),\n",
    "  StructField(\"City\", StringType(), True),\n",
    "  StructField(\"Properties\", MapType(StringType(), StringType(), True))])\n",
    "\n",
    "# Convert the StringType column to a MapType column\n",
    "df_map_str_tr = spark.createDataFrame(data, map_schema)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "display(df_map_str_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fccf68d-809e-477c-b021-8a728854da97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**key : StringType()**\n",
    "\n",
    "**value: StringType()**\n",
    "\n",
    "**valueContainsNull=False**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb032403-7b73-4bcf-a0d7-215131473467",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPySparkValueError\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-807085200884174>, line 17\u001B[0m\n",
       "\u001B[1;32m     11\u001B[0m map_schema \u001B[38;5;241m=\u001B[39m StructType([\n",
       "\u001B[1;32m     12\u001B[0m   StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mName\u001B[39m\u001B[38;5;124m\"\u001B[39m, StringType(), \u001B[38;5;28;01mTrue\u001B[39;00m),\n",
       "\u001B[1;32m     13\u001B[0m   StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCity\u001B[39m\u001B[38;5;124m\"\u001B[39m, StringType(), \u001B[38;5;28;01mTrue\u001B[39;00m),\n",
       "\u001B[1;32m     14\u001B[0m   StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mProperties\u001B[39m\u001B[38;5;124m\"\u001B[39m, MapType(StringType(), StringType(), \u001B[38;5;28;01mFalse\u001B[39;00m))])\n",
       "\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m# Convert the StringType column to a MapType column\u001B[39;00m\n",
       "\u001B[0;32m---> 17\u001B[0m df_map_str_fls \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mcreateDataFrame(data, map_schema)\n",
       "\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# Display the resulting DataFrame\u001B[39;00m\n",
       "\u001B[1;32m     20\u001B[0m display(df_map_str_fls)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     50\u001B[0m     )\n",
       "\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1605\u001B[0m, in \u001B[0;36mSparkSession.createDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n",
       "\u001B[1;32m   1600\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_pandas \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, pd\u001B[38;5;241m.\u001B[39mDataFrame):\n",
       "\u001B[1;32m   1601\u001B[0m     \u001B[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001B[39;00m\n",
       "\u001B[1;32m   1602\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m(SparkSession, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39mcreateDataFrame(  \u001B[38;5;66;03m# type: ignore[call-overload]\u001B[39;00m\n",
       "\u001B[1;32m   1603\u001B[0m         data, schema, samplingRatio, verifySchema\n",
       "\u001B[1;32m   1604\u001B[0m     )\n",
       "\u001B[0;32m-> 1605\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_dataframe(\n",
       "\u001B[1;32m   1606\u001B[0m     data, schema, samplingRatio, verifySchema  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n",
       "\u001B[1;32m   1607\u001B[0m )\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1662\u001B[0m, in \u001B[0;36mSparkSession._create_dataframe\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n",
       "\u001B[1;32m   1660\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_createFromRDD(data\u001B[38;5;241m.\u001B[39mmap(prepare), schema, samplingRatio)\n",
       "\u001B[1;32m   1661\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m-> 1662\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_createFromLocal(\u001B[38;5;28mmap\u001B[39m(prepare, data), schema)\n",
       "\u001B[1;32m   1663\u001B[0m jrdd \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mSerDeUtil\u001B[38;5;241m.\u001B[39mtoJavaArray(rdd\u001B[38;5;241m.\u001B[39m_to_java_object_rdd())\n",
       "\u001B[1;32m   1664\u001B[0m jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsparkSession\u001B[38;5;241m.\u001B[39mapplySchemaToPythonRDD(jrdd\u001B[38;5;241m.\u001B[39mrdd(), struct\u001B[38;5;241m.\u001B[39mjson())\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1229\u001B[0m, in \u001B[0;36mSparkSession._createFromLocal\u001B[0;34m(self, data, schema)\u001B[0m\n",
       "\u001B[1;32m   1221\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_createFromLocal\u001B[39m(\n",
       "\u001B[1;32m   1222\u001B[0m     \u001B[38;5;28mself\u001B[39m, data: Iterable[Any], schema: Optional[Union[DataType, List[\u001B[38;5;28mstr\u001B[39m]]]\n",
       "\u001B[1;32m   1223\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRDD[Tuple]\u001B[39m\u001B[38;5;124m\"\u001B[39m, StructType]:\n",
       "\u001B[1;32m   1224\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m   1225\u001B[0m \u001B[38;5;124;03m    Create an RDD for DataFrame from a list or pandas.DataFrame, returns the RDD and schema.\u001B[39;00m\n",
       "\u001B[1;32m   1226\u001B[0m \u001B[38;5;124;03m    This would be broken with table acl enabled as user process does not have permission to\u001B[39;00m\n",
       "\u001B[1;32m   1227\u001B[0m \u001B[38;5;124;03m    write temp files.\u001B[39;00m\n",
       "\u001B[1;32m   1228\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m-> 1229\u001B[0m     internal_data, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wrap_data_schema(data, schema)\n",
       "\u001B[1;32m   1230\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sc\u001B[38;5;241m.\u001B[39mparallelize(internal_data), struct\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1193\u001B[0m, in \u001B[0;36mSparkSession._wrap_data_schema\u001B[0;34m(self, data, schema)\u001B[0m\n",
       "\u001B[1;32m   1188\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_wrap_data_schema\u001B[39m(\n",
       "\u001B[1;32m   1189\u001B[0m     \u001B[38;5;28mself\u001B[39m, data: Iterable[Any], schema: Optional[Union[DataType, List[\u001B[38;5;28mstr\u001B[39m]]]\n",
       "\u001B[1;32m   1190\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[Iterable[Tuple], StructType]:\n",
       "\u001B[1;32m   1191\u001B[0m     \u001B[38;5;66;03m# make sure data could consumed multiple times\u001B[39;00m\n",
       "\u001B[1;32m   1192\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, \u001B[38;5;28mlist\u001B[39m):\n",
       "\u001B[0;32m-> 1193\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(data)\n",
       "\u001B[1;32m   1195\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m schema \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(schema, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n",
       "\u001B[1;32m   1196\u001B[0m         struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inferSchemaFromList(data, names\u001B[38;5;241m=\u001B[39mschema)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1621\u001B[0m, in \u001B[0;36mSparkSession._create_dataframe.<locals>.prepare\u001B[0;34m(obj)\u001B[0m\n",
       "\u001B[1;32m   1619\u001B[0m \u001B[38;5;129m@no_type_check\u001B[39m\n",
       "\u001B[1;32m   1620\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mprepare\u001B[39m(obj):\n",
       "\u001B[0;32m-> 1621\u001B[0m     verify_func(obj)\n",
       "\u001B[1;32m   1622\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:2913\u001B[0m, in \u001B[0;36m_make_type_verifier.<locals>.verify\u001B[0;34m(obj)\u001B[0m\n",
       "\u001B[1;32m   2911\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mverify\u001B[39m(obj: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m   2912\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m verify_nullability(obj):\n",
       "\u001B[0;32m-> 2913\u001B[0m         verify_value(obj)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:2868\u001B[0m, in \u001B[0;36m_make_type_verifier.<locals>.verify_struct\u001B[0;34m(obj)\u001B[0m\n",
       "\u001B[1;32m   2860\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m PySparkValueError(\n",
       "\u001B[1;32m   2861\u001B[0m             error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFIELD_STRUCT_LENGTH_MISMATCH\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m   2862\u001B[0m             message_parameters\u001B[38;5;241m=\u001B[39m{\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   2865\u001B[0m             },\n",
       "\u001B[1;32m   2866\u001B[0m         )\n",
       "\u001B[1;32m   2867\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m v, (_, verifier) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(obj, verifiers):\n",
       "\u001B[0;32m-> 2868\u001B[0m         verifier(v)\n",
       "\u001B[1;32m   2869\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(obj, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__dict__\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\u001B[1;32m   2870\u001B[0m     d \u001B[38;5;241m=\u001B[39m obj\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__dict__\u001B[39m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:2913\u001B[0m, in \u001B[0;36m_make_type_verifier.<locals>.verify\u001B[0;34m(obj)\u001B[0m\n",
       "\u001B[1;32m   2911\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mverify\u001B[39m(obj: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m   2912\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m verify_nullability(obj):\n",
       "\u001B[0;32m-> 2913\u001B[0m         verify_value(obj)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:2833\u001B[0m, in \u001B[0;36m_make_type_verifier.<locals>.verify_map\u001B[0;34m(obj)\u001B[0m\n",
       "\u001B[1;32m   2831\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m obj\u001B[38;5;241m.\u001B[39mitems():\n",
       "\u001B[1;32m   2832\u001B[0m     key_verifier(k)\n",
       "\u001B[0;32m-> 2833\u001B[0m     value_verifier(v)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:2912\u001B[0m, in \u001B[0;36m_make_type_verifier.<locals>.verify\u001B[0;34m(obj)\u001B[0m\n",
       "\u001B[1;32m   2911\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mverify\u001B[39m(obj: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[0;32m-> 2912\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m verify_nullability(obj):\n",
       "\u001B[1;32m   2913\u001B[0m         verify_value(obj)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:2657\u001B[0m, in \u001B[0;36m_make_type_verifier.<locals>.verify_nullability\u001B[0;34m(obj)\u001B[0m\n",
       "\u001B[1;32m   2655\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m   2656\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[0;32m-> 2657\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m PySparkValueError(\n",
       "\u001B[1;32m   2658\u001B[0m                 error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFIELD_NOT_NULLABLE_WITH_NAME\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m   2659\u001B[0m                 message_parameters\u001B[38;5;241m=\u001B[39m{\n",
       "\u001B[1;32m   2660\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfield_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mstr\u001B[39m(name),\n",
       "\u001B[1;32m   2661\u001B[0m                 },\n",
       "\u001B[1;32m   2662\u001B[0m             )\n",
       "\u001B[1;32m   2663\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m PySparkValueError(\n",
       "\u001B[1;32m   2664\u001B[0m             error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFIELD_NOT_NULLABLE\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m   2665\u001B[0m             message_parameters\u001B[38;5;241m=\u001B[39m{},\n",
       "\u001B[1;32m   2666\u001B[0m         )\n",
       "\u001B[1;32m   2667\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\n",
       "\u001B[0;31mPySparkValueError\u001B[0m: [FIELD_NOT_NULLABLE_WITH_NAME] value of map field Properties: This field is not nullable, but got None."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "PySparkValueError",
        "evalue": "[FIELD_NOT_NULLABLE_WITH_NAME] value of map field Properties: This field is not nullable, but got None."
       },
       "metadata": {
        "errorSummary": "[FIELD_NOT_NULLABLE_WITH_NAME] value of map field Properties: This field is not nullable, but got None."
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "FIELD_NOT_NULLABLE_WITH_NAME",
        "pysparkCallSite": null,
        "pysparkFragment": null,
        "sqlState": null,
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mPySparkValueError\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m<command-807085200884174>, line 17\u001B[0m\n\u001B[1;32m     11\u001B[0m map_schema \u001B[38;5;241m=\u001B[39m StructType([\n\u001B[1;32m     12\u001B[0m   StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mName\u001B[39m\u001B[38;5;124m\"\u001B[39m, StringType(), \u001B[38;5;28;01mTrue\u001B[39;00m),\n\u001B[1;32m     13\u001B[0m   StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCity\u001B[39m\u001B[38;5;124m\"\u001B[39m, StringType(), \u001B[38;5;28;01mTrue\u001B[39;00m),\n\u001B[1;32m     14\u001B[0m   StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mProperties\u001B[39m\u001B[38;5;124m\"\u001B[39m, MapType(StringType(), StringType(), \u001B[38;5;28;01mFalse\u001B[39;00m))])\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m# Convert the StringType column to a MapType column\u001B[39;00m\n\u001B[0;32m---> 17\u001B[0m df_map_str_fls \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mcreateDataFrame(data, map_schema)\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# Display the resulting DataFrame\u001B[39;00m\n\u001B[1;32m     20\u001B[0m display(df_map_str_fls)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     50\u001B[0m     )\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1605\u001B[0m, in \u001B[0;36mSparkSession.createDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m   1600\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_pandas \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, pd\u001B[38;5;241m.\u001B[39mDataFrame):\n\u001B[1;32m   1601\u001B[0m     \u001B[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001B[39;00m\n\u001B[1;32m   1602\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m(SparkSession, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39mcreateDataFrame(  \u001B[38;5;66;03m# type: ignore[call-overload]\u001B[39;00m\n\u001B[1;32m   1603\u001B[0m         data, schema, samplingRatio, verifySchema\n\u001B[1;32m   1604\u001B[0m     )\n\u001B[0;32m-> 1605\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_dataframe(\n\u001B[1;32m   1606\u001B[0m     data, schema, samplingRatio, verifySchema  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[1;32m   1607\u001B[0m )\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1662\u001B[0m, in \u001B[0;36mSparkSession._create_dataframe\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m   1660\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_createFromRDD(data\u001B[38;5;241m.\u001B[39mmap(prepare), schema, samplingRatio)\n\u001B[1;32m   1661\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1662\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_createFromLocal(\u001B[38;5;28mmap\u001B[39m(prepare, data), schema)\n\u001B[1;32m   1663\u001B[0m jrdd \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mSerDeUtil\u001B[38;5;241m.\u001B[39mtoJavaArray(rdd\u001B[38;5;241m.\u001B[39m_to_java_object_rdd())\n\u001B[1;32m   1664\u001B[0m jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsparkSession\u001B[38;5;241m.\u001B[39mapplySchemaToPythonRDD(jrdd\u001B[38;5;241m.\u001B[39mrdd(), struct\u001B[38;5;241m.\u001B[39mjson())\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1229\u001B[0m, in \u001B[0;36mSparkSession._createFromLocal\u001B[0;34m(self, data, schema)\u001B[0m\n\u001B[1;32m   1221\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_createFromLocal\u001B[39m(\n\u001B[1;32m   1222\u001B[0m     \u001B[38;5;28mself\u001B[39m, data: Iterable[Any], schema: Optional[Union[DataType, List[\u001B[38;5;28mstr\u001B[39m]]]\n\u001B[1;32m   1223\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRDD[Tuple]\u001B[39m\u001B[38;5;124m\"\u001B[39m, StructType]:\n\u001B[1;32m   1224\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1225\u001B[0m \u001B[38;5;124;03m    Create an RDD for DataFrame from a list or pandas.DataFrame, returns the RDD and schema.\u001B[39;00m\n\u001B[1;32m   1226\u001B[0m \u001B[38;5;124;03m    This would be broken with table acl enabled as user process does not have permission to\u001B[39;00m\n\u001B[1;32m   1227\u001B[0m \u001B[38;5;124;03m    write temp files.\u001B[39;00m\n\u001B[1;32m   1228\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1229\u001B[0m     internal_data, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wrap_data_schema(data, schema)\n\u001B[1;32m   1230\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sc\u001B[38;5;241m.\u001B[39mparallelize(internal_data), struct\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1193\u001B[0m, in \u001B[0;36mSparkSession._wrap_data_schema\u001B[0;34m(self, data, schema)\u001B[0m\n\u001B[1;32m   1188\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_wrap_data_schema\u001B[39m(\n\u001B[1;32m   1189\u001B[0m     \u001B[38;5;28mself\u001B[39m, data: Iterable[Any], schema: Optional[Union[DataType, List[\u001B[38;5;28mstr\u001B[39m]]]\n\u001B[1;32m   1190\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[Iterable[Tuple], StructType]:\n\u001B[1;32m   1191\u001B[0m     \u001B[38;5;66;03m# make sure data could consumed multiple times\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, \u001B[38;5;28mlist\u001B[39m):\n\u001B[0;32m-> 1193\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(data)\n\u001B[1;32m   1195\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m schema \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(schema, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n\u001B[1;32m   1196\u001B[0m         struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inferSchemaFromList(data, names\u001B[38;5;241m=\u001B[39mschema)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1621\u001B[0m, in \u001B[0;36mSparkSession._create_dataframe.<locals>.prepare\u001B[0;34m(obj)\u001B[0m\n\u001B[1;32m   1619\u001B[0m \u001B[38;5;129m@no_type_check\u001B[39m\n\u001B[1;32m   1620\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mprepare\u001B[39m(obj):\n\u001B[0;32m-> 1621\u001B[0m     verify_func(obj)\n\u001B[1;32m   1622\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:2913\u001B[0m, in \u001B[0;36m_make_type_verifier.<locals>.verify\u001B[0;34m(obj)\u001B[0m\n\u001B[1;32m   2911\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mverify\u001B[39m(obj: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   2912\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m verify_nullability(obj):\n\u001B[0;32m-> 2913\u001B[0m         verify_value(obj)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:2868\u001B[0m, in \u001B[0;36m_make_type_verifier.<locals>.verify_struct\u001B[0;34m(obj)\u001B[0m\n\u001B[1;32m   2860\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m PySparkValueError(\n\u001B[1;32m   2861\u001B[0m             error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFIELD_STRUCT_LENGTH_MISMATCH\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   2862\u001B[0m             message_parameters\u001B[38;5;241m=\u001B[39m{\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2865\u001B[0m             },\n\u001B[1;32m   2866\u001B[0m         )\n\u001B[1;32m   2867\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m v, (_, verifier) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(obj, verifiers):\n\u001B[0;32m-> 2868\u001B[0m         verifier(v)\n\u001B[1;32m   2869\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(obj, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__dict__\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m   2870\u001B[0m     d \u001B[38;5;241m=\u001B[39m obj\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__dict__\u001B[39m\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:2913\u001B[0m, in \u001B[0;36m_make_type_verifier.<locals>.verify\u001B[0;34m(obj)\u001B[0m\n\u001B[1;32m   2911\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mverify\u001B[39m(obj: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   2912\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m verify_nullability(obj):\n\u001B[0;32m-> 2913\u001B[0m         verify_value(obj)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:2833\u001B[0m, in \u001B[0;36m_make_type_verifier.<locals>.verify_map\u001B[0;34m(obj)\u001B[0m\n\u001B[1;32m   2831\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m obj\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m   2832\u001B[0m     key_verifier(k)\n\u001B[0;32m-> 2833\u001B[0m     value_verifier(v)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:2912\u001B[0m, in \u001B[0;36m_make_type_verifier.<locals>.verify\u001B[0;34m(obj)\u001B[0m\n\u001B[1;32m   2911\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mverify\u001B[39m(obj: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 2912\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m verify_nullability(obj):\n\u001B[1;32m   2913\u001B[0m         verify_value(obj)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:2657\u001B[0m, in \u001B[0;36m_make_type_verifier.<locals>.verify_nullability\u001B[0;34m(obj)\u001B[0m\n\u001B[1;32m   2655\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   2656\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 2657\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m PySparkValueError(\n\u001B[1;32m   2658\u001B[0m                 error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFIELD_NOT_NULLABLE_WITH_NAME\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   2659\u001B[0m                 message_parameters\u001B[38;5;241m=\u001B[39m{\n\u001B[1;32m   2660\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfield_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mstr\u001B[39m(name),\n\u001B[1;32m   2661\u001B[0m                 },\n\u001B[1;32m   2662\u001B[0m             )\n\u001B[1;32m   2663\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m PySparkValueError(\n\u001B[1;32m   2664\u001B[0m             error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFIELD_NOT_NULLABLE\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   2665\u001B[0m             message_parameters\u001B[38;5;241m=\u001B[39m{},\n\u001B[1;32m   2666\u001B[0m         )\n\u001B[1;32m   2667\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
        "\u001B[0;31mPySparkValueError\u001B[0m: [FIELD_NOT_NULLABLE_WITH_NAME] value of map field Properties: This field is not nullable, but got None."
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sample DataFrame with a StringType column containing JSON strings\n",
    "data = [(\"Naresh\", \"Bangalore\", {\"Domain\": \"Gas\", \"Branch\": \"IT\", \"Designation\": \"DE\"}), \n",
    "        (\"Harish\", \"Chennai\", {\"Domain\": None, \"Branch\": \"CSC\", \"Designation\": None}),\n",
    "        (\"Prem\", \"Hyderabad\", {\"Domain\": \"Trade\", \"Branch\": \"EEE\", \"Designation\": \"DE\"}), \n",
    "        (\"Prabhav\", \"kochin\", {\"Domain\": \"Sales\", \"Branch\": None, \"Designation\": \"DE\"}),\n",
    "        (\"Hari\", \"Nasik\", {\"Domain\": \"TELE\", \"Branch\": \"ECE\", \"Designation\": \"DE\"}), \n",
    "        (\"Druv\", \"Delhi\", {\"Domain\": None, \"Branch\": \"IT\", \"Designation\": \"DE\"}),\n",
    "        ]\n",
    "\n",
    "# Define the schema for the MapType column\n",
    "map_schema = StructType([\n",
    "  StructField(\"Name\", StringType(), True),\n",
    "  StructField(\"City\", StringType(), True),\n",
    "  StructField(\"Properties\", MapType(StringType(), StringType(), False))])\n",
    "\n",
    "# Convert the StringType column to a MapType column\n",
    "df_map_str_fls = spark.createDataFrame(data, map_schema)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "display(df_map_str_fls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a00d94a4-342c-4b47-b111-7fb2522088cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Name</th><th>City</th><th>Properties</th><th>Ages</th></tr></thead><tbody><tr><td>Naresh</td><td>Bangalore</td><td>Map(Exp -> 5, Age -> 25, emp_id -> 768954)</td><td>26</td></tr><tr><td>Harish</td><td>Chennai</td><td>Map(Exp -> 2, Age -> 30, emp_id -> 768956)</td><td>31</td></tr><tr><td>Prem</td><td>Hyderabad</td><td>Map(Exp -> 8, Age -> 28, emp_id -> 798954)</td><td>29</td></tr><tr><td>Prabhav</td><td>kochin</td><td>Map(Exp -> 6, Age -> 35, emp_id -> 788956)</td><td>36</td></tr><tr><td>Hari</td><td>Nasik</td><td>Map(Exp -> 9, Age -> 21, emp_id -> 769954)</td><td>22</td></tr><tr><td>Druv</td><td>Delhi</td><td>Map(Exp -> 4, Age -> 36, emp_id -> 768946)</td><td>37</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Naresh",
         "Bangalore",
         {
          "Age": 25,
          "Exp": 5,
          "emp_id": 768954
         },
         26
        ],
        [
         "Harish",
         "Chennai",
         {
          "Age": 30,
          "Exp": 2,
          "emp_id": 768956
         },
         31
        ],
        [
         "Prem",
         "Hyderabad",
         {
          "Age": 28,
          "Exp": 8,
          "emp_id": 798954
         },
         29
        ],
        [
         "Prabhav",
         "kochin",
         {
          "Age": 35,
          "Exp": 6,
          "emp_id": 788956
         },
         36
        ],
        [
         "Hari",
         "Nasik",
         {
          "Age": 21,
          "Exp": 9,
          "emp_id": 769954
         },
         22
        ],
        [
         "Druv",
         "Delhi",
         {
          "Age": 36,
          "Exp": 4,
          "emp_id": 768946
         },
         37
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "City",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Properties",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"integer\",\"valueContainsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "Ages",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_age = df_map_int.select(\"*\", expr(\"Properties['Age'] + 1\").alias('Ages'))\n",
    "display(df_age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c69a18c-e60d-4ae6-9fa2-1bcca6c6f34e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### **2) Creating a DataFrame with Nested MapType Schema**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b75bf354-4c66-4ce2-a3ee-b4ceeafea0d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Creating MapType from StructType**\n",
    "\n",
    "- We can create a more complex schema using **StructType and StructField**. This is useful when the data involves **nested structures**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a596264-7383-4a26-833c-8edd491e47a9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Nested MapType"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Name</th><th>Details</th></tr></thead><tbody><tr><td>John</td><td>Map(personal_info -> List(30, 517132, Bangalore, Male, 16-061981, 200000, 123 Main St))</td></tr><tr><td>Jaswanth</td><td>Map(personal_info -> List(25, 527332, Hyderabad, Male, 16-061981, 250000, 456 Maple Ave))</td></tr><tr><td>Dinesh</td><td>Map(personal_info -> List(28, 537432, Chennai, Male, 16-061981, 350000, 789 Elm St))</td></tr><tr><td>Watson</td><td>Map(personal_info -> List(30, 557672, Nasik, Male, 16-061981, 3550000, 451 27th Main))</td></tr><tr><td>David</td><td>Map(personal_info -> List(25, 757132, Cochin, Male, 16-061981, 4555000, #401 Madiwala))</td></tr><tr><td>Dravid</td><td>Map(personal_info -> List(28, 973132, Amaravati, Male, 16-061981, 6789000, 789 Mumbai))</td></tr><tr><td>Joseph</td><td>Map(personal_info -> List(30, 678132, Mumbai, Male, 16-061981, 233000, 323 3rd Cross))</td></tr><tr><td>Dhanush</td><td>Map(personal_info -> List(25, 874132, Delhi, Male, 16-061981, 9786000, 456 Maple Ave))</td></tr><tr><td>Sam</td><td>Map(personal_info -> List(28, 632132, Ahmadabad, Male, 16-061981, 984567000, 189 Walaja))</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "John",
         {
          "personal_info": [
           30,
           517132,
           "Bangalore",
           "Male",
           "16-061981",
           200000,
           "123 Main St"
          ]
         }
        ],
        [
         "Jaswanth",
         {
          "personal_info": [
           25,
           527332,
           "Hyderabad",
           "Male",
           "16-061981",
           250000,
           "456 Maple Ave"
          ]
         }
        ],
        [
         "Dinesh",
         {
          "personal_info": [
           28,
           537432,
           "Chennai",
           "Male",
           "16-061981",
           350000,
           "789 Elm St"
          ]
         }
        ],
        [
         "Watson",
         {
          "personal_info": [
           30,
           557672,
           "Nasik",
           "Male",
           "16-061981",
           3550000,
           "451 27th Main"
          ]
         }
        ],
        [
         "David",
         {
          "personal_info": [
           25,
           757132,
           "Cochin",
           "Male",
           "16-061981",
           4555000,
           "#401 Madiwala"
          ]
         }
        ],
        [
         "Dravid",
         {
          "personal_info": [
           28,
           973132,
           "Amaravati",
           "Male",
           "16-061981",
           6789000,
           "789 Mumbai"
          ]
         }
        ],
        [
         "Joseph",
         {
          "personal_info": [
           30,
           678132,
           "Mumbai",
           "Male",
           "16-061981",
           233000,
           "323 3rd Cross"
          ]
         }
        ],
        [
         "Dhanush",
         {
          "personal_info": [
           25,
           874132,
           "Delhi",
           "Male",
           "16-061981",
           9786000,
           "456 Maple Ave"
          ]
         }
        ],
        [
         "Sam",
         {
          "personal_info": [
           28,
           632132,
           "Ahmadabad",
           "Male",
           "16-061981",
           984567000,
           "189 Walaja"
          ]
         }
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Details",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"Age\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"Pin\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"City\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"Gender\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"DOB\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"Fees\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"Address\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]},\"valueContainsNull\":true}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Details\", MapType(StringType(), StructType([\n",
    "      StructField(\"Age\", IntegerType(), True),\n",
    "      StructField(\"Pin\", IntegerType(), True),\n",
    "      StructField(\"City\", StringType(), True),\n",
    "      StructField(\"Gender\", StringType(), True),\n",
    "      StructField(\"DOB\", StringType(), True),\n",
    "      StructField(\"Fees\", IntegerType(), True),\n",
    "      StructField(\"Experience\", IntegerType(), True),\n",
    "      StructField(\"Address\", StringType(), True)])), True)\n",
    "    ])\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (\"John\", {\"personal_info\": {\"Age\": 30, \"Pin\": 517132, \"City\": \"Bangalore\", \"Gender\": \"Male\", \"DOB\": \"16-061981\", \"Fees\": 200000, \"Experience\": 5, \"Address\": \"123 Main St\"}}),\n",
    "    (\"Jaswanth\", {\"personal_info\": {\"Age\": 25, \"Pin\": 527332, \"City\": \"Hyderabad\", \"Gender\": \"Male\", \"DOB\": \"16-061981\", \"Fees\": 250000, \"Experience\": 8, \"Address\": \"456 Maple Ave\"}}),\n",
    "    (\"Dinesh\", {\"personal_info\": {\"Age\": 28, \"Pin\": 537432, \"City\": \"Chennai\", \"Gender\": \"Male\", \"DOB\": \"16-061981\", \"Fees\": 350000, \"Experience\": 3, \"Address\": \"789 Elm St\"}}),\n",
    "    (\"Watson\", {\"personal_info\": {\"Age\": 30, \"Pin\": 557672, \"City\": \"Nasik\", \"Gender\": \"Male\", \"DOB\": \"16-061981\", \"Fees\": 3550000, \"Experience\": 6, \"Address\": \"451 27th Main\"}}),\n",
    "    (\"David\", {\"personal_info\": {\"Age\": 25, \"Pin\": 757132, \"City\": \"Cochin\", \"Gender\": \"Male\", \"DOB\": \"16-061981\", \"Fees\": 4555000, \"Experience\": 9, \"Address\": \"#401 Madiwala\"}}),\n",
    "    (\"Dravid\", {\"personal_info\": {\"Age\": 28, \"Pin\": 973132, \"City\": \"Amaravati\", \"Gender\": \"Male\", \"DOB\": \"16-061981\", \"Fees\": 6789000, \"Experience\": 12, \"Address\": \"789 Mumbai\"}}),\n",
    "    (\"Joseph\", {\"personal_info\": {\"Age\": 30, \"Pin\": 678132, \"City\": \"Mumbai\", \"Gender\": \"Male\", \"DOB\": \"16-061981\", \"Fees\": 233000, \"Experience\": 3, \"Address\": \"323 3rd Cross\"}}),\n",
    "    (\"Dhanush\", {\"personal_info\": {\"Age\": 25, \"Pin\": 874132, \"City\": \"Delhi\", \"Gender\": \"Male\", \"DOB\": \"16-061981\", \"Fees\": 9786000, \"Experience\": 10, \"Address\": \"456 Maple Ave\"}}),\n",
    "    (\"Sam\", {\"personal_info\": {\"Age\": 28, \"Pin\": 632132, \"City\": \"Ahmadabad\", \"Gender\": \"Male\", \"DOB\": \"16-061981\", \"Fees\": 984567000, \"Experience\": 11, \"Address\": \"189 Walaja\"}})\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df_nest_map = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Display the DataFrame\n",
    "display(df_nest_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e71d32d2-1030-49bb-869f-7574b0943fbb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Multi Nested MapType"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>ID</th><th>Profile</th></tr></thead><tbody><tr><td>1</td><td>Map(personal_info -> List(John, 30, Bangalore, Male, List(List(Python, 5), List(Spark, 3)), List(List(Python, 5), List(Spark, 3))))</td></tr><tr><td>2</td><td>Map(personal_info -> List(Kiran, 25, Hyderabad, Male, List(List(Java, 4), List(Kubernetes, 6)), List(List(Python, 5), List(Spark, 3))))</td></tr><tr><td>3</td><td>Map(personal_info -> List(Kishore, 45, Chennai, Male, List(List(PySpark, 6), List(Spark, 8)), List(List(Python, 5), List(Spark, 3))))</td></tr><tr><td>4</td><td>Map(personal_info -> List(Kashvi, 28, Nasik, Male, List(List(SQL, 8), List(Spark, 5)), List(List(Python, 5), List(Spark, 3))))</td></tr><tr><td>5</td><td>Map(personal_info -> List(Kamal, 39, Mumbai, Male, List(List(Devops, 2), List(Spark, 6)), List(List(Python, 5), List(Spark, 3))))</td></tr><tr><td>6</td><td>Map(personal_info -> List(Pratap, 49, Ahmadabad, Male, List(List(Databricks, 7), List(Spark, 7)), List(List(Python, 5), List(Spark, 3))))</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "1",
         {
          "personal_info": [
           "John",
           30,
           "Bangalore",
           "Male",
           [
            [
             "Python",
             5
            ],
            [
             "Spark",
             3
            ]
           ],
           [
            [
             "Python",
             5
            ],
            [
             "Spark",
             3
            ]
           ]
          ]
         }
        ],
        [
         "2",
         {
          "personal_info": [
           "Kiran",
           25,
           "Hyderabad",
           "Male",
           [
            [
             "Java",
             4
            ],
            [
             "Kubernetes",
             6
            ]
           ],
           [
            [
             "Python",
             5
            ],
            [
             "Spark",
             3
            ]
           ]
          ]
         }
        ],
        [
         "3",
         {
          "personal_info": [
           "Kishore",
           45,
           "Chennai",
           "Male",
           [
            [
             "PySpark",
             6
            ],
            [
             "Spark",
             8
            ]
           ],
           [
            [
             "Python",
             5
            ],
            [
             "Spark",
             3
            ]
           ]
          ]
         }
        ],
        [
         "4",
         {
          "personal_info": [
           "Kashvi",
           28,
           "Nasik",
           "Male",
           [
            [
             "SQL",
             8
            ],
            [
             "Spark",
             5
            ]
           ],
           [
            [
             "Python",
             5
            ],
            [
             "Spark",
             3
            ]
           ]
          ]
         }
        ],
        [
         "5",
         {
          "personal_info": [
           "Kamal",
           39,
           "Mumbai",
           "Male",
           [
            [
             "Devops",
             2
            ],
            [
             "Spark",
             6
            ]
           ],
           [
            [
             "Python",
             5
            ],
            [
             "Spark",
             3
            ]
           ]
          ]
         }
        ],
        [
         "6",
         {
          "personal_info": [
           "Pratap",
           49,
           "Ahmadabad",
           "Male",
           [
            [
             "Databricks",
             7
            ],
            [
             "Spark",
             7
            ]
           ],
           [
            [
             "Python",
             5
            ],
            [
             "Spark",
             3
            ]
           ]
          ]
         }
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "ID",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Profile",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"Name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"Age\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"City\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"Gender\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"Skills\",\"type\":{\"type\":\"array\",\"elementType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"SkillName\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ExperienceYears\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}}]},\"containsNull\":true},\"nullable\":true,\"metadata\":{}},{\"name\":\"Matrix\",\"type\":{\"type\":\"array\",\"elementType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"SkillName\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ExperienceYears\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}}]},\"containsNull\":true},\"nullable\":true,\"metadata\":{}}]},\"valueContainsNull\":true}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Corrected schema definition\n",
    "schema = StructType([\n",
    "    StructField(\"ID\", StringType(), True),\n",
    "    StructField(\"Profile\", MapType(\n",
    "        StringType(), \n",
    "        StructType([\n",
    "            StructField(\"Name\", StringType(), True),\n",
    "            StructField(\"Age\", IntegerType(), True),\n",
    "            StructField(\"City\", StringType(), True),\n",
    "            StructField(\"Gender\", StringType(), True),\n",
    "            StructField(\"Skills\", ArrayType(\n",
    "                StructType([\n",
    "                    StructField(\"SkillName\", StringType(), True),\n",
    "                    StructField(\"ExperienceYears\", IntegerType(), True)\n",
    "                ]), True), True),\n",
    "            StructField(\"Matrix\", ArrayType(\n",
    "                StructType([\n",
    "                    StructField(\"SkillName\", StringType(), True),\n",
    "                    StructField(\"ExperienceYears\", IntegerType(), True)\n",
    "                ]), True), True)\n",
    "        ]), True), True)\n",
    "])\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (\"1\", {\"personal_info\": {\"Name\": \"John\", \"Age\": 30, \"City\": \"Bangalore\", \"Gender\": \"Male\", \"Skills\": [{\"SkillName\": \"Python\", \"ExperienceYears\": 5}, {\"SkillName\": \"Spark\", \"ExperienceYears\": 3}], \"Matrix\": [{\"SkillName\": \"Python\", \"ExperienceYears\": 5}, {\"SkillName\": \"Spark\", \"ExperienceYears\": 3}]}}),\n",
    "    (\"2\", {\"personal_info\": {\"Name\": \"Kiran\", \"Age\": 25, \"City\": \"Hyderabad\", \"Gender\": \"Male\", \"Skills\": [{\"SkillName\": \"Java\", \"ExperienceYears\": 4}, {\"SkillName\": \"Kubernetes\", \"ExperienceYears\": 6}], \"Matrix\": [{\"SkillName\": \"Python\", \"ExperienceYears\": 5}, {\"SkillName\": \"Spark\", \"ExperienceYears\": 3}]}}),\n",
    "    (\"3\", {\"personal_info\": {\"Name\": \"Kishore\", \"Age\": 45, \"City\": \"Chennai\", \"Gender\": \"Male\", \"Skills\": [{\"SkillName\": \"PySpark\", \"ExperienceYears\": 6}, {\"SkillName\": \"Spark\", \"ExperienceYears\": 8}], \"Matrix\": [{\"SkillName\": \"Python\", \"ExperienceYears\": 5}, {\"SkillName\": \"Spark\", \"ExperienceYears\": 3}]}}),\n",
    "    (\"4\", {\"personal_info\": {\"Name\": \"Kashvi\", \"Age\": 28, \"City\": \"Nasik\", \"Gender\": \"Male\", \"Skills\": [{\"SkillName\": \"SQL\", \"ExperienceYears\": 8}, {\"SkillName\": \"Spark\", \"ExperienceYears\": 5}], \"Matrix\": [{\"SkillName\": \"Python\", \"ExperienceYears\": 5}, {\"SkillName\": \"Spark\", \"ExperienceYears\": 3}]}}),\n",
    "    (\"5\", {\"personal_info\": {\"Name\": \"Kamal\", \"Age\": 39, \"City\": \"Mumbai\", \"Gender\": \"Male\", \"Skills\": [{\"SkillName\": \"Devops\", \"ExperienceYears\": 2}, {\"SkillName\": \"Spark\", \"ExperienceYears\": 6}], \"Matrix\": [{\"SkillName\": \"Python\", \"ExperienceYears\": 5}, {\"SkillName\": \"Spark\", \"ExperienceYears\": 3}]}}),\n",
    "    (\"6\", {\"personal_info\": {\"Name\": \"Pratap\", \"Age\": 49, \"City\": \"Ahmadabad\", \"Gender\": \"Male\", \"Skills\": [{\"SkillName\": \"Databricks\", \"ExperienceYears\": 7}, {\"SkillName\": \"Spark\", \"ExperienceYears\": 7}], \"Matrix\": [{\"SkillName\": \"Python\", \"ExperienceYears\": 5}, {\"SkillName\": \"Spark\", \"ExperienceYears\": 3}]}})\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df_multi_nest = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Display the DataFrame\n",
    "display(df_multi_nest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9601b044-618e-4e66-9146-d5914970e1dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### **3) Creating a DataFrame using create_map**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59cc3b92-7097-4dc8-ad4d-7fac12f40f7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Name</th><th>City</th><th>key</th><th>mode</th><th>target</th><th>Properties</th></tr></thead><tbody><tr><td>Naresh</td><td>Bangalore</td><td>2</td><td>21</td><td>41</td><td>Map(Exp -> 5, Age -> 25, emp_id -> 768954)</td></tr><tr><td>Harish</td><td>Chennai</td><td>4</td><td>12</td><td>5</td><td>Map(Exp -> 2, Age -> 30, emp_id -> 768956)</td></tr><tr><td>Prem</td><td>Hyderabad</td><td>5</td><td>9</td><td>12</td><td>Map(Exp -> 8, Age -> 28, emp_id -> 798954)</td></tr><tr><td>Prabhav</td><td>kochin</td><td>7</td><td>12</td><td>4</td><td>Map(Exp -> 6, Age -> 35, emp_id -> 788956)</td></tr><tr><td>Hari</td><td>Nasik</td><td>8</td><td>51</td><td>35</td><td>Map(Exp -> 9, Age -> 21, emp_id -> 769954)</td></tr><tr><td>Druv</td><td>Delhi</td><td>12</td><td>15</td><td>12</td><td>Map(Exp -> 4, Age -> 36, emp_id -> 768946)</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Naresh",
         "Bangalore",
         2,
         21,
         41,
         {
          "Age": 25,
          "Exp": 5,
          "emp_id": 768954
         }
        ],
        [
         "Harish",
         "Chennai",
         4,
         12,
         5,
         {
          "Age": 30,
          "Exp": 2,
          "emp_id": 768956
         }
        ],
        [
         "Prem",
         "Hyderabad",
         5,
         9,
         12,
         {
          "Age": 28,
          "Exp": 8,
          "emp_id": 798954
         }
        ],
        [
         "Prabhav",
         "kochin",
         7,
         12,
         4,
         {
          "Age": 35,
          "Exp": 6,
          "emp_id": 788956
         }
        ],
        [
         "Hari",
         "Nasik",
         8,
         51,
         35,
         {
          "Age": 21,
          "Exp": 9,
          "emp_id": 769954
         }
        ],
        [
         "Druv",
         "Delhi",
         12,
         15,
         12,
         {
          "Age": 36,
          "Exp": 4,
          "emp_id": 768946
         }
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "City",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "key",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "mode",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "target",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "Properties",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"integer\",\"valueContainsNull\":true}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sample DataFrame with a StringType column containing JSON strings\n",
    "data = [(\"Naresh\", \"Bangalore\", 2, 21, 41, {\"Age\": 25, \"emp_id\": 768954, \"Exp\": 5}), \n",
    "        (\"Harish\", \"Chennai\", 4, 12, 5, {\"Age\": 30, \"emp_id\": 768956, \"Exp\": 2}),\n",
    "        (\"Prem\", \"Hyderabad\", 5, 9, 12, {\"Age\": 28, \"emp_id\": 798954, \"Exp\": 8}), \n",
    "        (\"Prabhav\", \"kochin\", 7, 12, 4, {\"Age\": 35, \"emp_id\": 788956, \"Exp\": 6}),\n",
    "        (\"Hari\", \"Nasik\", 8, 51, 35, {\"Age\": 21, \"emp_id\": 769954, \"Exp\": 9}), \n",
    "        (\"Druv\", \"Delhi\", 12, 15, 12, {\"Age\": 36, \"emp_id\": 768946, \"Exp\": 4}),\n",
    "        ]\n",
    "\n",
    "# Define the schema for the MapType column\n",
    "map_schema = StructType([\n",
    "  StructField(\"Name\", StringType(), True),\n",
    "  StructField(\"City\", StringType(), True),\n",
    "  StructField(\"key\", IntegerType(), True),\n",
    "  StructField(\"mode\", IntegerType(), True),\n",
    "  StructField(\"target\", IntegerType(), True),\n",
    "  StructField(\"Properties\", MapType(StringType(), IntegerType()))])\n",
    "\n",
    "# Convert the StringType column to a MapType column\n",
    "df_map_nest = spark.createDataFrame(data, map_schema)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "display(df_map_nest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c485c11a-515d-4778-982b-c657e988972f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Name</th><th>City</th><th>key</th><th>mode</th><th>target</th><th>Properties</th><th>audience</th><th>qualities</th></tr></thead><tbody><tr><td>Naresh</td><td>Bangalore</td><td>2</td><td>21</td><td>41</td><td>Map(Exp -> 5, Age -> 25, emp_id -> 768954)</td><td>List(2, 21, 41)</td><td>Map(tempo -> Naresh, energy -> Bangalore, liveness -> Bangalore, speechiness -> Bangalore, acousticness -> Naresh, danceability -> Bangalore, loudness -> Bangalore, instrumentalness -> Bangalore)</td></tr><tr><td>Harish</td><td>Chennai</td><td>4</td><td>12</td><td>5</td><td>Map(Exp -> 2, Age -> 30, emp_id -> 768956)</td><td>List(4, 12, 5)</td><td>Map(tempo -> Harish, energy -> Chennai, liveness -> Chennai, speechiness -> Chennai, acousticness -> Harish, danceability -> Chennai, loudness -> Chennai, instrumentalness -> Chennai)</td></tr><tr><td>Prem</td><td>Hyderabad</td><td>5</td><td>9</td><td>12</td><td>Map(Exp -> 8, Age -> 28, emp_id -> 798954)</td><td>List(5, 9, 12)</td><td>Map(tempo -> Prem, energy -> Hyderabad, liveness -> Hyderabad, speechiness -> Hyderabad, acousticness -> Prem, danceability -> Hyderabad, loudness -> Hyderabad, instrumentalness -> Hyderabad)</td></tr><tr><td>Prabhav</td><td>kochin</td><td>7</td><td>12</td><td>4</td><td>Map(Exp -> 6, Age -> 35, emp_id -> 788956)</td><td>List(7, 12, 4)</td><td>Map(tempo -> Prabhav, energy -> kochin, liveness -> kochin, speechiness -> kochin, acousticness -> Prabhav, danceability -> kochin, loudness -> kochin, instrumentalness -> kochin)</td></tr><tr><td>Hari</td><td>Nasik</td><td>8</td><td>51</td><td>35</td><td>Map(Exp -> 9, Age -> 21, emp_id -> 769954)</td><td>List(8, 51, 35)</td><td>Map(tempo -> Hari, energy -> Nasik, liveness -> Nasik, speechiness -> Nasik, acousticness -> Hari, danceability -> Nasik, loudness -> Nasik, instrumentalness -> Nasik)</td></tr><tr><td>Druv</td><td>Delhi</td><td>12</td><td>15</td><td>12</td><td>Map(Exp -> 4, Age -> 36, emp_id -> 768946)</td><td>List(12, 15, 12)</td><td>Map(tempo -> Druv, energy -> Delhi, liveness -> Delhi, speechiness -> Delhi, acousticness -> Druv, danceability -> Delhi, loudness -> Delhi, instrumentalness -> Delhi)</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Naresh",
         "Bangalore",
         2,
         21,
         41,
         {
          "Age": 25,
          "Exp": 5,
          "emp_id": 768954
         },
         [
          2,
          21,
          41
         ],
         {
          "acousticness": "Naresh",
          "danceability": "Bangalore",
          "energy": "Bangalore",
          "instrumentalness": "Bangalore",
          "liveness": "Bangalore",
          "loudness": "Bangalore",
          "speechiness": "Bangalore",
          "tempo": "Naresh"
         }
        ],
        [
         "Harish",
         "Chennai",
         4,
         12,
         5,
         {
          "Age": 30,
          "Exp": 2,
          "emp_id": 768956
         },
         [
          4,
          12,
          5
         ],
         {
          "acousticness": "Harish",
          "danceability": "Chennai",
          "energy": "Chennai",
          "instrumentalness": "Chennai",
          "liveness": "Chennai",
          "loudness": "Chennai",
          "speechiness": "Chennai",
          "tempo": "Harish"
         }
        ],
        [
         "Prem",
         "Hyderabad",
         5,
         9,
         12,
         {
          "Age": 28,
          "Exp": 8,
          "emp_id": 798954
         },
         [
          5,
          9,
          12
         ],
         {
          "acousticness": "Prem",
          "danceability": "Hyderabad",
          "energy": "Hyderabad",
          "instrumentalness": "Hyderabad",
          "liveness": "Hyderabad",
          "loudness": "Hyderabad",
          "speechiness": "Hyderabad",
          "tempo": "Prem"
         }
        ],
        [
         "Prabhav",
         "kochin",
         7,
         12,
         4,
         {
          "Age": 35,
          "Exp": 6,
          "emp_id": 788956
         },
         [
          7,
          12,
          4
         ],
         {
          "acousticness": "Prabhav",
          "danceability": "kochin",
          "energy": "kochin",
          "instrumentalness": "kochin",
          "liveness": "kochin",
          "loudness": "kochin",
          "speechiness": "kochin",
          "tempo": "Prabhav"
         }
        ],
        [
         "Hari",
         "Nasik",
         8,
         51,
         35,
         {
          "Age": 21,
          "Exp": 9,
          "emp_id": 769954
         },
         [
          8,
          51,
          35
         ],
         {
          "acousticness": "Hari",
          "danceability": "Nasik",
          "energy": "Nasik",
          "instrumentalness": "Nasik",
          "liveness": "Nasik",
          "loudness": "Nasik",
          "speechiness": "Nasik",
          "tempo": "Hari"
         }
        ],
        [
         "Druv",
         "Delhi",
         12,
         15,
         12,
         {
          "Age": 36,
          "Exp": 4,
          "emp_id": 768946
         },
         [
          12,
          15,
          12
         ],
         {
          "acousticness": "Druv",
          "danceability": "Delhi",
          "energy": "Delhi",
          "instrumentalness": "Delhi",
          "liveness": "Delhi",
          "loudness": "Delhi",
          "speechiness": "Delhi",
          "tempo": "Druv"
         }
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "City",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "key",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "mode",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "target",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "Properties",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"integer\",\"valueContainsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "audience",
         "type": "{\"type\":\"array\",\"elementType\":\"integer\",\"containsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "qualities",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_nest = df_map_nest.select(\"*\", f.array('key', 'mode', 'target').alias('audience'),\n",
    "                             f.create_map(\n",
    "                               f.lit('acousticness'), col('Name'), \n",
    "                               f.lit('danceability'), col('City'),\n",
    "                               f.lit('energy'), col('City'),\n",
    "                               f.lit('instrumentalness'), col('City'),\n",
    "                               f.lit('liveness'), col('City'),\n",
    "                               f.lit('loudness'), col('City'),\n",
    "                               f.lit('speechiness'), col('City'),\n",
    "                               f.lit('tempo'), col('Name')).alias('qualities'))\n",
    "\n",
    "display(df_nest)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "71_MapType Column in PySpark",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
