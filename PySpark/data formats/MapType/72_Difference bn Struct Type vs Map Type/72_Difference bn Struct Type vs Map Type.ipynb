{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d11ea15a-3367-4fdc-bfaf-99f081943806",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### **StructType**\n",
    "\n",
    "- Each row in the STRUCT column must have the **same keys**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb19bc01-9082-4ba3-9afe-b3c56d89bcb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, col, to_json\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import StringType, IntegerType, StructType, StructField, MapType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dbe5fcf-b1a8-409b-8268-09c0090f47dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Name: struct (nullable = true)\n |    |-- firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |    |-- age: integer (nullable = true)\n |    |-- experience: integer (nullable = true)\n |    |-- status: string (nullable = true)\n |-- city: string (nullable = true)\n |-- gender: string (nullable = true)\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Name</th><th>city</th><th>gender</th></tr></thead><tbody><tr><td>List(jagadish, null, Smith, 35, 5, buy)</td><td>chennai</td><td>M</td></tr><tr><td>List(Anand, Rose, , 30, 8, sell)</td><td>bangalore</td><td>M</td></tr><tr><td>List(Julia, , Williams, 25, 3, buy)</td><td>vizak</td><td>F</td></tr><tr><td>List(Mukesh, Bhat, Royal, 45, 8, buy)</td><td>madurai</td><td>M</td></tr><tr><td>List(Swetha, Kumari, Anand, 55, 15, sell)</td><td>mysore</td><td>F</td></tr><tr><td>List(Madan, Mohan, Nair, 22, 11, buy)</td><td>hyderabad</td><td>M</td></tr><tr><td>List(George, , Williams, 38, 7, sell)</td><td>London</td><td>M</td></tr><tr><td>List(Roshan, Bhat, , 41, 3, buy)</td><td>mandya</td><td>M</td></tr><tr><td>List(Sourabh, Sharma, , 27, 2, sell)</td><td>Nasik</td><td>M</td></tr><tr><td>List(Mohan, Rao, K, 42, 7, buy)</td><td>nizamabad</td><td>M</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         [
          "jagadish",
          null,
          "Smith",
          35,
          5,
          "buy"
         ],
         "chennai",
         "M"
        ],
        [
         [
          "Anand",
          "Rose",
          "",
          30,
          8,
          "sell"
         ],
         "bangalore",
         "M"
        ],
        [
         [
          "Julia",
          "",
          "Williams",
          25,
          3,
          "buy"
         ],
         "vizak",
         "F"
        ],
        [
         [
          "Mukesh",
          "Bhat",
          "Royal",
          45,
          8,
          "buy"
         ],
         "madurai",
         "M"
        ],
        [
         [
          "Swetha",
          "Kumari",
          "Anand",
          55,
          15,
          "sell"
         ],
         "mysore",
         "F"
        ],
        [
         [
          "Madan",
          "Mohan",
          "Nair",
          22,
          11,
          "buy"
         ],
         "hyderabad",
         "M"
        ],
        [
         [
          "George",
          "",
          "Williams",
          38,
          7,
          "sell"
         ],
         "London",
         "M"
        ],
        [
         [
          "Roshan",
          "Bhat",
          "",
          41,
          3,
          "buy"
         ],
         "mandya",
         "M"
        ],
        [
         [
          "Sourabh",
          "Sharma",
          "",
          27,
          2,
          "sell"
         ],
         "Nasik",
         "M"
        ],
        [
         [
          "Mohan",
          "Rao",
          "K",
          42,
          7,
          "buy"
         ],
         "nizamabad",
         "M"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Name",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"firstname\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"middlename\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"lastname\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"age\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"experience\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"status\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}"
        },
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "gender",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data\n",
    "data = [\n",
    "        ((\"jagadish\", None, \"Smith\", 35, 5, \"buy\"),\"chennai\",\"M\"),\n",
    "        ((\"Anand\", \"Rose\", \"\", 30, 8, \"sell\"), \"bangalore\", \"M\"),\n",
    "        ((\"Julia\", \"\", \"Williams\", 25, 3, \"buy\"), \"vizak\", \"F\"),\n",
    "        ((\"Mukesh\", \"Bhat\", \"Royal\", 45, 8, \"buy\"), \"madurai\", \"M\"),\n",
    "        ((\"Swetha\", \"Kumari\", \"Anand\", 55, 15, \"sell\"), \"mysore\", \"F\"),\n",
    "        ((\"Madan\", \"Mohan\", \"Nair\", 22, 11, \"buy\"), \"hyderabad\", \"M\"),\n",
    "        ((\"George\", \"\", \"Williams\", 38, 7, \"sell\"), \"London\", \"M\"),\n",
    "        ((\"Roshan\", \"Bhat\", \"\", 41, 3, \"buy\"), \"mandya\", \"M\"),\n",
    "        ((\"Sourabh\", \"Sharma\", \"\", 27, 2, \"sell\"), \"Nasik\", \"M\"),\n",
    "        ((\"Mohan\", \"Rao\", \"K\", 42, 7, \"buy\"), \"nizamabad\", \"M\")\n",
    "        ]\n",
    "\n",
    "# Schema\n",
    "schema_arr = StructType([\n",
    "    StructField('Name', StructType([\n",
    "         StructField('firstname', StringType(), True),\n",
    "         StructField('middlename', StringType(), True),\n",
    "         StructField('lastname', StringType(), True),\n",
    "         StructField('age', IntegerType(), True),\n",
    "         StructField('experience', IntegerType(), True),\n",
    "         StructField('status', StringType(), True)\n",
    "         ])),\n",
    "     StructField('city', StringType(), True),\n",
    "     StructField('gender', StringType(), True)\n",
    "     ])\n",
    "\n",
    "# Create DataFrame\n",
    "df_arr = spark.createDataFrame(data=data, schema=schema_arr)\n",
    "df_arr.printSchema()\n",
    "display(df_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eab25ced-6eb5-4e5c-9edc-56f36f67c2f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Each row in the STRUCT column must have the same keys**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b54c15e-016e-40fa-a5a0-9bc04d288a17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPySparkValueError\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2711746228538018>, line 30\u001B[0m\n",
       "\u001B[1;32m     16\u001B[0m schema_arr \u001B[38;5;241m=\u001B[39m StructType([\n",
       "\u001B[1;32m     17\u001B[0m     StructField(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mName\u001B[39m\u001B[38;5;124m'\u001B[39m, StructType([\n",
       "\u001B[1;32m     18\u001B[0m          StructField(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfirstname\u001B[39m\u001B[38;5;124m'\u001B[39m, StringType(), \u001B[38;5;28;01mTrue\u001B[39;00m),\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m     26\u001B[0m      StructField(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgender\u001B[39m\u001B[38;5;124m'\u001B[39m, StringType(), \u001B[38;5;28;01mTrue\u001B[39;00m)\n",
       "\u001B[1;32m     27\u001B[0m      ])\n",
       "\u001B[1;32m     29\u001B[0m \u001B[38;5;66;03m# Create DataFrame\u001B[39;00m\n",
       "\u001B[0;32m---> 30\u001B[0m df_arr \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mcreateDataFrame(data\u001B[38;5;241m=\u001B[39mdata, schema\u001B[38;5;241m=\u001B[39mschema_arr)\n",
       "\u001B[1;32m     31\u001B[0m df_arr\u001B[38;5;241m.\u001B[39mprintSchema()\n",
       "\u001B[1;32m     32\u001B[0m display(df_arr)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     50\u001B[0m     )\n",
       "\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1605\u001B[0m, in \u001B[0;36mSparkSession.createDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n",
       "\u001B[1;32m   1600\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_pandas \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, pd\u001B[38;5;241m.\u001B[39mDataFrame):\n",
       "\u001B[1;32m   1601\u001B[0m     \u001B[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001B[39;00m\n",
       "\u001B[1;32m   1602\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m(SparkSession, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39mcreateDataFrame(  \u001B[38;5;66;03m# type: ignore[call-overload]\u001B[39;00m\n",
       "\u001B[1;32m   1603\u001B[0m         data, schema, samplingRatio, verifySchema\n",
       "\u001B[1;32m   1604\u001B[0m     )\n",
       "\u001B[0;32m-> 1605\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_dataframe(\n",
       "\u001B[1;32m   1606\u001B[0m     data, schema, samplingRatio, verifySchema  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n",
       "\u001B[1;32m   1607\u001B[0m )\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1662\u001B[0m, in \u001B[0;36mSparkSession._create_dataframe\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n",
       "\u001B[1;32m   1660\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_createFromRDD(data\u001B[38;5;241m.\u001B[39mmap(prepare), schema, samplingRatio)\n",
       "\u001B[1;32m   1661\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m-> 1662\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_createFromLocal(\u001B[38;5;28mmap\u001B[39m(prepare, data), schema)\n",
       "\u001B[1;32m   1663\u001B[0m jrdd \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mSerDeUtil\u001B[38;5;241m.\u001B[39mtoJavaArray(rdd\u001B[38;5;241m.\u001B[39m_to_java_object_rdd())\n",
       "\u001B[1;32m   1664\u001B[0m jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsparkSession\u001B[38;5;241m.\u001B[39mapplySchemaToPythonRDD(jrdd\u001B[38;5;241m.\u001B[39mrdd(), struct\u001B[38;5;241m.\u001B[39mjson())\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1229\u001B[0m, in \u001B[0;36mSparkSession._createFromLocal\u001B[0;34m(self, data, schema)\u001B[0m\n",
       "\u001B[1;32m   1221\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_createFromLocal\u001B[39m(\n",
       "\u001B[1;32m   1222\u001B[0m     \u001B[38;5;28mself\u001B[39m, data: Iterable[Any], schema: Optional[Union[DataType, List[\u001B[38;5;28mstr\u001B[39m]]]\n",
       "\u001B[1;32m   1223\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRDD[Tuple]\u001B[39m\u001B[38;5;124m\"\u001B[39m, StructType]:\n",
       "\u001B[1;32m   1224\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m   1225\u001B[0m \u001B[38;5;124;03m    Create an RDD for DataFrame from a list or pandas.DataFrame, returns the RDD and schema.\u001B[39;00m\n",
       "\u001B[1;32m   1226\u001B[0m \u001B[38;5;124;03m    This would be broken with table acl enabled as user process does not have permission to\u001B[39;00m\n",
       "\u001B[1;32m   1227\u001B[0m \u001B[38;5;124;03m    write temp files.\u001B[39;00m\n",
       "\u001B[1;32m   1228\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m-> 1229\u001B[0m     internal_data, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wrap_data_schema(data, schema)\n",
       "\u001B[1;32m   1230\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sc\u001B[38;5;241m.\u001B[39mparallelize(internal_data), struct\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1193\u001B[0m, in \u001B[0;36mSparkSession._wrap_data_schema\u001B[0;34m(self, data, schema)\u001B[0m\n",
       "\u001B[1;32m   1188\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_wrap_data_schema\u001B[39m(\n",
       "\u001B[1;32m   1189\u001B[0m     \u001B[38;5;28mself\u001B[39m, data: Iterable[Any], schema: Optional[Union[DataType, List[\u001B[38;5;28mstr\u001B[39m]]]\n",
       "\u001B[1;32m   1190\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[Iterable[Tuple], StructType]:\n",
       "\u001B[1;32m   1191\u001B[0m     \u001B[38;5;66;03m# make sure data could consumed multiple times\u001B[39;00m\n",
       "\u001B[1;32m   1192\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, \u001B[38;5;28mlist\u001B[39m):\n",
       "\u001B[0;32m-> 1193\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(data)\n",
       "\u001B[1;32m   1195\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m schema \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(schema, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n",
       "\u001B[1;32m   1196\u001B[0m         struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inferSchemaFromList(data, names\u001B[38;5;241m=\u001B[39mschema)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1621\u001B[0m, in \u001B[0;36mSparkSession._create_dataframe.<locals>.prepare\u001B[0;34m(obj)\u001B[0m\n",
       "\u001B[1;32m   1619\u001B[0m \u001B[38;5;129m@no_type_check\u001B[39m\n",
       "\u001B[1;32m   1620\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mprepare\u001B[39m(obj):\n",
       "\u001B[0;32m-> 1621\u001B[0m     verify_func(obj)\n",
       "\u001B[1;32m   1622\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:2913\u001B[0m, in \u001B[0;36m_make_type_verifier.<locals>.verify\u001B[0;34m(obj)\u001B[0m\n",
       "\u001B[1;32m   2911\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mverify\u001B[39m(obj: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m   2912\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m verify_nullability(obj):\n",
       "\u001B[0;32m-> 2913\u001B[0m         verify_value(obj)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:2868\u001B[0m, in \u001B[0;36m_make_type_verifier.<locals>.verify_struct\u001B[0;34m(obj)\u001B[0m\n",
       "\u001B[1;32m   2860\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m PySparkValueError(\n",
       "\u001B[1;32m   2861\u001B[0m             error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFIELD_STRUCT_LENGTH_MISMATCH\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m   2862\u001B[0m             message_parameters\u001B[38;5;241m=\u001B[39m{\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   2865\u001B[0m             },\n",
       "\u001B[1;32m   2866\u001B[0m         )\n",
       "\u001B[1;32m   2867\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m v, (_, verifier) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(obj, verifiers):\n",
       "\u001B[0;32m-> 2868\u001B[0m         verifier(v)\n",
       "\u001B[1;32m   2869\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(obj, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__dict__\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\u001B[1;32m   2870\u001B[0m     d \u001B[38;5;241m=\u001B[39m obj\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__dict__\u001B[39m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:2913\u001B[0m, in \u001B[0;36m_make_type_verifier.<locals>.verify\u001B[0;34m(obj)\u001B[0m\n",
       "\u001B[1;32m   2911\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mverify\u001B[39m(obj: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m   2912\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m verify_nullability(obj):\n",
       "\u001B[0;32m-> 2913\u001B[0m         verify_value(obj)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:2852\u001B[0m, in \u001B[0;36m_make_type_verifier.<locals>.verify_struct\u001B[0;34m(obj)\u001B[0m\n",
       "\u001B[1;32m   2850\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(obj) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(verifiers):\n",
       "\u001B[1;32m   2851\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[0;32m-> 2852\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m PySparkValueError(\n",
       "\u001B[1;32m   2853\u001B[0m             error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFIELD_STRUCT_LENGTH_MISMATCH_WITH_NAME\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m   2854\u001B[0m             message_parameters\u001B[38;5;241m=\u001B[39m{\n",
       "\u001B[1;32m   2855\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfield_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mstr\u001B[39m(name),\n",
       "\u001B[1;32m   2856\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mobject_length\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mlen\u001B[39m(obj)),\n",
       "\u001B[1;32m   2857\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfield_length\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mlen\u001B[39m(verifiers)),\n",
       "\u001B[1;32m   2858\u001B[0m             },\n",
       "\u001B[1;32m   2859\u001B[0m         )\n",
       "\u001B[1;32m   2860\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkValueError(\n",
       "\u001B[1;32m   2861\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFIELD_STRUCT_LENGTH_MISMATCH\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m   2862\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   2865\u001B[0m         },\n",
       "\u001B[1;32m   2866\u001B[0m     )\n",
       "\u001B[1;32m   2867\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m v, (_, verifier) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(obj, verifiers):\n",
       "\n",
       "\u001B[0;31mPySparkValueError\u001B[0m: [FIELD_STRUCT_LENGTH_MISMATCH_WITH_NAME] field Name: Length of object (5) does not match with length of fields (6)."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "PySparkValueError",
        "evalue": "[FIELD_STRUCT_LENGTH_MISMATCH_WITH_NAME] field Name: Length of object (5) does not match with length of fields (6)."
       },
       "metadata": {
        "errorSummary": "[FIELD_STRUCT_LENGTH_MISMATCH_WITH_NAME] field Name: Length of object (5) does not match with length of fields (6)."
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "FIELD_STRUCT_LENGTH_MISMATCH_WITH_NAME",
        "pysparkCallSite": null,
        "pysparkFragment": null,
        "sqlState": null,
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mPySparkValueError\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m<command-2711746228538018>, line 30\u001B[0m\n\u001B[1;32m     16\u001B[0m schema_arr \u001B[38;5;241m=\u001B[39m StructType([\n\u001B[1;32m     17\u001B[0m     StructField(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mName\u001B[39m\u001B[38;5;124m'\u001B[39m, StructType([\n\u001B[1;32m     18\u001B[0m          StructField(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfirstname\u001B[39m\u001B[38;5;124m'\u001B[39m, StringType(), \u001B[38;5;28;01mTrue\u001B[39;00m),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     26\u001B[0m      StructField(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgender\u001B[39m\u001B[38;5;124m'\u001B[39m, StringType(), \u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     27\u001B[0m      ])\n\u001B[1;32m     29\u001B[0m \u001B[38;5;66;03m# Create DataFrame\u001B[39;00m\n\u001B[0;32m---> 30\u001B[0m df_arr \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mcreateDataFrame(data\u001B[38;5;241m=\u001B[39mdata, schema\u001B[38;5;241m=\u001B[39mschema_arr)\n\u001B[1;32m     31\u001B[0m df_arr\u001B[38;5;241m.\u001B[39mprintSchema()\n\u001B[1;32m     32\u001B[0m display(df_arr)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     50\u001B[0m     )\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1605\u001B[0m, in \u001B[0;36mSparkSession.createDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m   1600\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_pandas \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, pd\u001B[38;5;241m.\u001B[39mDataFrame):\n\u001B[1;32m   1601\u001B[0m     \u001B[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001B[39;00m\n\u001B[1;32m   1602\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m(SparkSession, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39mcreateDataFrame(  \u001B[38;5;66;03m# type: ignore[call-overload]\u001B[39;00m\n\u001B[1;32m   1603\u001B[0m         data, schema, samplingRatio, verifySchema\n\u001B[1;32m   1604\u001B[0m     )\n\u001B[0;32m-> 1605\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_dataframe(\n\u001B[1;32m   1606\u001B[0m     data, schema, samplingRatio, verifySchema  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[1;32m   1607\u001B[0m )\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1662\u001B[0m, in \u001B[0;36mSparkSession._create_dataframe\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m   1660\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_createFromRDD(data\u001B[38;5;241m.\u001B[39mmap(prepare), schema, samplingRatio)\n\u001B[1;32m   1661\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1662\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_createFromLocal(\u001B[38;5;28mmap\u001B[39m(prepare, data), schema)\n\u001B[1;32m   1663\u001B[0m jrdd \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mSerDeUtil\u001B[38;5;241m.\u001B[39mtoJavaArray(rdd\u001B[38;5;241m.\u001B[39m_to_java_object_rdd())\n\u001B[1;32m   1664\u001B[0m jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsparkSession\u001B[38;5;241m.\u001B[39mapplySchemaToPythonRDD(jrdd\u001B[38;5;241m.\u001B[39mrdd(), struct\u001B[38;5;241m.\u001B[39mjson())\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1229\u001B[0m, in \u001B[0;36mSparkSession._createFromLocal\u001B[0;34m(self, data, schema)\u001B[0m\n\u001B[1;32m   1221\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_createFromLocal\u001B[39m(\n\u001B[1;32m   1222\u001B[0m     \u001B[38;5;28mself\u001B[39m, data: Iterable[Any], schema: Optional[Union[DataType, List[\u001B[38;5;28mstr\u001B[39m]]]\n\u001B[1;32m   1223\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRDD[Tuple]\u001B[39m\u001B[38;5;124m\"\u001B[39m, StructType]:\n\u001B[1;32m   1224\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1225\u001B[0m \u001B[38;5;124;03m    Create an RDD for DataFrame from a list or pandas.DataFrame, returns the RDD and schema.\u001B[39;00m\n\u001B[1;32m   1226\u001B[0m \u001B[38;5;124;03m    This would be broken with table acl enabled as user process does not have permission to\u001B[39;00m\n\u001B[1;32m   1227\u001B[0m \u001B[38;5;124;03m    write temp files.\u001B[39;00m\n\u001B[1;32m   1228\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1229\u001B[0m     internal_data, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wrap_data_schema(data, schema)\n\u001B[1;32m   1230\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sc\u001B[38;5;241m.\u001B[39mparallelize(internal_data), struct\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1193\u001B[0m, in \u001B[0;36mSparkSession._wrap_data_schema\u001B[0;34m(self, data, schema)\u001B[0m\n\u001B[1;32m   1188\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_wrap_data_schema\u001B[39m(\n\u001B[1;32m   1189\u001B[0m     \u001B[38;5;28mself\u001B[39m, data: Iterable[Any], schema: Optional[Union[DataType, List[\u001B[38;5;28mstr\u001B[39m]]]\n\u001B[1;32m   1190\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[Iterable[Tuple], StructType]:\n\u001B[1;32m   1191\u001B[0m     \u001B[38;5;66;03m# make sure data could consumed multiple times\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, \u001B[38;5;28mlist\u001B[39m):\n\u001B[0;32m-> 1193\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(data)\n\u001B[1;32m   1195\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m schema \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(schema, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n\u001B[1;32m   1196\u001B[0m         struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inferSchemaFromList(data, names\u001B[38;5;241m=\u001B[39mschema)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1621\u001B[0m, in \u001B[0;36mSparkSession._create_dataframe.<locals>.prepare\u001B[0;34m(obj)\u001B[0m\n\u001B[1;32m   1619\u001B[0m \u001B[38;5;129m@no_type_check\u001B[39m\n\u001B[1;32m   1620\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mprepare\u001B[39m(obj):\n\u001B[0;32m-> 1621\u001B[0m     verify_func(obj)\n\u001B[1;32m   1622\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:2913\u001B[0m, in \u001B[0;36m_make_type_verifier.<locals>.verify\u001B[0;34m(obj)\u001B[0m\n\u001B[1;32m   2911\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mverify\u001B[39m(obj: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   2912\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m verify_nullability(obj):\n\u001B[0;32m-> 2913\u001B[0m         verify_value(obj)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:2868\u001B[0m, in \u001B[0;36m_make_type_verifier.<locals>.verify_struct\u001B[0;34m(obj)\u001B[0m\n\u001B[1;32m   2860\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m PySparkValueError(\n\u001B[1;32m   2861\u001B[0m             error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFIELD_STRUCT_LENGTH_MISMATCH\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   2862\u001B[0m             message_parameters\u001B[38;5;241m=\u001B[39m{\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2865\u001B[0m             },\n\u001B[1;32m   2866\u001B[0m         )\n\u001B[1;32m   2867\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m v, (_, verifier) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(obj, verifiers):\n\u001B[0;32m-> 2868\u001B[0m         verifier(v)\n\u001B[1;32m   2869\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(obj, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__dict__\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m   2870\u001B[0m     d \u001B[38;5;241m=\u001B[39m obj\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__dict__\u001B[39m\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:2913\u001B[0m, in \u001B[0;36m_make_type_verifier.<locals>.verify\u001B[0;34m(obj)\u001B[0m\n\u001B[1;32m   2911\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mverify\u001B[39m(obj: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   2912\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m verify_nullability(obj):\n\u001B[0;32m-> 2913\u001B[0m         verify_value(obj)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:2852\u001B[0m, in \u001B[0;36m_make_type_verifier.<locals>.verify_struct\u001B[0;34m(obj)\u001B[0m\n\u001B[1;32m   2850\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(obj) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(verifiers):\n\u001B[1;32m   2851\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 2852\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m PySparkValueError(\n\u001B[1;32m   2853\u001B[0m             error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFIELD_STRUCT_LENGTH_MISMATCH_WITH_NAME\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   2854\u001B[0m             message_parameters\u001B[38;5;241m=\u001B[39m{\n\u001B[1;32m   2855\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfield_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mstr\u001B[39m(name),\n\u001B[1;32m   2856\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mobject_length\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mlen\u001B[39m(obj)),\n\u001B[1;32m   2857\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfield_length\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mlen\u001B[39m(verifiers)),\n\u001B[1;32m   2858\u001B[0m             },\n\u001B[1;32m   2859\u001B[0m         )\n\u001B[1;32m   2860\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkValueError(\n\u001B[1;32m   2861\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFIELD_STRUCT_LENGTH_MISMATCH\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   2862\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2865\u001B[0m         },\n\u001B[1;32m   2866\u001B[0m     )\n\u001B[1;32m   2867\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m v, (_, verifier) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(obj, verifiers):\n",
        "\u001B[0;31mPySparkValueError\u001B[0m: [FIELD_STRUCT_LENGTH_MISMATCH_WITH_NAME] field Name: Length of object (5) does not match with length of fields (6)."
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data\n",
    "data = [\n",
    "        ((\"jagadish\", \"Smith\", 35, 5, \"buy\"),\"chennai\",\"M\"),\n",
    "        ((\"Anand\", \"Rose\", \"\", 30, 8, \"sell\"), \"bangalore\", \"M\"),\n",
    "        ((\"Julia\", \"\", \"Williams\", 25, 3, \"buy\"), \"vizak\", \"F\"),\n",
    "        ((\"Mukesh\", \"Bhat\", \"Royal\", 45, 8, \"buy\"), \"madurai\", \"M\"),\n",
    "        ((\"Swetha\", \"Kumari\", \"Anand\", 55, 15, \"sell\"), \"mysore\", \"F\"),\n",
    "        ((\"Madan\", \"Mohan\", \"Nair\", 22, 11, \"buy\"), \"hyderabad\", \"M\"),\n",
    "        ((\"George\", \"\", \"Williams\", 38, 7, \"sell\"), \"London\", \"M\"),\n",
    "        ((\"Roshan\", \"Bhat\", \"\", 41, 3, \"buy\"), \"mandya\", \"M\"),\n",
    "        ((\"Sourabh\", \"Sharma\", \"\", 27, 2, \"sell\"), \"Nasik\", \"M\"),\n",
    "        ((\"Mohan\", \"Rao\", \"K\", 42, 7, \"buy\"), \"nizamabad\", \"M\")\n",
    "        ]\n",
    "\n",
    "# Schema\n",
    "schema_arr = StructType([\n",
    "    StructField('Name', StructType([\n",
    "         StructField('firstname', StringType(), True),\n",
    "         StructField('middlename', StringType(), True),\n",
    "         StructField('lastname', StringType(), True),\n",
    "         StructField('age', IntegerType(), True),\n",
    "         StructField('experience', IntegerType(), True),\n",
    "         StructField('status', StringType(), True)\n",
    "         ])),\n",
    "     StructField('city', StringType(), True),\n",
    "     StructField('gender', StringType(), True)\n",
    "     ])\n",
    "\n",
    "# Create DataFrame\n",
    "df_arr = spark.createDataFrame(data=data, schema=schema_arr)\n",
    "df_arr.printSchema()\n",
    "display(df_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dcbf9ed0-eedc-4a75-ab7c-afff6ebec2b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### **MapType**\n",
    "\n",
    "- **MapType** column is used to represent **key-value** pair data.\n",
    "\n",
    "      you want to store data for a person in key-value data structure\n",
    "      {\"name\": \"John\", \"age\": 29, \"emp_id\": \"123657\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b46a80e-1bfb-4f95-8d8b-3eea43833b78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Name</th><th>City</th><th>Properties</th></tr></thead><tbody><tr><td>Naresh</td><td>Bangalore</td><td>Map(Company -> TCS, Mode -> Transport, Branch -> IT, Designation -> DE, Domain -> Gas)</td></tr><tr><td>Harish</td><td>Chennai</td><td>Map(Designation -> DE, Company -> Sony, Domain -> DS, Branch -> CSC)</td></tr><tr><td>Prem</td><td>Hyderabad</td><td>Map(Designation -> DE, Domain -> Trade, Branch -> EEE)</td></tr><tr><td>Prabhav</td><td>kochin</td><td>Map(Domain -> Sales, Branch -> AI)</td></tr><tr><td>Hari</td><td>Nasik</td><td>Map(Domain -> TELE)</td></tr><tr><td>Druv</td><td>Delhi</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Naresh",
         "Bangalore",
         {
          "Branch": "IT",
          "Company": "TCS",
          "Designation": "DE",
          "Domain": "Gas",
          "Mode": "Transport"
         }
        ],
        [
         "Harish",
         "Chennai",
         {
          "Branch": "CSC",
          "Company": "Sony",
          "Designation": "DE",
          "Domain": "DS"
         }
        ],
        [
         "Prem",
         "Hyderabad",
         {
          "Branch": "EEE",
          "Designation": "DE",
          "Domain": "Trade"
         }
        ],
        [
         "Prabhav",
         "kochin",
         {
          "Branch": "AI",
          "Domain": "Sales"
         }
        ],
        [
         "Hari",
         "Nasik",
         {
          "Domain": "TELE"
         }
        ],
        [
         "Druv",
         "Delhi",
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "City",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Properties",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sample DataFrame with a StringType column containing JSON strings\n",
    "data = [(\"Naresh\", \"Bangalore\", {\"Domain\": \"Gas\", \"Branch\": \"IT\", \"Designation\": \"DE\", \"Company\": \"TCS\", \"Mode\": \"Transport\"}), \n",
    "        (\"Harish\", \"Chennai\", {\"Domain\": \"DS\", \"Branch\": \"CSC\", \"Designation\": \"DE\", \"Company\": \"Sony\"}),\n",
    "        (\"Prem\", \"Hyderabad\", {\"Domain\": \"Trade\", \"Branch\": \"EEE\", \"Designation\": \"DE\"}), \n",
    "        (\"Prabhav\", \"kochin\", {\"Domain\": \"Sales\", \"Branch\": \"AI\"}),\n",
    "        (\"Hari\", \"Nasik\", {\"Domain\": \"TELE\"}), \n",
    "        (\"Druv\", \"Delhi\", None),\n",
    "        ]\n",
    "\n",
    "# Define the schema for the MapType column\n",
    "map_schema = StructType([\n",
    "  StructField(\"Name\", StringType(), True),\n",
    "  StructField(\"City\", StringType(), True),\n",
    "  StructField(\"Properties\", MapType(StringType(), StringType()), True)])\n",
    "\n",
    "# Convert the StringType column to a MapType column\n",
    "df_map = spark.createDataFrame(data, map_schema)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "display(df_map)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "72_Difference bn Struct Type vs Map Type",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}