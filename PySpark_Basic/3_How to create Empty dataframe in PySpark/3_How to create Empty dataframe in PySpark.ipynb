{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46accc67-d773-4919-99d0-628a19d293b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16ef1f6c-295c-458e-9932-ebaba73ce599",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**1) Create Empty DataFrame without Schema (no columns)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "390c075e-0738-4d5e-8b03-0d2a3565b692",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n\n"
     ]
    }
   ],
   "source": [
    "# Create empty DatFrame with no schema (no columns)\n",
    "df3 = spark.createDataFrame([], StructType([]))\n",
    "display(df3)\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9779a4c1-d699-42da-b17f-83a41de21ff8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**2) Create Empty DataFrame with Schema**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28cb048b-b280-49f8-9a6f-a2c0e08d0858",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create Schema\n",
    "schema = StructType([\n",
    "  StructField('FirstName', StringType(), True),\n",
    "  StructField('Age', IntegerType(), True),\n",
    "  StructField('Experience', IntegerType(), True),\n",
    "  StructField('Label_Type', StringType(), True),\n",
    "  StructField('Last_transaction_date', StringType(), True),\n",
    "  StructField('last_timestamp', StringType(), True),\n",
    "  StructField('Sensex_Category', StringType(), True)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6038e0eb-f0bb-44af-aea2-70cfe8ff0300",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>FirstName</th><th>Age</th><th>Experience</th><th>Label_Type</th><th>Last_transaction_date</th><th>last_timestamp</th><th>Sensex_Category</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "FirstName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Age",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "Experience",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "Label_Type",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Last_transaction_date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "last_timestamp",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Sensex_Category",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create empty DataFrame directly.\n",
    "emp_df = spark.createDataFrame([], schema)\n",
    "display(emp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "969e9506-544e-4c4c-98bf-c76a724e9f50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**3) Creating empty DataFrame with NULL placeholders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cc0da18-f116-433c-8988-7e0ed1865484",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4378187-dbb0-4333-8c47-f13514116769",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>Name</th><th>Age</th><th>Sales</th></tr></thead><tbody><tr><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>null</td><td>null</td><td>null</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         null,
         null,
         null,
         null
        ],
        [
         null,
         null,
         null,
         null
        ],
        [
         null,
         null,
         null,
         null
        ],
        [
         null,
         null,
         null,
         null
        ],
        [
         null,
         null,
         null,
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Age",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "Sales",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Age\", IntegerType(), True),\n",
    "    StructField(\"Sales\", IntegerType(), True),\n",
    "])\n",
    "\n",
    "# Just placeholders with all nulls\n",
    "empty_data = [(None, None, None, None),\n",
    "              (None, None, None, None),\n",
    "              (None, None, None, None),\n",
    "              (None, None, None, None),\n",
    "              (None, None, None, None)]\n",
    "              \n",
    "df_null_placeholder = spark.createDataFrame(empty_data, schema)\n",
    "display(df_null_placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e09096c5-8b8c-4c45-a9f9-a4f74bef8a13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>Name</th><th>Age</th><th>Sales</th></tr></thead><tbody><tr><td>1</td><td>null</td><td>null</td><td>null</td></tr><tr><td>2</td><td>Neol</td><td>25</td><td>150</td></tr><tr><td>3</td><td>null</td><td>null</td><td>null</td></tr><tr><td>4</td><td>Niroop</td><td>35</td><td>350</td></tr><tr><td>5</td><td>null</td><td>null</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         null,
         null,
         null
        ],
        [
         2,
         "Neol",
         25,
         150
        ],
        [
         3,
         null,
         null,
         null
        ],
        [
         4,
         "Niroop",
         35,
         350
        ],
        [
         5,
         null,
         null,
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Age",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "Sales",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Age\", IntegerType(), True),\n",
    "    StructField(\"Sales\", IntegerType(), True),\n",
    "])\n",
    "\n",
    "# Just placeholders with all nulls\n",
    "empty_data = [(1, None, None, None),\n",
    "              (2, \"Neol\", 25, 150),\n",
    "              (3, None, None, None),\n",
    "              (4, \"Niroop\", 35, 350),\n",
    "              (5, None, None, None)]\n",
    "              \n",
    "df_null_placeholder = spark.createDataFrame(empty_data, schema)\n",
    "display(df_null_placeholder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "680a6cf4-a7e1-4222-9707-880cf0a469ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 3) How to union existing dataframe to empty dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2986c5d-1d2f-4f54-bbcc-1b7643b3e4d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⚠️ Things to Avoid**\n",
    "\n",
    "- **Different schemas:** Will throw an error.\n",
    "- **Mismatched column names or types:** Columns must match exactly in name, type, and order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a163df4-5473-4e9c-b1cb-0f536251ce05",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "empty dataframe"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>FirstName</th><th>Age</th><th>Experience</th><th>Label_Type</th><th>Last_transaction_date</th><th>last_timestamp</th><th>Sensex_Category</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "FirstName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Age",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "Experience",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "Label_Type",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Last_transaction_date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "last_timestamp",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Sensex_Category",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "empty_df = spark.createDataFrame([], emp_df.schema)\n",
    "display(empty_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "076a776a-c5f8-48d3-be46-00ebecc63b2e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "existing dataframe"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>FirstName</th><th>Age</th><th>Experience</th><th>Label_Type</th><th>Last_transaction_date</th><th>last_timestamp</th><th>Sensex_Category</th></tr></thead><tbody><tr><td>Kalmesh</td><td>15</td><td>9</td><td>Medium</td><td>23/11/2025</td><td>2025-09-27T19:45:35</td><td>Admin</td></tr><tr><td>Rohan</td><td>18</td><td>5</td><td>Small</td><td>20/12/2024</td><td>2023-11-29T19:55:35</td><td>Sales</td></tr><tr><td>Kiran</td><td>19</td><td>3</td><td>Average</td><td>15/06/2023</td><td>2024-09-27T19:35:35</td><td>Marketing</td></tr><tr><td>Asha</td><td>29</td><td>8</td><td>Short</td><td>13/03/2021</td><td>2021-09-27T19:25:35</td><td>IT</td></tr><tr><td>Amir</td><td>32</td><td>6</td><td>Long</td><td>17/09/2020</td><td>2018-05-21T15:49:39</td><td>Maintenance</td></tr><tr><td>Rupesh</td><td>36</td><td>11</td><td>Less</td><td>19/02/2022</td><td>2016-06-27T19:45:35</td><td>Logistics</td></tr><tr><td>Krupa</td><td>50</td><td>7</td><td>Medium</td><td>12/06/2018</td><td>2019-08-17T22:25:45</td><td>Supplychain</td></tr><tr><td>Vishnu</td><td>55</td><td>8</td><td>Short</td><td>19/08/2019</td><td>2014-09-27T23:55:45</td><td>Transport</td></tr><tr><td>Radha</td><td>58</td><td>9</td><td>Long</td><td>26/04/2016</td><td>2015-05-13T15:35:25</td><td>Safety</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Kalmesh",
         15,
         9,
         "Medium",
         "23/11/2025",
         "2025-09-27T19:45:35",
         "Admin"
        ],
        [
         "Rohan",
         18,
         5,
         "Small",
         "20/12/2024",
         "2023-11-29T19:55:35",
         "Sales"
        ],
        [
         "Kiran",
         19,
         3,
         "Average",
         "15/06/2023",
         "2024-09-27T19:35:35",
         "Marketing"
        ],
        [
         "Asha",
         29,
         8,
         "Short",
         "13/03/2021",
         "2021-09-27T19:25:35",
         "IT"
        ],
        [
         "Amir",
         32,
         6,
         "Long",
         "17/09/2020",
         "2018-05-21T15:49:39",
         "Maintenance"
        ],
        [
         "Rupesh",
         36,
         11,
         "Less",
         "19/02/2022",
         "2016-06-27T19:45:35",
         "Logistics"
        ],
        [
         "Krupa",
         50,
         7,
         "Medium",
         "12/06/2018",
         "2019-08-17T22:25:45",
         "Supplychain"
        ],
        [
         "Vishnu",
         55,
         8,
         "Short",
         "19/08/2019",
         "2014-09-27T23:55:45",
         "Transport"
        ],
        [
         "Radha",
         58,
         9,
         "Long",
         "26/04/2016",
         "2015-05-13T15:35:25",
         "Safety"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "FirstName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Age",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "Experience",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "Label_Type",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Last_transaction_date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "last_timestamp",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Sensex_Category",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sample DataFrame to union\n",
    "data = [(\"Kalmesh\", 15, 9, \"Medium\", \"23/11/2025\", \"2025-09-27T19:45:35\", \"Admin\"),\n",
    "        (\"Rohan\", 18, 5, \"Small\", \"20/12/2024\", \"2023-11-29T19:55:35\", \"Sales\"),\n",
    "        (\"Kiran\", 19, 3, \"Average\", \"15/06/2023\", \"2024-09-27T19:35:35\", \"Marketing\"),\n",
    "        (\"Asha\", 29, 8, \"Short\", \"13/03/2021\", \"2021-09-27T19:25:35\", \"IT\"),\n",
    "        (\"Amir\", 32, 6, \"Long\", \"17/09/2020\", \"2018-05-21T15:49:39\", \"Maintenance\"),\n",
    "        (\"Rupesh\", 36, 11, \"Less\", \"19/02/2022\", \"2016-06-27T19:45:35\", \"Logistics\"),\n",
    "        (\"Krupa\", 50, 7, \"Medium\", \"12/06/2018\", \"2019-08-17T22:25:45\", \"Supplychain\"),\n",
    "        (\"Vishnu\", 55, 8, \"Short\", \"19/08/2019\", \"2014-09-27T23:55:45\", \"Transport\"),\n",
    "        (\"Radha\", 58, 9, \"Long\", \"26/04/2016\", \"2015-05-13T15:35:25\", \"Safety\"),\n",
    "        ]\n",
    "        \n",
    "df = spark.createDataFrame(data, schema)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc0401ba-4f95-45b9-aff3-5c408d4728f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>FirstName</th><th>Age</th><th>Experience</th><th>Label_Type</th><th>Last_transaction_date</th><th>last_timestamp</th><th>Sensex_Category</th></tr></thead><tbody><tr><td>Kalmesh</td><td>15</td><td>9</td><td>Medium</td><td>23/11/2025</td><td>2025-09-27T19:45:35</td><td>Admin</td></tr><tr><td>Rohan</td><td>18</td><td>5</td><td>Small</td><td>20/12/2024</td><td>2023-11-29T19:55:35</td><td>Sales</td></tr><tr><td>Kiran</td><td>19</td><td>3</td><td>Average</td><td>15/06/2023</td><td>2024-09-27T19:35:35</td><td>Marketing</td></tr><tr><td>Asha</td><td>29</td><td>8</td><td>Short</td><td>13/03/2021</td><td>2021-09-27T19:25:35</td><td>IT</td></tr><tr><td>Amir</td><td>32</td><td>6</td><td>Long</td><td>17/09/2020</td><td>2018-05-21T15:49:39</td><td>Maintenance</td></tr><tr><td>Rupesh</td><td>36</td><td>11</td><td>Less</td><td>19/02/2022</td><td>2016-06-27T19:45:35</td><td>Logistics</td></tr><tr><td>Krupa</td><td>50</td><td>7</td><td>Medium</td><td>12/06/2018</td><td>2019-08-17T22:25:45</td><td>Supplychain</td></tr><tr><td>Vishnu</td><td>55</td><td>8</td><td>Short</td><td>19/08/2019</td><td>2014-09-27T23:55:45</td><td>Transport</td></tr><tr><td>Radha</td><td>58</td><td>9</td><td>Long</td><td>26/04/2016</td><td>2015-05-13T15:35:25</td><td>Safety</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Kalmesh",
         15,
         9,
         "Medium",
         "23/11/2025",
         "2025-09-27T19:45:35",
         "Admin"
        ],
        [
         "Rohan",
         18,
         5,
         "Small",
         "20/12/2024",
         "2023-11-29T19:55:35",
         "Sales"
        ],
        [
         "Kiran",
         19,
         3,
         "Average",
         "15/06/2023",
         "2024-09-27T19:35:35",
         "Marketing"
        ],
        [
         "Asha",
         29,
         8,
         "Short",
         "13/03/2021",
         "2021-09-27T19:25:35",
         "IT"
        ],
        [
         "Amir",
         32,
         6,
         "Long",
         "17/09/2020",
         "2018-05-21T15:49:39",
         "Maintenance"
        ],
        [
         "Rupesh",
         36,
         11,
         "Less",
         "19/02/2022",
         "2016-06-27T19:45:35",
         "Logistics"
        ],
        [
         "Krupa",
         50,
         7,
         "Medium",
         "12/06/2018",
         "2019-08-17T22:25:45",
         "Supplychain"
        ],
        [
         "Vishnu",
         55,
         8,
         "Short",
         "19/08/2019",
         "2014-09-27T23:55:45",
         "Transport"
        ],
        [
         "Radha",
         58,
         9,
         "Long",
         "26/04/2016",
         "2015-05-13T15:35:25",
         "Safety"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "FirstName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Age",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "Experience",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "Label_Type",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Last_transaction_date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "last_timestamp",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Sensex_Category",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Union operation\n",
    "result_df = empty_df.union(df)\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6f11390-74d0-4168-a925-3b0505287b92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**⚠️ Common Mistake**\n",
    "- You must always pass a **schema** if the DataFrame is **empty**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2401fe9-5ece-4867-a809-99ba9e5f648e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3178736082817283>, line 2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# This will raise an error because schema is not provided or mismatched\u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m empty_df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mcreateDataFrame([])\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Error on this line:\u001B[39;00m\n",
       "\u001B[1;32m      5\u001B[0m result \u001B[38;5;241m=\u001B[39m empty_df\u001B[38;5;241m.\u001B[39munion(df)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     50\u001B[0m     )\n",
       "\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1610\u001B[0m, in \u001B[0;36mSparkSession.createDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n",
       "\u001B[1;32m   1605\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_pandas \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, pd\u001B[38;5;241m.\u001B[39mDataFrame):\n",
       "\u001B[1;32m   1606\u001B[0m     \u001B[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001B[39;00m\n",
       "\u001B[1;32m   1607\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m(SparkSession, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39mcreateDataFrame(  \u001B[38;5;66;03m# type: ignore[call-overload]\u001B[39;00m\n",
       "\u001B[1;32m   1608\u001B[0m         data, schema, samplingRatio, verifySchema\n",
       "\u001B[1;32m   1609\u001B[0m     )\n",
       "\u001B[0;32m-> 1610\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_dataframe(\n",
       "\u001B[1;32m   1611\u001B[0m     data, schema, samplingRatio, verifySchema  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n",
       "\u001B[1;32m   1612\u001B[0m )\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1667\u001B[0m, in \u001B[0;36mSparkSession._create_dataframe\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n",
       "\u001B[1;32m   1665\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_createFromRDD(data\u001B[38;5;241m.\u001B[39mmap(prepare), schema, samplingRatio)\n",
       "\u001B[1;32m   1666\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m-> 1667\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_createFromLocal(\u001B[38;5;28mmap\u001B[39m(prepare, data), schema)\n",
       "\u001B[1;32m   1668\u001B[0m jrdd \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mSerDeUtil\u001B[38;5;241m.\u001B[39mtoJavaArray(rdd\u001B[38;5;241m.\u001B[39m_to_java_object_rdd())\n",
       "\u001B[1;32m   1669\u001B[0m jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsparkSession\u001B[38;5;241m.\u001B[39mapplySchemaToPythonRDD(jrdd\u001B[38;5;241m.\u001B[39mrdd(), struct\u001B[38;5;241m.\u001B[39mjson())\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1234\u001B[0m, in \u001B[0;36mSparkSession._createFromLocal\u001B[0;34m(self, data, schema)\u001B[0m\n",
       "\u001B[1;32m   1226\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_createFromLocal\u001B[39m(\n",
       "\u001B[1;32m   1227\u001B[0m     \u001B[38;5;28mself\u001B[39m, data: Iterable[Any], schema: Optional[Union[DataType, List[\u001B[38;5;28mstr\u001B[39m]]]\n",
       "\u001B[1;32m   1228\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRDD[Tuple]\u001B[39m\u001B[38;5;124m\"\u001B[39m, StructType]:\n",
       "\u001B[1;32m   1229\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m   1230\u001B[0m \u001B[38;5;124;03m    Create an RDD for DataFrame from a list or pandas.DataFrame, returns the RDD and schema.\u001B[39;00m\n",
       "\u001B[1;32m   1231\u001B[0m \u001B[38;5;124;03m    This would be broken with table acl enabled as user process does not have permission to\u001B[39;00m\n",
       "\u001B[1;32m   1232\u001B[0m \u001B[38;5;124;03m    write temp files.\u001B[39;00m\n",
       "\u001B[1;32m   1233\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m-> 1234\u001B[0m     internal_data, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wrap_data_schema(data, schema)\n",
       "\u001B[1;32m   1235\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sc\u001B[38;5;241m.\u001B[39mparallelize(internal_data), struct\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1201\u001B[0m, in \u001B[0;36mSparkSession._wrap_data_schema\u001B[0;34m(self, data, schema)\u001B[0m\n",
       "\u001B[1;32m   1198\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(data)\n",
       "\u001B[1;32m   1200\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m schema \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(schema, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n",
       "\u001B[0;32m-> 1201\u001B[0m     struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inferSchemaFromList(data, names\u001B[38;5;241m=\u001B[39mschema)\n",
       "\u001B[1;32m   1202\u001B[0m     converter \u001B[38;5;241m=\u001B[39m _create_converter(struct)\n",
       "\u001B[1;32m   1203\u001B[0m     tupled_data: Iterable[Tuple] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmap\u001B[39m(converter, data)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1061\u001B[0m, in \u001B[0;36mSparkSession._inferSchemaFromList\u001B[0;34m(self, data, names)\u001B[0m\n",
       "\u001B[1;32m   1046\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m   1047\u001B[0m \u001B[38;5;124;03mInfer schema from list of Row, dict, or tuple.\u001B[39;00m\n",
       "\u001B[1;32m   1048\u001B[0m \n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   1058\u001B[0m \u001B[38;5;124;03m:class:`pyspark.sql.types.StructType`\u001B[39;00m\n",
       "\u001B[1;32m   1059\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m   1060\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m data:\n",
       "\u001B[0;32m-> 1061\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcan not infer schema from empty dataset\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m   1062\u001B[0m infer_dict_as_struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jconf\u001B[38;5;241m.\u001B[39minferDictAsStruct()\n",
       "\u001B[1;32m   1063\u001B[0m infer_array_from_first_element \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jconf\u001B[38;5;241m.\u001B[39mlegacyInferArrayTypeFromFirstElement()\n",
       "\n",
       "\u001B[0;31mValueError\u001B[0m: can not infer schema from empty dataset"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "ValueError",
        "evalue": "can not infer schema from empty dataset"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>ValueError</span>: can not infer schema from empty dataset"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
        "File \u001B[0;32m<command-3178736082817283>, line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# This will raise an error because schema is not provided or mismatched\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m empty_df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mcreateDataFrame([])\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Error on this line:\u001B[39;00m\n\u001B[1;32m      5\u001B[0m result \u001B[38;5;241m=\u001B[39m empty_df\u001B[38;5;241m.\u001B[39munion(df)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     50\u001B[0m     )\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1610\u001B[0m, in \u001B[0;36mSparkSession.createDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m   1605\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_pandas \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, pd\u001B[38;5;241m.\u001B[39mDataFrame):\n\u001B[1;32m   1606\u001B[0m     \u001B[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001B[39;00m\n\u001B[1;32m   1607\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m(SparkSession, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39mcreateDataFrame(  \u001B[38;5;66;03m# type: ignore[call-overload]\u001B[39;00m\n\u001B[1;32m   1608\u001B[0m         data, schema, samplingRatio, verifySchema\n\u001B[1;32m   1609\u001B[0m     )\n\u001B[0;32m-> 1610\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_dataframe(\n\u001B[1;32m   1611\u001B[0m     data, schema, samplingRatio, verifySchema  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[1;32m   1612\u001B[0m )\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1667\u001B[0m, in \u001B[0;36mSparkSession._create_dataframe\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m   1665\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_createFromRDD(data\u001B[38;5;241m.\u001B[39mmap(prepare), schema, samplingRatio)\n\u001B[1;32m   1666\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1667\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_createFromLocal(\u001B[38;5;28mmap\u001B[39m(prepare, data), schema)\n\u001B[1;32m   1668\u001B[0m jrdd \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mSerDeUtil\u001B[38;5;241m.\u001B[39mtoJavaArray(rdd\u001B[38;5;241m.\u001B[39m_to_java_object_rdd())\n\u001B[1;32m   1669\u001B[0m jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsparkSession\u001B[38;5;241m.\u001B[39mapplySchemaToPythonRDD(jrdd\u001B[38;5;241m.\u001B[39mrdd(), struct\u001B[38;5;241m.\u001B[39mjson())\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1234\u001B[0m, in \u001B[0;36mSparkSession._createFromLocal\u001B[0;34m(self, data, schema)\u001B[0m\n\u001B[1;32m   1226\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_createFromLocal\u001B[39m(\n\u001B[1;32m   1227\u001B[0m     \u001B[38;5;28mself\u001B[39m, data: Iterable[Any], schema: Optional[Union[DataType, List[\u001B[38;5;28mstr\u001B[39m]]]\n\u001B[1;32m   1228\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRDD[Tuple]\u001B[39m\u001B[38;5;124m\"\u001B[39m, StructType]:\n\u001B[1;32m   1229\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1230\u001B[0m \u001B[38;5;124;03m    Create an RDD for DataFrame from a list or pandas.DataFrame, returns the RDD and schema.\u001B[39;00m\n\u001B[1;32m   1231\u001B[0m \u001B[38;5;124;03m    This would be broken with table acl enabled as user process does not have permission to\u001B[39;00m\n\u001B[1;32m   1232\u001B[0m \u001B[38;5;124;03m    write temp files.\u001B[39;00m\n\u001B[1;32m   1233\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1234\u001B[0m     internal_data, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wrap_data_schema(data, schema)\n\u001B[1;32m   1235\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sc\u001B[38;5;241m.\u001B[39mparallelize(internal_data), struct\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1201\u001B[0m, in \u001B[0;36mSparkSession._wrap_data_schema\u001B[0;34m(self, data, schema)\u001B[0m\n\u001B[1;32m   1198\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(data)\n\u001B[1;32m   1200\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m schema \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(schema, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n\u001B[0;32m-> 1201\u001B[0m     struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inferSchemaFromList(data, names\u001B[38;5;241m=\u001B[39mschema)\n\u001B[1;32m   1202\u001B[0m     converter \u001B[38;5;241m=\u001B[39m _create_converter(struct)\n\u001B[1;32m   1203\u001B[0m     tupled_data: Iterable[Tuple] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmap\u001B[39m(converter, data)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1061\u001B[0m, in \u001B[0;36mSparkSession._inferSchemaFromList\u001B[0;34m(self, data, names)\u001B[0m\n\u001B[1;32m   1046\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1047\u001B[0m \u001B[38;5;124;03mInfer schema from list of Row, dict, or tuple.\u001B[39;00m\n\u001B[1;32m   1048\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1058\u001B[0m \u001B[38;5;124;03m:class:`pyspark.sql.types.StructType`\u001B[39;00m\n\u001B[1;32m   1059\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1060\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m data:\n\u001B[0;32m-> 1061\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcan not infer schema from empty dataset\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1062\u001B[0m infer_dict_as_struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jconf\u001B[38;5;241m.\u001B[39minferDictAsStruct()\n\u001B[1;32m   1063\u001B[0m infer_array_from_first_element \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jconf\u001B[38;5;241m.\u001B[39mlegacyInferArrayTypeFromFirstElement()\n",
        "\u001B[0;31mValueError\u001B[0m: can not infer schema from empty dataset"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This will raise an error because schema is not provided or mismatched\n",
    "empty_df = spark.createDataFrame([])\n",
    "\n",
    "# Error on this line:\n",
    "result = empty_df.union(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e92b6e1b-a768-450b-8288-1c168f7dc7e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**4) Using loop with empty DataFrame for accumulation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1edde0c6-d21b-45f8-b372-e653072ea733",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>FirstName</th><th>Age</th><th>Experience</th><th>Label_Type</th><th>Last_transaction_date</th><th>last_timestamp</th><th>Sensex_Category</th></tr></thead><tbody><tr><td>Kalmesh</td><td>15</td><td>9</td><td>Medium</td><td>23/11/2025</td><td>2025-09-27T19:45:35</td><td>Admin</td></tr><tr><td>Rohan</td><td>18</td><td>5</td><td>Small</td><td>20/12/2024</td><td>2023-11-29T19:55:35</td><td>Sales</td></tr><tr><td>Kiran</td><td>19</td><td>3</td><td>Average</td><td>15/06/2023</td><td>2024-09-27T19:35:35</td><td>Marketing</td></tr><tr><td>Asha</td><td>29</td><td>8</td><td>Short</td><td>13/03/2021</td><td>2021-09-27T19:25:35</td><td>IT</td></tr><tr><td>Amir</td><td>32</td><td>6</td><td>Long</td><td>17/09/2020</td><td>2018-05-21T15:49:39</td><td>Maintenance</td></tr><tr><td>Rupesh</td><td>36</td><td>11</td><td>Less</td><td>19/02/2022</td><td>2016-06-27T19:45:35</td><td>Logistics</td></tr><tr><td>Krupa</td><td>50</td><td>7</td><td>Medium</td><td>12/06/2018</td><td>2019-08-17T22:25:45</td><td>Supplychain</td></tr><tr><td>Vishnu</td><td>55</td><td>8</td><td>Short</td><td>19/08/2019</td><td>2014-09-27T23:55:45</td><td>Transport</td></tr><tr><td>Radha</td><td>58</td><td>9</td><td>Long</td><td>26/04/2016</td><td>2015-05-13T15:35:25</td><td>Safety</td></tr><tr><td>Asha</td><td>29</td><td>8</td><td>Short</td><td>13/03/2021</td><td>2021-09-27T19:25:35</td><td>IT</td></tr><tr><td>Amir</td><td>32</td><td>6</td><td>Long</td><td>17/09/2020</td><td>2018-05-21T15:49:39</td><td>Maintenance</td></tr><tr><td>Rupesh</td><td>36</td><td>11</td><td>Less</td><td>19/02/2022</td><td>2016-06-27T19:45:35</td><td>Logistics</td></tr><tr><td>Krupa</td><td>50</td><td>7</td><td>Medium</td><td>12/06/2018</td><td>2019-08-17T22:25:45</td><td>Supplychain</td></tr><tr><td>Vishnu</td><td>55</td><td>8</td><td>Short</td><td>19/08/2019</td><td>2014-09-27T23:55:45</td><td>Transport</td></tr><tr><td>Radha</td><td>58</td><td>9</td><td>Long</td><td>26/04/2016</td><td>2015-05-13T15:35:25</td><td>Safety</td></tr><tr><td>Krupa</td><td>50</td><td>7</td><td>Medium</td><td>12/06/2018</td><td>2019-08-17T22:25:45</td><td>Supplychain</td></tr><tr><td>Vishnu</td><td>55</td><td>8</td><td>Short</td><td>19/08/2019</td><td>2014-09-27T23:55:45</td><td>Transport</td></tr><tr><td>Radha</td><td>58</td><td>9</td><td>Long</td><td>26/04/2016</td><td>2015-05-13T15:35:25</td><td>Safety</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Kalmesh",
         15,
         9,
         "Medium",
         "23/11/2025",
         "2025-09-27T19:45:35",
         "Admin"
        ],
        [
         "Rohan",
         18,
         5,
         "Small",
         "20/12/2024",
         "2023-11-29T19:55:35",
         "Sales"
        ],
        [
         "Kiran",
         19,
         3,
         "Average",
         "15/06/2023",
         "2024-09-27T19:35:35",
         "Marketing"
        ],
        [
         "Asha",
         29,
         8,
         "Short",
         "13/03/2021",
         "2021-09-27T19:25:35",
         "IT"
        ],
        [
         "Amir",
         32,
         6,
         "Long",
         "17/09/2020",
         "2018-05-21T15:49:39",
         "Maintenance"
        ],
        [
         "Rupesh",
         36,
         11,
         "Less",
         "19/02/2022",
         "2016-06-27T19:45:35",
         "Logistics"
        ],
        [
         "Krupa",
         50,
         7,
         "Medium",
         "12/06/2018",
         "2019-08-17T22:25:45",
         "Supplychain"
        ],
        [
         "Vishnu",
         55,
         8,
         "Short",
         "19/08/2019",
         "2014-09-27T23:55:45",
         "Transport"
        ],
        [
         "Radha",
         58,
         9,
         "Long",
         "26/04/2016",
         "2015-05-13T15:35:25",
         "Safety"
        ],
        [
         "Asha",
         29,
         8,
         "Short",
         "13/03/2021",
         "2021-09-27T19:25:35",
         "IT"
        ],
        [
         "Amir",
         32,
         6,
         "Long",
         "17/09/2020",
         "2018-05-21T15:49:39",
         "Maintenance"
        ],
        [
         "Rupesh",
         36,
         11,
         "Less",
         "19/02/2022",
         "2016-06-27T19:45:35",
         "Logistics"
        ],
        [
         "Krupa",
         50,
         7,
         "Medium",
         "12/06/2018",
         "2019-08-17T22:25:45",
         "Supplychain"
        ],
        [
         "Vishnu",
         55,
         8,
         "Short",
         "19/08/2019",
         "2014-09-27T23:55:45",
         "Transport"
        ],
        [
         "Radha",
         58,
         9,
         "Long",
         "26/04/2016",
         "2015-05-13T15:35:25",
         "Safety"
        ],
        [
         "Krupa",
         50,
         7,
         "Medium",
         "12/06/2018",
         "2019-08-17T22:25:45",
         "Supplychain"
        ],
        [
         "Vishnu",
         55,
         8,
         "Short",
         "19/08/2019",
         "2014-09-27T23:55:45",
         "Transport"
        ],
        [
         "Radha",
         58,
         9,
         "Long",
         "26/04/2016",
         "2015-05-13T15:35:25",
         "Safety"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "FirstName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Age",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "Experience",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "Label_Type",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Last_transaction_date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "last_timestamp",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Sensex_Category",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an empty DataFrame with the same schema\n",
    "df_loop = spark.createDataFrame([], df.schema)\n",
    "\n",
    "for i in range(3):\n",
    "    # Filter the original DataFrame based on Age\n",
    "    filtered_df = df.filter(df.Age > i * 20)\n",
    "\n",
    "    # Accumulate the results into df_loop\n",
    "    df_loop = df_loop.union(filtered_df)\n",
    "\n",
    "# Display the final accumulated DataFrame\n",
    "display(df_loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d399f947-b1c6-49e0-97b6-e4015676a33b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**5) Chaining multiple unions with empty DataFrame as starting point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3c2b254-d45b-4658-bc72-bbb4c9165bf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>Name</th><th>Age</th><th>Type</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Age",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Type",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assume df1, df2, df3 all have the same schema\n",
    "df1 = spark.createDataFrame([(1, \"Naresh\", 28, \"Medium\"),\n",
    "                             (2, \"Mohan\", 25, \"Low\"),\n",
    "                             (3, \"Hitesh\", 29, \"High\"),\n",
    "                             (4, \"Vedita\", 32, \"Less\"),\n",
    "                             (5, \"Sushmita\", 35, \"Higher\")],\n",
    "                             [\"id\", \"Name\", \"Age\", \"Type\"])\n",
    "\n",
    "df2 = spark.createDataFrame([(1, \"Neha\", 21, \"Medium\"),\n",
    "                             (2, \"Mohin\", 26, \"Low\"),\n",
    "                             (3, \"Hritik\", 27, \"High\"),\n",
    "                             (4, \"Vasu\", 35, \"Less\"),\n",
    "                             (5, \"Susi\", 37, \"Higher\")],\n",
    "                             [\"id\", \"Name\", \"Age\", \"Type\"])\n",
    "df3 = spark.createDataFrame([(1, \"Druv\", 31, \"Medium\"),\n",
    "                             (2, \"Eashwar\", 33, \"Low\"),\n",
    "                             (3, \"Guru\", 29, \"High\"),\n",
    "                             (4, \"Vishwak\", 39, \"Less\"),\n",
    "                             (5, \"Sophia\", 41, \"Higher\")],\n",
    "                             [\"id\", \"Name\", \"Age\", \"Type\"])\n",
    "\n",
    "# Start with an empty DataFrame\n",
    "final_df = spark.createDataFrame([], df1.schema)\n",
    "display(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0cd8d57-dc9e-47c6-8998-1b6b04a3a704",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>Name</th><th>Age</th><th>Type</th></tr></thead><tbody><tr><td>1</td><td>Naresh</td><td>28</td><td>Medium</td></tr><tr><td>2</td><td>Mohan</td><td>25</td><td>Low</td></tr><tr><td>3</td><td>Hitesh</td><td>29</td><td>High</td></tr><tr><td>4</td><td>Vedita</td><td>32</td><td>Less</td></tr><tr><td>5</td><td>Sushmita</td><td>35</td><td>Higher</td></tr><tr><td>1</td><td>Neha</td><td>21</td><td>Medium</td></tr><tr><td>2</td><td>Mohin</td><td>26</td><td>Low</td></tr><tr><td>3</td><td>Hritik</td><td>27</td><td>High</td></tr><tr><td>4</td><td>Vasu</td><td>35</td><td>Less</td></tr><tr><td>5</td><td>Susi</td><td>37</td><td>Higher</td></tr><tr><td>1</td><td>Druv</td><td>31</td><td>Medium</td></tr><tr><td>2</td><td>Eashwar</td><td>33</td><td>Low</td></tr><tr><td>3</td><td>Guru</td><td>29</td><td>High</td></tr><tr><td>4</td><td>Vishwak</td><td>39</td><td>Less</td></tr><tr><td>5</td><td>Sophia</td><td>41</td><td>Higher</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Naresh",
         28,
         "Medium"
        ],
        [
         2,
         "Mohan",
         25,
         "Low"
        ],
        [
         3,
         "Hitesh",
         29,
         "High"
        ],
        [
         4,
         "Vedita",
         32,
         "Less"
        ],
        [
         5,
         "Sushmita",
         35,
         "Higher"
        ],
        [
         1,
         "Neha",
         21,
         "Medium"
        ],
        [
         2,
         "Mohin",
         26,
         "Low"
        ],
        [
         3,
         "Hritik",
         27,
         "High"
        ],
        [
         4,
         "Vasu",
         35,
         "Less"
        ],
        [
         5,
         "Susi",
         37,
         "Higher"
        ],
        [
         1,
         "Druv",
         31,
         "Medium"
        ],
        [
         2,
         "Eashwar",
         33,
         "Low"
        ],
        [
         3,
         "Guru",
         29,
         "High"
        ],
        [
         4,
         "Vishwak",
         39,
         "Less"
        ],
        [
         5,
         "Sophia",
         41,
         "Higher"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Age",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Type",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Union multiple DataFrames\n",
    "final_df = final_df.union(df1).union(df2).union(df3)\n",
    "display(final_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3_How to create Empty dataframe in PySpark",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}