{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "335c77a9-ed63-41f3-8205-127bf1392b63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Method 01**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acf0479e-a268-461f-b72d-2989487e7c54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Numbers</th></tr></thead><tbody><tr><td>15</td></tr><tr><td>25</td></tr><tr><td>36</td></tr><tr><td>44</td></tr><tr><td>57</td></tr><tr><td>65</td></tr><tr><td>89</td></tr><tr><td>95</td></tr><tr><td>9</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         15
        ],
        [
         25
        ],
        [
         36
        ],
        [
         44
        ],
        [
         57
        ],
        [
         65
        ],
        [
         89
        ],
        [
         95
        ],
        [
         9
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Numbers",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [15, 25, 36, 44, 57, 65, 89, 95, 9]\n",
    "\n",
    "df4 = spark.createDataFrame([(x,) for x in data], [\"Numbers\"])\n",
    "display(df4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e86135f-b3ce-4cf2-9a2a-674edba5e046",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**✅ Why use (x,) and not just [x for x in data]?**\n",
    "\n",
    "- **[(x,) for x in data]** creates a **list of 1-element tuples**:\n",
    "\n",
    "      [(15,), (25,), (36,), ..., (9,)]\n",
    "\n",
    "  - Each item is a **tuple with one element**, which maps correctly to the **one-column schema [\"Numbers\"]**. PySpark sees **each tuple as a row**.\n",
    "  \n",
    "  - **createDataFrame()** expects each element of the input **list to be a row**, which means a **tuple or list** representing the values in each column.\n",
    "\n",
    "- So, when you write:\n",
    "\n",
    "      [(x,) for x in data]\n",
    "\n",
    "  - **[(x,)]** creates a **list of tuples**, each containing a **single value** — this is what PySpark expects when you give it a schema like [\"Numbers\"]\n",
    "\n",
    "  - **each row** has **1 value** (for the column Numbers), so create it as a **tuple of one element**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78357ed3-06e7-47bc-ba87-26a8e1703cb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPySparkTypeError\u001B[0m                          Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3931534879136477>, line 1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m df4 \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mcreateDataFrame([x \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m data], [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNumbers\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n",
       "\u001B[1;32m      2\u001B[0m display(df4)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     50\u001B[0m     )\n",
       "\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1610\u001B[0m, in \u001B[0;36mSparkSession.createDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n",
       "\u001B[1;32m   1605\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_pandas \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, pd\u001B[38;5;241m.\u001B[39mDataFrame):\n",
       "\u001B[1;32m   1606\u001B[0m     \u001B[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001B[39;00m\n",
       "\u001B[1;32m   1607\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m(SparkSession, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39mcreateDataFrame(  \u001B[38;5;66;03m# type: ignore[call-overload]\u001B[39;00m\n",
       "\u001B[1;32m   1608\u001B[0m         data, schema, samplingRatio, verifySchema\n",
       "\u001B[1;32m   1609\u001B[0m     )\n",
       "\u001B[0;32m-> 1610\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_dataframe(\n",
       "\u001B[1;32m   1611\u001B[0m     data, schema, samplingRatio, verifySchema  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n",
       "\u001B[1;32m   1612\u001B[0m )\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1667\u001B[0m, in \u001B[0;36mSparkSession._create_dataframe\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n",
       "\u001B[1;32m   1665\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_createFromRDD(data\u001B[38;5;241m.\u001B[39mmap(prepare), schema, samplingRatio)\n",
       "\u001B[1;32m   1666\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m-> 1667\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_createFromLocal(\u001B[38;5;28mmap\u001B[39m(prepare, data), schema)\n",
       "\u001B[1;32m   1668\u001B[0m jrdd \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mSerDeUtil\u001B[38;5;241m.\u001B[39mtoJavaArray(rdd\u001B[38;5;241m.\u001B[39m_to_java_object_rdd())\n",
       "\u001B[1;32m   1669\u001B[0m jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsparkSession\u001B[38;5;241m.\u001B[39mapplySchemaToPythonRDD(jrdd\u001B[38;5;241m.\u001B[39mrdd(), struct\u001B[38;5;241m.\u001B[39mjson())\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1234\u001B[0m, in \u001B[0;36mSparkSession._createFromLocal\u001B[0;34m(self, data, schema)\u001B[0m\n",
       "\u001B[1;32m   1226\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_createFromLocal\u001B[39m(\n",
       "\u001B[1;32m   1227\u001B[0m     \u001B[38;5;28mself\u001B[39m, data: Iterable[Any], schema: Optional[Union[DataType, List[\u001B[38;5;28mstr\u001B[39m]]]\n",
       "\u001B[1;32m   1228\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRDD[Tuple]\u001B[39m\u001B[38;5;124m\"\u001B[39m, StructType]:\n",
       "\u001B[1;32m   1229\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m   1230\u001B[0m \u001B[38;5;124;03m    Create an RDD for DataFrame from a list or pandas.DataFrame, returns the RDD and schema.\u001B[39;00m\n",
       "\u001B[1;32m   1231\u001B[0m \u001B[38;5;124;03m    This would be broken with table acl enabled as user process does not have permission to\u001B[39;00m\n",
       "\u001B[1;32m   1232\u001B[0m \u001B[38;5;124;03m    write temp files.\u001B[39;00m\n",
       "\u001B[1;32m   1233\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m-> 1234\u001B[0m     internal_data, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wrap_data_schema(data, schema)\n",
       "\u001B[1;32m   1235\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sc\u001B[38;5;241m.\u001B[39mparallelize(internal_data), struct\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1201\u001B[0m, in \u001B[0;36mSparkSession._wrap_data_schema\u001B[0;34m(self, data, schema)\u001B[0m\n",
       "\u001B[1;32m   1198\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(data)\n",
       "\u001B[1;32m   1200\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m schema \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(schema, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n",
       "\u001B[0;32m-> 1201\u001B[0m     struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inferSchemaFromList(data, names\u001B[38;5;241m=\u001B[39mschema)\n",
       "\u001B[1;32m   1202\u001B[0m     converter \u001B[38;5;241m=\u001B[39m _create_converter(struct)\n",
       "\u001B[1;32m   1203\u001B[0m     tupled_data: Iterable[Tuple] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmap\u001B[39m(converter, data)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1066\u001B[0m, in \u001B[0;36mSparkSession._inferSchemaFromList\u001B[0;34m(self, data, names)\u001B[0m\n",
       "\u001B[1;32m   1064\u001B[0m infer_map_from_first_pair \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jconf\u001B[38;5;241m.\u001B[39mlegacyInferMapStructTypeFromFirstItem()\n",
       "\u001B[1;32m   1065\u001B[0m prefer_timestamp_ntz \u001B[38;5;241m=\u001B[39m is_timestamp_ntz_preferred()\n",
       "\u001B[0;32m-> 1066\u001B[0m schema \u001B[38;5;241m=\u001B[39m reduce(\n",
       "\u001B[1;32m   1067\u001B[0m     _merge_type,\n",
       "\u001B[1;32m   1068\u001B[0m     (\n",
       "\u001B[1;32m   1069\u001B[0m         _infer_schema(\n",
       "\u001B[1;32m   1070\u001B[0m             row,\n",
       "\u001B[1;32m   1071\u001B[0m             names,\n",
       "\u001B[1;32m   1072\u001B[0m             infer_dict_as_struct\u001B[38;5;241m=\u001B[39minfer_dict_as_struct,\n",
       "\u001B[1;32m   1073\u001B[0m             infer_array_from_first_element\u001B[38;5;241m=\u001B[39minfer_array_from_first_element,\n",
       "\u001B[1;32m   1074\u001B[0m             infer_map_from_first_pair\u001B[38;5;241m=\u001B[39minfer_map_from_first_pair,\n",
       "\u001B[1;32m   1075\u001B[0m             prefer_timestamp_ntz\u001B[38;5;241m=\u001B[39mprefer_timestamp_ntz,\n",
       "\u001B[1;32m   1076\u001B[0m         )\n",
       "\u001B[1;32m   1077\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m row \u001B[38;5;129;01min\u001B[39;00m data\n",
       "\u001B[1;32m   1078\u001B[0m     ),\n",
       "\u001B[1;32m   1079\u001B[0m )\n",
       "\u001B[1;32m   1080\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _has_nulltype(schema):\n",
       "\u001B[1;32m   1081\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkValueError(\n",
       "\u001B[1;32m   1082\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCANNOT_DETERMINE_TYPE\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m   1083\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{},\n",
       "\u001B[1;32m   1084\u001B[0m     )\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1069\u001B[0m, in \u001B[0;36m<genexpr>\u001B[0;34m(.0)\u001B[0m\n",
       "\u001B[1;32m   1064\u001B[0m infer_map_from_first_pair \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jconf\u001B[38;5;241m.\u001B[39mlegacyInferMapStructTypeFromFirstItem()\n",
       "\u001B[1;32m   1065\u001B[0m prefer_timestamp_ntz \u001B[38;5;241m=\u001B[39m is_timestamp_ntz_preferred()\n",
       "\u001B[1;32m   1066\u001B[0m schema \u001B[38;5;241m=\u001B[39m reduce(\n",
       "\u001B[1;32m   1067\u001B[0m     _merge_type,\n",
       "\u001B[1;32m   1068\u001B[0m     (\n",
       "\u001B[0;32m-> 1069\u001B[0m         _infer_schema(\n",
       "\u001B[1;32m   1070\u001B[0m             row,\n",
       "\u001B[1;32m   1071\u001B[0m             names,\n",
       "\u001B[1;32m   1072\u001B[0m             infer_dict_as_struct\u001B[38;5;241m=\u001B[39minfer_dict_as_struct,\n",
       "\u001B[1;32m   1073\u001B[0m             infer_array_from_first_element\u001B[38;5;241m=\u001B[39minfer_array_from_first_element,\n",
       "\u001B[1;32m   1074\u001B[0m             infer_map_from_first_pair\u001B[38;5;241m=\u001B[39minfer_map_from_first_pair,\n",
       "\u001B[1;32m   1075\u001B[0m             prefer_timestamp_ntz\u001B[38;5;241m=\u001B[39mprefer_timestamp_ntz,\n",
       "\u001B[1;32m   1076\u001B[0m         )\n",
       "\u001B[1;32m   1077\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m row \u001B[38;5;129;01min\u001B[39;00m data\n",
       "\u001B[1;32m   1078\u001B[0m     ),\n",
       "\u001B[1;32m   1079\u001B[0m )\n",
       "\u001B[1;32m   1080\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _has_nulltype(schema):\n",
       "\u001B[1;32m   1081\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkValueError(\n",
       "\u001B[1;32m   1082\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCANNOT_DETERMINE_TYPE\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m   1083\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{},\n",
       "\u001B[1;32m   1084\u001B[0m     )\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:2320\u001B[0m, in \u001B[0;36m_infer_schema\u001B[0;34m(row, names, infer_dict_as_struct, infer_array_from_first_element, infer_map_from_first_pair, prefer_timestamp_ntz)\u001B[0m\n",
       "\u001B[1;32m   2317\u001B[0m     items \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msorted\u001B[39m(row\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__dict__\u001B[39m\u001B[38;5;241m.\u001B[39mitems())\n",
       "\u001B[1;32m   2319\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m-> 2320\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n",
       "\u001B[1;32m   2321\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCANNOT_INFER_SCHEMA_FOR_TYPE\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m   2322\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(row)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n",
       "\u001B[1;32m   2323\u001B[0m     )\n",
       "\u001B[1;32m   2325\u001B[0m fields \u001B[38;5;241m=\u001B[39m []\n",
       "\u001B[1;32m   2326\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m items:\n",
       "\n",
       "\u001B[0;31mPySparkTypeError\u001B[0m: [CANNOT_INFER_SCHEMA_FOR_TYPE] Can not infer schema for type: `int`."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "PySparkTypeError",
        "evalue": "[CANNOT_INFER_SCHEMA_FOR_TYPE] Can not infer schema for type: `int`."
       },
       "metadata": {
        "errorSummary": "[CANNOT_INFER_SCHEMA_FOR_TYPE] Can not infer schema for type: `int`."
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "CANNOT_INFER_SCHEMA_FOR_TYPE",
        "pysparkCallSite": null,
        "pysparkFragment": null,
        "pysparkSummary": null,
        "sqlState": null,
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mPySparkTypeError\u001B[0m                          Traceback (most recent call last)",
        "File \u001B[0;32m<command-3931534879136477>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m df4 \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mcreateDataFrame([x \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m data], [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNumbers\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m      2\u001B[0m display(df4)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     50\u001B[0m     )\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1610\u001B[0m, in \u001B[0;36mSparkSession.createDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m   1605\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_pandas \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, pd\u001B[38;5;241m.\u001B[39mDataFrame):\n\u001B[1;32m   1606\u001B[0m     \u001B[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001B[39;00m\n\u001B[1;32m   1607\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m(SparkSession, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39mcreateDataFrame(  \u001B[38;5;66;03m# type: ignore[call-overload]\u001B[39;00m\n\u001B[1;32m   1608\u001B[0m         data, schema, samplingRatio, verifySchema\n\u001B[1;32m   1609\u001B[0m     )\n\u001B[0;32m-> 1610\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_dataframe(\n\u001B[1;32m   1611\u001B[0m     data, schema, samplingRatio, verifySchema  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[1;32m   1612\u001B[0m )\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1667\u001B[0m, in \u001B[0;36mSparkSession._create_dataframe\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m   1665\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_createFromRDD(data\u001B[38;5;241m.\u001B[39mmap(prepare), schema, samplingRatio)\n\u001B[1;32m   1666\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1667\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_createFromLocal(\u001B[38;5;28mmap\u001B[39m(prepare, data), schema)\n\u001B[1;32m   1668\u001B[0m jrdd \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mSerDeUtil\u001B[38;5;241m.\u001B[39mtoJavaArray(rdd\u001B[38;5;241m.\u001B[39m_to_java_object_rdd())\n\u001B[1;32m   1669\u001B[0m jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsparkSession\u001B[38;5;241m.\u001B[39mapplySchemaToPythonRDD(jrdd\u001B[38;5;241m.\u001B[39mrdd(), struct\u001B[38;5;241m.\u001B[39mjson())\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1234\u001B[0m, in \u001B[0;36mSparkSession._createFromLocal\u001B[0;34m(self, data, schema)\u001B[0m\n\u001B[1;32m   1226\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_createFromLocal\u001B[39m(\n\u001B[1;32m   1227\u001B[0m     \u001B[38;5;28mself\u001B[39m, data: Iterable[Any], schema: Optional[Union[DataType, List[\u001B[38;5;28mstr\u001B[39m]]]\n\u001B[1;32m   1228\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRDD[Tuple]\u001B[39m\u001B[38;5;124m\"\u001B[39m, StructType]:\n\u001B[1;32m   1229\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1230\u001B[0m \u001B[38;5;124;03m    Create an RDD for DataFrame from a list or pandas.DataFrame, returns the RDD and schema.\u001B[39;00m\n\u001B[1;32m   1231\u001B[0m \u001B[38;5;124;03m    This would be broken with table acl enabled as user process does not have permission to\u001B[39;00m\n\u001B[1;32m   1232\u001B[0m \u001B[38;5;124;03m    write temp files.\u001B[39;00m\n\u001B[1;32m   1233\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1234\u001B[0m     internal_data, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wrap_data_schema(data, schema)\n\u001B[1;32m   1235\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sc\u001B[38;5;241m.\u001B[39mparallelize(internal_data), struct\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1201\u001B[0m, in \u001B[0;36mSparkSession._wrap_data_schema\u001B[0;34m(self, data, schema)\u001B[0m\n\u001B[1;32m   1198\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(data)\n\u001B[1;32m   1200\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m schema \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(schema, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n\u001B[0;32m-> 1201\u001B[0m     struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inferSchemaFromList(data, names\u001B[38;5;241m=\u001B[39mschema)\n\u001B[1;32m   1202\u001B[0m     converter \u001B[38;5;241m=\u001B[39m _create_converter(struct)\n\u001B[1;32m   1203\u001B[0m     tupled_data: Iterable[Tuple] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmap\u001B[39m(converter, data)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1066\u001B[0m, in \u001B[0;36mSparkSession._inferSchemaFromList\u001B[0;34m(self, data, names)\u001B[0m\n\u001B[1;32m   1064\u001B[0m infer_map_from_first_pair \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jconf\u001B[38;5;241m.\u001B[39mlegacyInferMapStructTypeFromFirstItem()\n\u001B[1;32m   1065\u001B[0m prefer_timestamp_ntz \u001B[38;5;241m=\u001B[39m is_timestamp_ntz_preferred()\n\u001B[0;32m-> 1066\u001B[0m schema \u001B[38;5;241m=\u001B[39m reduce(\n\u001B[1;32m   1067\u001B[0m     _merge_type,\n\u001B[1;32m   1068\u001B[0m     (\n\u001B[1;32m   1069\u001B[0m         _infer_schema(\n\u001B[1;32m   1070\u001B[0m             row,\n\u001B[1;32m   1071\u001B[0m             names,\n\u001B[1;32m   1072\u001B[0m             infer_dict_as_struct\u001B[38;5;241m=\u001B[39minfer_dict_as_struct,\n\u001B[1;32m   1073\u001B[0m             infer_array_from_first_element\u001B[38;5;241m=\u001B[39minfer_array_from_first_element,\n\u001B[1;32m   1074\u001B[0m             infer_map_from_first_pair\u001B[38;5;241m=\u001B[39minfer_map_from_first_pair,\n\u001B[1;32m   1075\u001B[0m             prefer_timestamp_ntz\u001B[38;5;241m=\u001B[39mprefer_timestamp_ntz,\n\u001B[1;32m   1076\u001B[0m         )\n\u001B[1;32m   1077\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m row \u001B[38;5;129;01min\u001B[39;00m data\n\u001B[1;32m   1078\u001B[0m     ),\n\u001B[1;32m   1079\u001B[0m )\n\u001B[1;32m   1080\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _has_nulltype(schema):\n\u001B[1;32m   1081\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkValueError(\n\u001B[1;32m   1082\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCANNOT_DETERMINE_TYPE\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   1083\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{},\n\u001B[1;32m   1084\u001B[0m     )\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1069\u001B[0m, in \u001B[0;36m<genexpr>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m   1064\u001B[0m infer_map_from_first_pair \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jconf\u001B[38;5;241m.\u001B[39mlegacyInferMapStructTypeFromFirstItem()\n\u001B[1;32m   1065\u001B[0m prefer_timestamp_ntz \u001B[38;5;241m=\u001B[39m is_timestamp_ntz_preferred()\n\u001B[1;32m   1066\u001B[0m schema \u001B[38;5;241m=\u001B[39m reduce(\n\u001B[1;32m   1067\u001B[0m     _merge_type,\n\u001B[1;32m   1068\u001B[0m     (\n\u001B[0;32m-> 1069\u001B[0m         _infer_schema(\n\u001B[1;32m   1070\u001B[0m             row,\n\u001B[1;32m   1071\u001B[0m             names,\n\u001B[1;32m   1072\u001B[0m             infer_dict_as_struct\u001B[38;5;241m=\u001B[39minfer_dict_as_struct,\n\u001B[1;32m   1073\u001B[0m             infer_array_from_first_element\u001B[38;5;241m=\u001B[39minfer_array_from_first_element,\n\u001B[1;32m   1074\u001B[0m             infer_map_from_first_pair\u001B[38;5;241m=\u001B[39minfer_map_from_first_pair,\n\u001B[1;32m   1075\u001B[0m             prefer_timestamp_ntz\u001B[38;5;241m=\u001B[39mprefer_timestamp_ntz,\n\u001B[1;32m   1076\u001B[0m         )\n\u001B[1;32m   1077\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m row \u001B[38;5;129;01min\u001B[39;00m data\n\u001B[1;32m   1078\u001B[0m     ),\n\u001B[1;32m   1079\u001B[0m )\n\u001B[1;32m   1080\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _has_nulltype(schema):\n\u001B[1;32m   1081\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkValueError(\n\u001B[1;32m   1082\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCANNOT_DETERMINE_TYPE\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   1083\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{},\n\u001B[1;32m   1084\u001B[0m     )\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:2320\u001B[0m, in \u001B[0;36m_infer_schema\u001B[0;34m(row, names, infer_dict_as_struct, infer_array_from_first_element, infer_map_from_first_pair, prefer_timestamp_ntz)\u001B[0m\n\u001B[1;32m   2317\u001B[0m     items \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msorted\u001B[39m(row\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__dict__\u001B[39m\u001B[38;5;241m.\u001B[39mitems())\n\u001B[1;32m   2319\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 2320\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n\u001B[1;32m   2321\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCANNOT_INFER_SCHEMA_FOR_TYPE\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   2322\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(row)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n\u001B[1;32m   2323\u001B[0m     )\n\u001B[1;32m   2325\u001B[0m fields \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m   2326\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m items:\n",
        "\u001B[0;31mPySparkTypeError\u001B[0m: [CANNOT_INFER_SCHEMA_FOR_TYPE] Can not infer schema for type: `int`."
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df4 = spark.createDataFrame([x for x in data], [\"Numbers\"])\n",
    "display(df4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95f6b6c8-d630-4a53-9ab5-e99fe17df74b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**❌ What happens with [x for x in data]?**\n",
    "\n",
    "- **[x for x in data]** produces\n",
    "\n",
    "      [15, 25, 36, 44, 57, 65, 89, 95, 9]\n",
    "\n",
    "  - This is a **list of integers, not a list of rows/tuples**.\n",
    "  - These are **integers, not tuples or rows**. Spark doesn't know how to treat an **int as a row**. It expects something like **(15,) or [15]** to match with the column definition **[\"Numbers\"]**.\n",
    "- If you try:\n",
    "\n",
    "      df = spark.createDataFrame([15, 25, 36], [\"Numbers\"])\n",
    "\n",
    "- You'll get an **error** like:\n",
    "\n",
    "      TypeError: StructType can only be created from list or tuple, got <class 'int'>\n",
    "      TypeError: StructType can not be applied to an int\n",
    "\n",
    "  - Because Spark tries to interpret **15 as a row**, and it **can't match** it to the **schema**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f5ba0ee-873d-4870-b283-c68d4988f04d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**✅ Method 02: Alternate using List**\n",
    "\n",
    "- This works too, because each **[x]** is a **one-element list**, Spark can treat this as a **single-row** entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33508ba6-078c-48b3-a42a-350b34529a6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Numbers</th></tr></thead><tbody><tr><td>15</td></tr><tr><td>25</td></tr><tr><td>36</td></tr><tr><td>44</td></tr><tr><td>57</td></tr><tr><td>65</td></tr><tr><td>89</td></tr><tr><td>95</td></tr><tr><td>9</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         15
        ],
        [
         25
        ],
        [
         36
        ],
        [
         44
        ],
        [
         57
        ],
        [
         65
        ],
        [
         89
        ],
        [
         95
        ],
        [
         9
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Numbers",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df4 = spark.createDataFrame([[x] for x in data], [\"Numbers\"])\n",
    "display(df4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0fd5b182-f5fd-40ca-a3b0-524c12f155fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**✅ Method 03: Alternative using Row**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9924b44-3e63-40e8-9596-bbca047880c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Numbers</th></tr></thead><tbody><tr><td>15</td></tr><tr><td>25</td></tr><tr><td>36</td></tr><tr><td>44</td></tr><tr><td>57</td></tr><tr><td>65</td></tr><tr><td>89</td></tr><tr><td>95</td></tr><tr><td>9</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         15
        ],
        [
         25
        ],
        [
         36
        ],
        [
         44
        ],
        [
         57
        ],
        [
         65
        ],
        [
         89
        ],
        [
         95
        ],
        [
         9
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Numbers",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "df = spark.createDataFrame([Row(Numbers=x) for x in data])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59b25aa6-0e0f-4ad9-ab9a-9a762fc5f2f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Summary:**\n",
    "| Expression\t| Works?\t | Reason |\n",
    "|-------------|----------|--------|\n",
    "| [(x,) for x in data]\t| ✅\t| Tuple per row (1-element row). Spark accepts this.|\n",
    "| [[x] for x in data]\t| ✅\t| List per row. Spark accepts this as well. |\n",
    "| [x for x in data]\t| ❌\t| Just a list of integers — Spark can't treat plain ints as row data. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e82d317e-d10e-4a8d-a00a-d6787b5d886c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Method 04: List of tuples (single element)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "547b4bd8-ae27-46dc-9fc2-a1fa4430d9e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th></tr></thead><tbody><tr><td>1</td></tr><tr><td>2</td></tr><tr><td>3</td></tr><tr><td>4</td></tr><tr><td>5</td></tr><tr><td>6</td></tr><tr><td>7</td></tr><tr><td>8</td></tr><tr><td>9</td></tr><tr><td>10</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1
        ],
        [
         2
        ],
        [
         3
        ],
        [
         4
        ],
        [
         5
        ],
        [
         6
        ],
        [
         7
        ],
        [
         8
        ],
        [
         9
        ],
        [
         10
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a sample DataFrame\n",
    "data = [(1,), (2,), (3,), (4,), (5,), (6,), (7,), (8,), (9,), (10,)]\n",
    "df = spark.createDataFrame(data, [\"id\"])\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "5_Why use [(x,) for x in data]?",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}