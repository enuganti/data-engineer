{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ba92a20-afd4-4654-9297-de0bfe99f8b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Why lit() is Used**\n",
    "\n",
    "      array([lit(x) for x in fixed_values_cust])\n",
    "      \n",
    "- The elements of **fixed_values_cust** are simple **Python integers**, and to use them in Spark expressions like **array**, they must be **converted** to **PySpark column-compatible** literals.\n",
    "\n",
    "- **Without lit()**, Spark would **not recognize** the elements as **valid column expressions**, and the code would throw an **error**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "751538b0-dc74-4846-bdff-575a343f9d74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[10, 13, 15, 20, 25, 28, 30, 35, 38, 40, 45]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the fixed integer values\n",
    "fixed_values_cust = [10, 13, 15, 20, 25, 28, 30, 35, 38, 40, 45]\n",
    "\n",
    "[x for x in fixed_values_cust]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74079361-3371-4d2d-8870-3d76a49e5163",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Column<'10'>,\n",
       " Column<'13'>,\n",
       " Column<'15'>,\n",
       " Column<'20'>,\n",
       " Column<'25'>,\n",
       " Column<'28'>,\n",
       " Column<'30'>,\n",
       " Column<'35'>,\n",
       " Column<'38'>,\n",
       " Column<'40'>,\n",
       " Column<'45'>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "[lit(x) for x in fixed_values_cust]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "069c382f-3255-4885-8385-a749da4d8fd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "     [\n",
    "        Column<'10'>,  # A PySpark column object for the literal value 10\n",
    "        Column<'13'>,  # A PySpark column object for the literal value 13\n",
    "        Column<'15'>,  # A PySpark column object for the literal value 15\n",
    "        Column<'20'>,  # A PySpark column object for the literal value 20\n",
    "        Column<'25'>,  # A PySpark column object for the literal value 25\n",
    "        Column<'28'>,  # A PySpark column object for the literal value 28\n",
    "        Column<'30'>, # A PySpark column object for the literal value 30\n",
    "        Column<'35'>, # A PySpark column object for the literal value 35\n",
    "        Column<'38'>, # A PySpark column object for the literal value 38\n",
    "        Column<'40'>, # A PySpark column object for the literal value 40\n",
    "        Column<'45'>  # A PySpark column object for the literal value 45\n",
    "     ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "937543f0-4ecb-4524-a5e5-6888a50314b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### **Explanation:**\n",
    "\n",
    "      array([lit(x) for x in fixed_values_cust]\n",
    "\n",
    "**fixed_values_cust:**\n",
    "- This is a **Python list** containing predefined **numeric values**:\n",
    "\n",
    "    [10, 13, 15, 20, 25, 28, 30, 35, 38, 40, 45]\n",
    "\n",
    "**lit(x):**\n",
    "\n",
    "- The **lit()** function in PySpark creates a **column object** representing a **literal value (a constant)**.\n",
    "\n",
    "- For each element **x** in the **fixed_values_cust** list, **lit(x)** converts it into a **PySpark literal column**.\n",
    "\n",
    "- **lit(x)** converts each element **x** from the **Python list** a into a **Spark Column type**, which is necessary for the **array()** function to create a **new array column** within the **DataFrame**.\n",
    "\n",
    "**List Comprehension:**\n",
    "\n",
    "- The comprehension **[lit(x) for x in fixed_values_cust]** iterates over every value **x** in the list **fixed_values_cust** and applies the **lit(x)** function to it.\n",
    "\n",
    "- As a result, it produces a new list where each item is a PySpark column object representing the corresponding value from **fixed_values_cust**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d76b4730-8c05-4d5a-9188-30dc3d6bf848",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Imagine you want to add a new column named **source** with the value **manual** to every row of your DataFrame.\n",
    "\n",
    "- You would use **df.withColumn(\"source\", lit(\"manual\"))**.\n",
    "\n",
    "- If you tried **df.withColumn(\"source\", \"manual\")** directly, it would likely result in an **error** because **manual is a Python string, not a PySpark Column**.\n",
    "\n",
    "- While **list comprehensions** themselves are a **Python construct** for creating **lists**, their **integration** with **PySpark** operations often necessitates **lit()** to **bridge the gap** between **Python literals and PySpark's Column-based operations**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d5ca8c8-4364-4f3b-9891-7a196a245e10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col, array, array_contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d4bd625-2152-440e-8001-d56cac02876a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Column<'array(10, 13, 15, 20, 25, 28, 30, 35, 38, 40, 45)'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converts the `fixed_values_cust` Python list into a `PySpark array column` where each element is wrapped as a literal (`lit`).\n",
    "array([lit(x) for x in fixed_values_cust])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e390d75e-c8bb-48b7-a254-64e6d6ef15ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Column<'array(10, 13, 15, 20, 25, 28, 30, 35, 38, 40, 45)'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array(*[lit(x) for x in fixed_values_cust])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c6d9473-20ce-4df0-ac58-647c321d1c43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Why Use array for List Comprehension?**\n",
    "\n",
    "- **Consolidate Fixed Values into a Single Data Structure**:\n",
    "\n",
    "  - The **[lit(x) for x in fixed_values_cust]** generates a **list of PySpark literal column objects**. However, PySpark operations, such as **indexing or random selection**, cannot directly operate on a Python list.\n",
    "  \n",
    "  - The **array()** function **combines** these individual **column literals into a single PySpark array column**, which is a valid column type for further DataFrame operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d88cd72d-d219-43b9-883a-b5bfb0b3a819",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>array_col_wrong</th></tr></thead><tbody><tr><td>0</td><td>List(10, 13, 15)</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         0,
         [
          10,
          13,
          15
         ]
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "array_col_wrong",
         "type": "{\"type\":\"array\",\"elementType\":\"integer\",\"containsNull\":false}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>array_col_right</th></tr></thead><tbody><tr><td>0</td><td>List(10, 13, 15)</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         0,
         [
          10,
          13,
          15
         ]
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "array_col_right",
         "type": "{\"type\":\"array\",\"elementType\":\"integer\",\"containsNull\":false}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = spark.range(1)  # dummy row\n",
    "fixed_values_cust = [10, 13, 15]\n",
    "\n",
    "# Case 1 - without unpacking\n",
    "df1 = df.withColumn(\"array_col_wrong\", array([lit(x) for x in fixed_values_cust]))\n",
    "\n",
    "# Case 2 - with unpacking\n",
    "df2 = df.withColumn(\"array_col_right\", array(*[lit(x) for x in fixed_values_cust]))\n",
    "\n",
    "display(df1)\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7008854-b257-42d2-92bb-4a7ac520297a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "     +-------------------+\n",
    "     |  array_col_wrong  |\n",
    "     +-------------------+\n",
    "     |  [[10, 13, 15]]   |    <-- nested array (1 element inside)\n",
    "     +-------------------+\n",
    "\n",
    "     +-------------------+\n",
    "     |  array_col_right  |\n",
    "     +-------------------+\n",
    "     |   [10, 13, 15]    |    <-- correct array\n",
    "     +-------------------+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef9659d9-0d6b-4d8f-a0a1-a5bf1c168ce2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n'Project [id#24L, 'array(10, 13, 15) AS array_col_wrong#26]\n+- Range (0, 1, step=1, splits=Some(8))\n\n== Analyzed Logical Plan ==\nid: bigint, array_col_wrong: array<int>\nProject [id#24L, array(10, 13, 15) AS array_col_wrong#26]\n+- Range (0, 1, step=1, splits=Some(8))\n\n== Optimized Logical Plan ==\nProject [id#24L, [10,13,15] AS array_col_wrong#26]\n+- Range (0, 1, step=1, splits=Some(8))\n\n== Physical Plan ==\n*(1) Project [id#24L, [10,13,15] AS array_col_wrong#26]\n+- *(1) Range (0, 1, step=1, splits=8)\n\n== Parsed Logical Plan ==\n'Project [id#24L, 'array(10, 13, 15) AS array_col_right#29]\n+- Range (0, 1, step=1, splits=Some(8))\n\n== Analyzed Logical Plan ==\nid: bigint, array_col_right: array<int>\nProject [id#24L, array(10, 13, 15) AS array_col_right#29]\n+- Range (0, 1, step=1, splits=Some(8))\n\n== Optimized Logical Plan ==\nProject [id#24L, [10,13,15] AS array_col_right#29]\n+- Range (0, 1, step=1, splits=Some(8))\n\n== Physical Plan ==\n*(1) Project [id#24L, [10,13,15] AS array_col_right#29]\n+- *(1) Range (0, 1, step=1, splits=8)\n\n"
     ]
    }
   ],
   "source": [
    "df1.explain(True)\n",
    "df2.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e3d5f93-8e39-4f52-9cb8-404fe3723168",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>Name</th><th>Age</th><th>Department</th></tr></thead><tbody><tr><td>1</td><td>Rakesh</td><td>25</td><td>Sales</td></tr><tr><td>2</td><td>Kiran</td><td>29</td><td>Admin</td></tr><tr><td>3</td><td>Preeti</td><td>31</td><td>Marketing</td></tr><tr><td>4</td><td>Subash</td><td>33</td><td>HR</td></tr><tr><td>5</td><td>Sekhar</td><td>35</td><td>Maintenance</td></tr><tr><td>6</td><td>Nirmal</td><td>55</td><td>Security</td></tr><tr><td>7</td><td>Sailesh</td><td>35</td><td>IT</td></tr><tr><td>8</td><td>kumar</td><td>29</td><td>Sales</td></tr><tr><td>9</td><td>Asif</td><td>39</td><td>HR</td></tr><tr><td>10</td><td>Murugan</td><td>40</td><td>Admin</td></tr><tr><td>11</td><td>Prakash</td><td>45</td><td>Marketing</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Rakesh",
         25,
         "Sales"
        ],
        [
         2,
         "Kiran",
         29,
         "Admin"
        ],
        [
         3,
         "Preeti",
         31,
         "Marketing"
        ],
        [
         4,
         "Subash",
         33,
         "HR"
        ],
        [
         5,
         "Sekhar",
         35,
         "Maintenance"
        ],
        [
         6,
         "Nirmal",
         55,
         "Security"
        ],
        [
         7,
         "Sailesh",
         35,
         "IT"
        ],
        [
         8,
         "kumar",
         29,
         "Sales"
        ],
        [
         9,
         "Asif",
         39,
         "HR"
        ],
        [
         10,
         "Murugan",
         40,
         "Admin"
        ],
        [
         11,
         "Prakash",
         45,
         "Marketing"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Age",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Department",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [(1, \"Rakesh\", 25, \"Sales\"),\n",
    "        (2, \"Kiran\", 29, \"Admin\"),\n",
    "        (3, \"Preeti\", 31, \"Marketing\"),\n",
    "        (4, \"Subash\", 33, \"HR\"),\n",
    "        (5, \"Sekhar\", 35, \"Maintenance\"),\n",
    "        (6, \"Nirmal\", 55, \"Security\"),\n",
    "        (7, \"Sailesh\", 35, \"IT\"),\n",
    "        (8, \"kumar\", 29, \"Sales\"),\n",
    "        (9, \"Asif\", 39, \"HR\"),\n",
    "        (10, \"Murugan\", 40, \"Admin\"),\n",
    "        (11, \"Prakash\", 45, \"Marketing\")]\n",
    "\n",
    "columns = [\"id\", \"Name\", \"Age\", \"Department\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "658dbafd-e7f8-4b60-841f-e1b135bd7912",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Error: without lit()"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPySparkTypeError\u001B[0m                          Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1884206424256289>, line 2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Add a column with random fixed values\u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m df_with_fixed_value33 \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfixed_value\u001B[39m\u001B[38;5;124m'\u001B[39m, array([x \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m fixed_values_cust]))\n",
       "\u001B[1;32m      3\u001B[0m display(df_with_fixed_value33)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py:264\u001B[0m, in \u001B[0;36mtry_remote_functions.<locals>.wrapped\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    262\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(functions, f\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m    263\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m--> 264\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/functions/builtin.py:13499\u001B[0m, in \u001B[0;36marray\u001B[0;34m(*cols)\u001B[0m\n",
       "\u001B[1;32m  13497\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(cols) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(cols[\u001B[38;5;241m0\u001B[39m], (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mset\u001B[39m)):\n",
       "\u001B[1;32m  13498\u001B[0m     cols \u001B[38;5;241m=\u001B[39m cols[\u001B[38;5;241m0\u001B[39m]  \u001B[38;5;66;03m# type: ignore[assignment]\u001B[39;00m\n",
       "\u001B[0;32m> 13499\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _invoke_function_over_seq_of_columns(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marray\u001B[39m\u001B[38;5;124m\"\u001B[39m, cols)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/functions/builtin.py:120\u001B[0m, in \u001B[0;36m_invoke_function_over_seq_of_columns\u001B[0;34m(name, cols)\u001B[0m\n",
       "\u001B[1;32m    115\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m    116\u001B[0m \u001B[38;5;124;03mInvokes unary JVM function identified by name with\u001B[39;00m\n",
       "\u001B[1;32m    117\u001B[0m \u001B[38;5;124;03mand wraps the result with :class:`~pyspark.sql.Column`.\u001B[39;00m\n",
       "\u001B[1;32m    118\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m    119\u001B[0m sc \u001B[38;5;241m=\u001B[39m _get_active_spark_context()\n",
       "\u001B[0;32m--> 120\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _invoke_function(name, _to_seq(sc, cols, _to_java_column))\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/column.py:106\u001B[0m, in \u001B[0;36m_to_seq\u001B[0;34m(sc, cols, converter)\u001B[0m\n",
       "\u001B[1;32m     99\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m    100\u001B[0m \u001B[38;5;124;03mConvert a list of Columns (or names) into a JVM Seq of Column.\u001B[39;00m\n",
       "\u001B[1;32m    101\u001B[0m \n",
       "\u001B[1;32m    102\u001B[0m \u001B[38;5;124;03mAn optional `converter` could be used to convert items in `cols`\u001B[39;00m\n",
       "\u001B[1;32m    103\u001B[0m \u001B[38;5;124;03minto JVM Column objects.\u001B[39;00m\n",
       "\u001B[1;32m    104\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m    105\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m converter:\n",
       "\u001B[0;32m--> 106\u001B[0m     cols \u001B[38;5;241m=\u001B[39m [converter(c) \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m cols]\n",
       "\u001B[1;32m    107\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m sc\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    108\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m sc\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mPythonUtils\u001B[38;5;241m.\u001B[39mtoSeq(cols)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/column.py:106\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n",
       "\u001B[1;32m     99\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m    100\u001B[0m \u001B[38;5;124;03mConvert a list of Columns (or names) into a JVM Seq of Column.\u001B[39;00m\n",
       "\u001B[1;32m    101\u001B[0m \n",
       "\u001B[1;32m    102\u001B[0m \u001B[38;5;124;03mAn optional `converter` could be used to convert items in `cols`\u001B[39;00m\n",
       "\u001B[1;32m    103\u001B[0m \u001B[38;5;124;03minto JVM Column objects.\u001B[39;00m\n",
       "\u001B[1;32m    104\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m    105\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m converter:\n",
       "\u001B[0;32m--> 106\u001B[0m     cols \u001B[38;5;241m=\u001B[39m [converter(c) \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m cols]\n",
       "\u001B[1;32m    107\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m sc\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    108\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m sc\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mPythonUtils\u001B[38;5;241m.\u001B[39mtoSeq(cols)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/column.py:69\u001B[0m, in \u001B[0;36m_to_java_column\u001B[0;34m(col)\u001B[0m\n",
       "\u001B[1;32m     67\u001B[0m     jcol \u001B[38;5;241m=\u001B[39m _create_column_from_name(col)\n",
       "\u001B[1;32m     68\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m---> 69\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n",
       "\u001B[1;32m     70\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_COLUMN_OR_STR\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m     71\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcol\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(col)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n",
       "\u001B[1;32m     72\u001B[0m     )\n",
       "\u001B[1;32m     73\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m jcol\n",
       "\n",
       "\u001B[0;31mPySparkTypeError\u001B[0m: [NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got int."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "PySparkTypeError",
        "evalue": "[NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got int."
       },
       "metadata": {
        "errorSummary": "[NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got int."
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "NOT_COLUMN_OR_STR",
        "pysparkCallSite": null,
        "pysparkFragment": null,
        "pysparkSummary": null,
        "sqlState": null,
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mPySparkTypeError\u001B[0m                          Traceback (most recent call last)",
        "File \u001B[0;32m<command-1884206424256289>, line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Add a column with random fixed values\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m df_with_fixed_value33 \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfixed_value\u001B[39m\u001B[38;5;124m'\u001B[39m, array([x \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m fixed_values_cust]))\n\u001B[1;32m      3\u001B[0m display(df_with_fixed_value33)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py:264\u001B[0m, in \u001B[0;36mtry_remote_functions.<locals>.wrapped\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    262\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(functions, f\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    263\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 264\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/functions/builtin.py:13499\u001B[0m, in \u001B[0;36marray\u001B[0;34m(*cols)\u001B[0m\n\u001B[1;32m  13497\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(cols) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(cols[\u001B[38;5;241m0\u001B[39m], (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mset\u001B[39m)):\n\u001B[1;32m  13498\u001B[0m     cols \u001B[38;5;241m=\u001B[39m cols[\u001B[38;5;241m0\u001B[39m]  \u001B[38;5;66;03m# type: ignore[assignment]\u001B[39;00m\n\u001B[0;32m> 13499\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _invoke_function_over_seq_of_columns(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marray\u001B[39m\u001B[38;5;124m\"\u001B[39m, cols)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/functions/builtin.py:120\u001B[0m, in \u001B[0;36m_invoke_function_over_seq_of_columns\u001B[0;34m(name, cols)\u001B[0m\n\u001B[1;32m    115\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    116\u001B[0m \u001B[38;5;124;03mInvokes unary JVM function identified by name with\u001B[39;00m\n\u001B[1;32m    117\u001B[0m \u001B[38;5;124;03mand wraps the result with :class:`~pyspark.sql.Column`.\u001B[39;00m\n\u001B[1;32m    118\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    119\u001B[0m sc \u001B[38;5;241m=\u001B[39m _get_active_spark_context()\n\u001B[0;32m--> 120\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _invoke_function(name, _to_seq(sc, cols, _to_java_column))\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/column.py:106\u001B[0m, in \u001B[0;36m_to_seq\u001B[0;34m(sc, cols, converter)\u001B[0m\n\u001B[1;32m     99\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    100\u001B[0m \u001B[38;5;124;03mConvert a list of Columns (or names) into a JVM Seq of Column.\u001B[39;00m\n\u001B[1;32m    101\u001B[0m \n\u001B[1;32m    102\u001B[0m \u001B[38;5;124;03mAn optional `converter` could be used to convert items in `cols`\u001B[39;00m\n\u001B[1;32m    103\u001B[0m \u001B[38;5;124;03minto JVM Column objects.\u001B[39;00m\n\u001B[1;32m    104\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    105\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m converter:\n\u001B[0;32m--> 106\u001B[0m     cols \u001B[38;5;241m=\u001B[39m [converter(c) \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m cols]\n\u001B[1;32m    107\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m sc\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    108\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m sc\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mPythonUtils\u001B[38;5;241m.\u001B[39mtoSeq(cols)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/column.py:106\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     99\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    100\u001B[0m \u001B[38;5;124;03mConvert a list of Columns (or names) into a JVM Seq of Column.\u001B[39;00m\n\u001B[1;32m    101\u001B[0m \n\u001B[1;32m    102\u001B[0m \u001B[38;5;124;03mAn optional `converter` could be used to convert items in `cols`\u001B[39;00m\n\u001B[1;32m    103\u001B[0m \u001B[38;5;124;03minto JVM Column objects.\u001B[39;00m\n\u001B[1;32m    104\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    105\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m converter:\n\u001B[0;32m--> 106\u001B[0m     cols \u001B[38;5;241m=\u001B[39m [converter(c) \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m cols]\n\u001B[1;32m    107\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m sc\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    108\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m sc\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mPythonUtils\u001B[38;5;241m.\u001B[39mtoSeq(cols)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/column.py:69\u001B[0m, in \u001B[0;36m_to_java_column\u001B[0;34m(col)\u001B[0m\n\u001B[1;32m     67\u001B[0m     jcol \u001B[38;5;241m=\u001B[39m _create_column_from_name(col)\n\u001B[1;32m     68\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 69\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n\u001B[1;32m     70\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_COLUMN_OR_STR\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     71\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcol\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(col)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n\u001B[1;32m     72\u001B[0m     )\n\u001B[1;32m     73\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m jcol\n",
        "\u001B[0;31mPySparkTypeError\u001B[0m: [NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got int."
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add a column with random fixed values\n",
    "df_with_fixed_value33 = df.withColumn('fixed_value', array([x for x in fixed_values_cust]))\n",
    "display(df_with_fixed_value33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62cced40-366a-4cab-825b-b2f4e2515665",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>Name</th><th>Age</th><th>Department</th><th>fixed_value</th><th>fixed_value_unpack</th></tr></thead><tbody><tr><td>1</td><td>Rakesh</td><td>25</td><td>Sales</td><td>List(10, 13, 15, 20, 25, 28, 30, 35, 38, 40, 45)</td><td>List(10, 13, 15, 20, 25, 28, 30, 35, 38, 40, 45)</td></tr><tr><td>2</td><td>Kiran</td><td>29</td><td>Admin</td><td>List(10, 13, 15, 20, 25, 28, 30, 35, 38, 40, 45)</td><td>List(10, 13, 15, 20, 25, 28, 30, 35, 38, 40, 45)</td></tr><tr><td>3</td><td>Preeti</td><td>31</td><td>Marketing</td><td>List(10, 13, 15, 20, 25, 28, 30, 35, 38, 40, 45)</td><td>List(10, 13, 15, 20, 25, 28, 30, 35, 38, 40, 45)</td></tr><tr><td>4</td><td>Subash</td><td>33</td><td>HR</td><td>List(10, 13, 15, 20, 25, 28, 30, 35, 38, 40, 45)</td><td>List(10, 13, 15, 20, 25, 28, 30, 35, 38, 40, 45)</td></tr><tr><td>5</td><td>Sekhar</td><td>35</td><td>Maintenance</td><td>List(10, 13, 15, 20, 25, 28, 30, 35, 38, 40, 45)</td><td>List(10, 13, 15, 20, 25, 28, 30, 35, 38, 40, 45)</td></tr><tr><td>6</td><td>Nirmal</td><td>55</td><td>Security</td><td>List(10, 13, 15, 20, 25, 28, 30, 35, 38, 40, 45)</td><td>List(10, 13, 15, 20, 25, 28, 30, 35, 38, 40, 45)</td></tr><tr><td>7</td><td>Sailesh</td><td>35</td><td>IT</td><td>List(10, 13, 15, 20, 25, 28, 30, 35, 38, 40, 45)</td><td>List(10, 13, 15, 20, 25, 28, 30, 35, 38, 40, 45)</td></tr><tr><td>8</td><td>kumar</td><td>29</td><td>Sales</td><td>List(10, 13, 15, 20, 25, 28, 30, 35, 38, 40, 45)</td><td>List(10, 13, 15, 20, 25, 28, 30, 35, 38, 40, 45)</td></tr><tr><td>9</td><td>Asif</td><td>39</td><td>HR</td><td>List(10, 13, 15, 20, 25, 28, 30, 35, 38, 40, 45)</td><td>List(10, 13, 15, 20, 25, 28, 30, 35, 38, 40, 45)</td></tr><tr><td>10</td><td>Murugan</td><td>40</td><td>Admin</td><td>List(10, 13, 15, 20, 25, 28, 30, 35, 38, 40, 45)</td><td>List(10, 13, 15, 20, 25, 28, 30, 35, 38, 40, 45)</td></tr><tr><td>11</td><td>Prakash</td><td>45</td><td>Marketing</td><td>List(10, 13, 15, 20, 25, 28, 30, 35, 38, 40, 45)</td><td>List(10, 13, 15, 20, 25, 28, 30, 35, 38, 40, 45)</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Rakesh",
         25,
         "Sales",
         [
          10,
          13,
          15,
          20,
          25,
          28,
          30,
          35,
          38,
          40,
          45
         ],
         [
          10,
          13,
          15,
          20,
          25,
          28,
          30,
          35,
          38,
          40,
          45
         ]
        ],
        [
         2,
         "Kiran",
         29,
         "Admin",
         [
          10,
          13,
          15,
          20,
          25,
          28,
          30,
          35,
          38,
          40,
          45
         ],
         [
          10,
          13,
          15,
          20,
          25,
          28,
          30,
          35,
          38,
          40,
          45
         ]
        ],
        [
         3,
         "Preeti",
         31,
         "Marketing",
         [
          10,
          13,
          15,
          20,
          25,
          28,
          30,
          35,
          38,
          40,
          45
         ],
         [
          10,
          13,
          15,
          20,
          25,
          28,
          30,
          35,
          38,
          40,
          45
         ]
        ],
        [
         4,
         "Subash",
         33,
         "HR",
         [
          10,
          13,
          15,
          20,
          25,
          28,
          30,
          35,
          38,
          40,
          45
         ],
         [
          10,
          13,
          15,
          20,
          25,
          28,
          30,
          35,
          38,
          40,
          45
         ]
        ],
        [
         5,
         "Sekhar",
         35,
         "Maintenance",
         [
          10,
          13,
          15,
          20,
          25,
          28,
          30,
          35,
          38,
          40,
          45
         ],
         [
          10,
          13,
          15,
          20,
          25,
          28,
          30,
          35,
          38,
          40,
          45
         ]
        ],
        [
         6,
         "Nirmal",
         55,
         "Security",
         [
          10,
          13,
          15,
          20,
          25,
          28,
          30,
          35,
          38,
          40,
          45
         ],
         [
          10,
          13,
          15,
          20,
          25,
          28,
          30,
          35,
          38,
          40,
          45
         ]
        ],
        [
         7,
         "Sailesh",
         35,
         "IT",
         [
          10,
          13,
          15,
          20,
          25,
          28,
          30,
          35,
          38,
          40,
          45
         ],
         [
          10,
          13,
          15,
          20,
          25,
          28,
          30,
          35,
          38,
          40,
          45
         ]
        ],
        [
         8,
         "kumar",
         29,
         "Sales",
         [
          10,
          13,
          15,
          20,
          25,
          28,
          30,
          35,
          38,
          40,
          45
         ],
         [
          10,
          13,
          15,
          20,
          25,
          28,
          30,
          35,
          38,
          40,
          45
         ]
        ],
        [
         9,
         "Asif",
         39,
         "HR",
         [
          10,
          13,
          15,
          20,
          25,
          28,
          30,
          35,
          38,
          40,
          45
         ],
         [
          10,
          13,
          15,
          20,
          25,
          28,
          30,
          35,
          38,
          40,
          45
         ]
        ],
        [
         10,
         "Murugan",
         40,
         "Admin",
         [
          10,
          13,
          15,
          20,
          25,
          28,
          30,
          35,
          38,
          40,
          45
         ],
         [
          10,
          13,
          15,
          20,
          25,
          28,
          30,
          35,
          38,
          40,
          45
         ]
        ],
        [
         11,
         "Prakash",
         45,
         "Marketing",
         [
          10,
          13,
          15,
          20,
          25,
          28,
          30,
          35,
          38,
          40,
          45
         ],
         [
          10,
          13,
          15,
          20,
          25,
          28,
          30,
          35,
          38,
          40,
          45
         ]
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Age",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Department",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "fixed_value",
         "type": "{\"type\":\"array\",\"elementType\":\"integer\",\"containsNull\":false}"
        },
        {
         "metadata": "{}",
         "name": "fixed_value_unpack",
         "type": "{\"type\":\"array\",\"elementType\":\"integer\",\"containsNull\":false}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add a column with random fixed values\n",
    "df_with_fixed_value3 = df.withColumn('fixed_value', array([lit(x) for x in fixed_values_cust])) \\\n",
    "                         .withColumn('fixed_value_unpack', array(*[lit(x) for x in fixed_values_cust]))\n",
    "display(df_with_fixed_value3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc20784a-434f-4047-9f7f-30f90e8f9a99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>Name</th><th>Age</th><th>Department</th><th>fixed_value</th></tr></thead><tbody><tr><td>1</td><td>Rakesh</td><td>25</td><td>Sales</td><td>10</td></tr><tr><td>2</td><td>Kiran</td><td>29</td><td>Admin</td><td>10</td></tr><tr><td>3</td><td>Preeti</td><td>31</td><td>Marketing</td><td>10</td></tr><tr><td>4</td><td>Subash</td><td>33</td><td>HR</td><td>10</td></tr><tr><td>5</td><td>Sekhar</td><td>35</td><td>Maintenance</td><td>10</td></tr><tr><td>6</td><td>Nirmal</td><td>55</td><td>Security</td><td>10</td></tr><tr><td>7</td><td>Sailesh</td><td>35</td><td>IT</td><td>10</td></tr><tr><td>8</td><td>kumar</td><td>29</td><td>Sales</td><td>10</td></tr><tr><td>9</td><td>Asif</td><td>39</td><td>HR</td><td>10</td></tr><tr><td>10</td><td>Murugan</td><td>40</td><td>Admin</td><td>10</td></tr><tr><td>11</td><td>Prakash</td><td>45</td><td>Marketing</td><td>10</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Rakesh",
         25,
         "Sales",
         10
        ],
        [
         2,
         "Kiran",
         29,
         "Admin",
         10
        ],
        [
         3,
         "Preeti",
         31,
         "Marketing",
         10
        ],
        [
         4,
         "Subash",
         33,
         "HR",
         10
        ],
        [
         5,
         "Sekhar",
         35,
         "Maintenance",
         10
        ],
        [
         6,
         "Nirmal",
         55,
         "Security",
         10
        ],
        [
         7,
         "Sailesh",
         35,
         "IT",
         10
        ],
        [
         8,
         "kumar",
         29,
         "Sales",
         10
        ],
        [
         9,
         "Asif",
         39,
         "HR",
         10
        ],
        [
         10,
         "Murugan",
         40,
         "Admin",
         10
        ],
        [
         11,
         "Prakash",
         45,
         "Marketing",
         10
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Age",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Department",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "fixed_value",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add a column with random fixed values\n",
    "df_with_fixed_value4 = df.withColumn(\n",
    "    'fixed_value', \n",
    "    array([lit(x) for x in fixed_values_cust])[0]\n",
    ")\n",
    "display(df_with_fixed_value4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dceec173-b55f-4ee3-b236-b88749fa779d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Filtering Rows (to check if a column value is in a fixed list?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7c76411-42ab-4506-9c8b-8745a636fa11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>Name</th><th>Age</th><th>Department</th><th>fixed_value</th></tr></thead><tbody><tr><td>1</td><td>Rakesh</td><td>25</td><td>Sales</td><td>List(10, 13, 15, 20, 25, 28, 30, 35, 38, 40, 45)</td></tr><tr><td>5</td><td>Sekhar</td><td>35</td><td>Maintenance</td><td>List(10, 13, 15, 20, 25, 28, 30, 35, 38, 40, 45)</td></tr><tr><td>7</td><td>Sailesh</td><td>35</td><td>IT</td><td>List(10, 13, 15, 20, 25, 28, 30, 35, 38, 40, 45)</td></tr><tr><td>10</td><td>Murugan</td><td>40</td><td>Admin</td><td>List(10, 13, 15, 20, 25, 28, 30, 35, 38, 40, 45)</td></tr><tr><td>11</td><td>Prakash</td><td>45</td><td>Marketing</td><td>List(10, 13, 15, 20, 25, 28, 30, 35, 38, 40, 45)</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Rakesh",
         25,
         "Sales",
         [
          10,
          13,
          15,
          20,
          25,
          28,
          30,
          35,
          38,
          40,
          45
         ]
        ],
        [
         5,
         "Sekhar",
         35,
         "Maintenance",
         [
          10,
          13,
          15,
          20,
          25,
          28,
          30,
          35,
          38,
          40,
          45
         ]
        ],
        [
         7,
         "Sailesh",
         35,
         "IT",
         [
          10,
          13,
          15,
          20,
          25,
          28,
          30,
          35,
          38,
          40,
          45
         ]
        ],
        [
         10,
         "Murugan",
         40,
         "Admin",
         [
          10,
          13,
          15,
          20,
          25,
          28,
          30,
          35,
          38,
          40,
          45
         ]
        ],
        [
         11,
         "Prakash",
         45,
         "Marketing",
         [
          10,
          13,
          15,
          20,
          25,
          28,
          30,
          35,
          38,
          40,
          45
         ]
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Age",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Department",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "fixed_value",
         "type": "{\"type\":\"array\",\"elementType\":\"integer\",\"containsNull\":false}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fixed_values_cust = [10, 13, 15, 20, 25, 28, 30, 35, 38, 40, 45]\n",
    "\n",
    "# Create an array literal and check if Age is in that array\n",
    "df_filtered = df.withColumn('fixed_value', array([lit(x) for x in fixed_values_cust])) \\\n",
    "                .filter(array_contains(array(*[lit(x) for x in fixed_values_cust]), col(\"Age\")))\n",
    "\n",
    "display(df_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "965380c8-6653-4363-a427-47e5a56e44af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- **array()** needs **multiple column expressions**, not a **single list** of them.\n",
    "- `*` operator (called **unpacking** in Python).\n",
    "- `*` unpacks the list into individual arguments.\n",
    "\n",
    "**without `*`:**\n",
    "\n",
    "        array([lit(10), lit(13), lit(15), lit(20), lit(25), lit(28), lit(30), lit(35), lit(38), lit(40), lit(45)])\n",
    "\n",
    "- This passes a **single list** as **one argument** to array(), which is **not valid for PySpark's array()** function. It expects **multiple column arguments**, not a list of columns.\n",
    "\n",
    "**with `*`:**\n",
    "\n",
    "      array(*[lit(10), lit(13), lit(15), lit(20), lit(25), lit(28), lit(30), lit(35), lit(38), lit(40), lit(45)])\n",
    "\n",
    "      # This unpacks the list so that it becomes:\n",
    "      array(lit(10), lit(13), lit(15), lit(20), lit(25), lit(28), lit(30), lit(35), lit(38), lit(40), lit(45))\n",
    "\n",
    "\n",
    "- **array(*[lit(x) for x in fixed_values_cust])** creates a **literal** array column from your list.\n",
    "- **array_contains(..., col(\"Age\"))** checks if the **Age value exists** in that **array**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d47f5b1-a22e-42aa-b07d-344a51e7b4fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Using in when/case Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc7695e3-3fe5-4fab-995e-ba538e8c192b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>Name</th><th>Age</th><th>Department</th><th>is_fixed</th></tr></thead><tbody><tr><td>1</td><td>Rakesh</td><td>25</td><td>Sales</td><td>true</td></tr><tr><td>2</td><td>Kiran</td><td>29</td><td>Admin</td><td>false</td></tr><tr><td>3</td><td>Preeti</td><td>31</td><td>Marketing</td><td>false</td></tr><tr><td>4</td><td>Subash</td><td>33</td><td>HR</td><td>false</td></tr><tr><td>5</td><td>Sekhar</td><td>35</td><td>Maintenance</td><td>true</td></tr><tr><td>6</td><td>Nirmal</td><td>55</td><td>Security</td><td>false</td></tr><tr><td>7</td><td>Sailesh</td><td>35</td><td>IT</td><td>true</td></tr><tr><td>8</td><td>kumar</td><td>29</td><td>Sales</td><td>false</td></tr><tr><td>9</td><td>Asif</td><td>39</td><td>HR</td><td>false</td></tr><tr><td>10</td><td>Murugan</td><td>40</td><td>Admin</td><td>true</td></tr><tr><td>11</td><td>Prakash</td><td>45</td><td>Marketing</td><td>true</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Rakesh",
         25,
         "Sales",
         true
        ],
        [
         2,
         "Kiran",
         29,
         "Admin",
         false
        ],
        [
         3,
         "Preeti",
         31,
         "Marketing",
         false
        ],
        [
         4,
         "Subash",
         33,
         "HR",
         false
        ],
        [
         5,
         "Sekhar",
         35,
         "Maintenance",
         true
        ],
        [
         6,
         "Nirmal",
         55,
         "Security",
         false
        ],
        [
         7,
         "Sailesh",
         35,
         "IT",
         true
        ],
        [
         8,
         "kumar",
         29,
         "Sales",
         false
        ],
        [
         9,
         "Asif",
         39,
         "HR",
         false
        ],
        [
         10,
         "Murugan",
         40,
         "Admin",
         true
        ],
        [
         11,
         "Prakash",
         45,
         "Marketing",
         true
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Age",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Department",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "is_fixed",
         "type": "\"boolean\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fixed_values_cust = [10, 13, 15, 20, 25, 28, 30, 35, 38, 40, 45]\n",
    "\n",
    "df_caseWhen = df.withColumn(\n",
    "    \"is_fixed\",\n",
    "    when(col(\"Age\").isin(*fixed_values_cust), lit(True)).otherwise(lit(False))\n",
    ")\n",
    "display(df_caseWhen)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "6_why lit() used in array([lit(x) for x in fixed_values_cust])?",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}