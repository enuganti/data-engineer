{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef92b353-976f-41c7-a7d8-785aa0892e43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Why passing argument `date_format=None` in function?\n",
    "- How to **cast** multiple columns to specific data types?\n",
    "- How the table looks before and after casting?\n",
    "- when the type is **\"date\" or \"timestamp\"**, you can **optionally** pass a **date_format**.\n",
    "- so that Spark knows how to parse the **string** values into proper **DateType or TimestampType**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42d0c7ee-a345-4cdb-8906-0f243f2521cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05bb6487-b5e6-497a-bd92-5bf45e498790",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"last_login\":206},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1758460191554}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "date cols: default format"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>age</th><th>salary</th><th>is_active</th><th>join_date</th><th>Exp</th><th>last_login</th></tr></thead><tbody><tr><td>Albert</td><td>25</td><td>55000.50</td><td>true</td><td>2025-08-25</td><td>6</td><td>2025-08-25T15:30:00.000+00:00</td></tr><tr><td>Baskar</td><td>30</td><td>72000.00</td><td>false</td><td>2024-12-31</td><td>7</td><td>2024-12-31T08:45:15.000+00:00</td></tr><tr><td>Chetan</td><td>27</td><td>63000.75</td><td>true</td><td>2023-01-15</td><td>8</td><td>2023-01-15T20:00:00.000+00:00</td></tr><tr><td>Dravid</td><td>26</td><td>66000.50</td><td>true</td><td>2024-06-20</td><td>9</td><td>2024-05-22T15:40:55.000+00:00</td></tr><tr><td>Nishant</td><td>32</td><td>98700.00</td><td>false</td><td>2023-10-21</td><td>10</td><td>2020-10-31T06:45:45.000+00:00</td></tr><tr><td>David</td><td>29</td><td>34512.75</td><td>true</td><td>2023-03-15</td><td>4</td><td>2021-08-28T20:15:55.000+00:00</td></tr><tr><td>Mohan</td><td>33</td><td>34908.50</td><td>true</td><td>2022-04-15</td><td>12</td><td>2019-09-29T19:35:55.000+00:00</td></tr><tr><td>Niroop</td><td>35</td><td>49654.98</td><td>false</td><td>2021-02-18</td><td>3</td><td>2024-05-22T08:45:25.000+00:00</td></tr><tr><td>Pushpa</td><td>23</td><td>44111.99</td><td>true</td><td>2020-07-19</td><td>5</td><td>2023-09-25T20:00:00.000+00:00</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Albert",
         "25",
         "55000.50",
         "true",
         "2025-08-25",
         "6",
         "2025-08-25T15:30:00.000+00:00"
        ],
        [
         "Baskar",
         "30",
         "72000.00",
         "false",
         "2024-12-31",
         "7",
         "2024-12-31T08:45:15.000+00:00"
        ],
        [
         "Chetan",
         "27",
         "63000.75",
         "true",
         "2023-01-15",
         "8",
         "2023-01-15T20:00:00.000+00:00"
        ],
        [
         "Dravid",
         "26",
         "66000.50",
         "true",
         "2024-06-20",
         "9",
         "2024-05-22T15:40:55.000+00:00"
        ],
        [
         "Nishant",
         "32",
         "98700.00",
         "false",
         "2023-10-21",
         "10",
         "2020-10-31T06:45:45.000+00:00"
        ],
        [
         "David",
         "29",
         "34512.75",
         "true",
         "2023-03-15",
         "4",
         "2021-08-28T20:15:55.000+00:00"
        ],
        [
         "Mohan",
         "33",
         "34908.50",
         "true",
         "2022-04-15",
         "12",
         "2019-09-29T19:35:55.000+00:00"
        ],
        [
         "Niroop",
         "35",
         "49654.98",
         "false",
         "2021-02-18",
         "3",
         "2024-05-22T08:45:25.000+00:00"
        ],
        [
         "Pushpa",
         "23",
         "44111.99",
         "true",
         "2020-07-19",
         "5",
         "2023-09-25T20:00:00.000+00:00"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "is_active",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "join_date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Exp",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "last_login",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# All columns are in string type\n",
    "# All rows of date & timestamp are in default format\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (\"Albert\", \"25\", \"55000.50\", \"true\", \"2025-08-25\", \"6\", \"2025-08-25T15:30:00.000+00:00\"),      # All rows of date & timestamp are in default format\n",
    "    (\"Baskar\", \"30\", \"72000.00\", \"false\", \"2024-12-31\", \"7\", \"2024-12-31T08:45:15.000+00:00\"),\n",
    "    (\"Chetan\", \"27\", \"63000.75\", \"true\", \"2023-01-15\", \"8\", \"2023-01-15T20:00:00.000+00:00\"),\n",
    "    (\"Dravid\", \"26\", \"66000.50\", \"true\", \"2024-06-20\", \"9\", \"2024-05-22T15:40:55.000+00:00\"),\n",
    "    (\"Nishant\", \"32\", \"98700.00\", \"false\", \"2023-10-21\", \"10\", \"2020-10-31T06:45:45.000+00:00\"),\n",
    "    (\"David\", \"29\", \"34512.75\", \"true\", \"2023-03-15\", \"4\", \"2021-08-28T20:15:55.000+00:00\"),\n",
    "    (\"Mohan\", \"33\", \"34908.50\", \"true\", \"2022-04-15\", \"12\", \"2019-09-29T19:35:55.000+00:00\"),\n",
    "    (\"Niroop\", \"35\", \"49654.98\", \"false\", \"2021-02-18\", \"3\", \"2024-05-22T08:45:25.000+00:00\"),\n",
    "    (\"Pushpa\", \"23\", \"44111.99\", \"true\", \"2020-07-19\", \"5\", \"2023-09-25T20:00:00.000+00:00\")\n",
    "]\n",
    "\n",
    "# Define schema\n",
    "columns = [\"name\", \"age\", \"salary\", \"is_active\", \"join_date\", \"Exp\" ,\"last_login\"]\n",
    "\n",
    "df_default_format = spark.createDataFrame(data, columns)\n",
    "display(df_default_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a6e59d1-8dfe-4571-ad6c-3d319a6d7f8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "col_type_map = {\n",
    "    \"age\": \"int\",\n",
    "    \"salary\": \"double\",\n",
    "    \"is_active\": \"boolean\",\n",
    "    \"join_date\": \"date\",\n",
    "    \"Exp\": \"Int\",\n",
    "    \"last_login\": \"timestamp\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f7f593f-ffb6-4c04-b356-fe506591ef0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Scenario 01\n",
    "- **date_format=None**\n",
    "- **Default argument behavior**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "414dfa02-51ea-418e-9bea-a14176d71665",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def cast_dataframe_columns_wo(df, col_type_map, date_format=None):\n",
    "    for col_name, data_type in col_type_map.items():\n",
    "        data_type = data_type.lower()\n",
    "\n",
    "        if data_type == \"string\":\n",
    "            df = df.withColumn(col_name, F.col(col_name).cast(\"string\"))\n",
    "\n",
    "        elif data_type in [\"int\", \"Int\", \"integer\"]:\n",
    "            df = df.withColumn(col_name, F.col(col_name).cast(\"int\"))\n",
    "\n",
    "        elif data_type in [\"double\", \"float\"]:\n",
    "            df = df.withColumn(col_name, F.col(col_name).cast(\"double\"))\n",
    "\n",
    "        elif data_type == \"boolean\":\n",
    "            df = df.withColumn(col_name, F.col(col_name).cast(\"boolean\"))\n",
    "\n",
    "        elif data_type == \"date\":\n",
    "            df = df.withColumn(col_name, F.to_date(F.col(col_name)))              # since date & timestamp are in default format, no need to pass date_format\n",
    "\n",
    "        elif data_type == \"timestamp\":\n",
    "            df = df.withColumn(col_name, F.to_timestamp(F.col(col_name)))         # since date & timestamp are in default format, no need to pass date_format\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8b7e417-100d-4e23-a627-c32241d10439",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"last_login\":201},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763794350046}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "default_format"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>age</th><th>salary</th><th>is_active</th><th>join_date</th><th>Exp</th><th>last_login</th></tr></thead><tbody><tr><td>Albert</td><td>25</td><td>55000.5</td><td>true</td><td>2025-08-25</td><td>6</td><td>2025-08-25T15:30:00.000Z</td></tr><tr><td>Baskar</td><td>30</td><td>72000.0</td><td>false</td><td>2024-12-31</td><td>7</td><td>2024-12-31T08:45:15.000Z</td></tr><tr><td>Chetan</td><td>27</td><td>63000.75</td><td>true</td><td>2023-01-15</td><td>8</td><td>2023-01-15T20:00:00.000Z</td></tr><tr><td>Dravid</td><td>26</td><td>66000.5</td><td>true</td><td>2024-06-20</td><td>9</td><td>2024-05-22T15:40:55.000Z</td></tr><tr><td>Nishant</td><td>32</td><td>98700.0</td><td>false</td><td>2023-10-21</td><td>10</td><td>2020-10-31T06:45:45.000Z</td></tr><tr><td>David</td><td>29</td><td>34512.75</td><td>true</td><td>2023-03-15</td><td>4</td><td>2021-08-28T20:15:55.000Z</td></tr><tr><td>Mohan</td><td>33</td><td>34908.5</td><td>true</td><td>2022-04-15</td><td>12</td><td>2019-09-29T19:35:55.000Z</td></tr><tr><td>Niroop</td><td>35</td><td>49654.98</td><td>false</td><td>2021-02-18</td><td>3</td><td>2024-05-22T08:45:25.000Z</td></tr><tr><td>Pushpa</td><td>23</td><td>44111.99</td><td>true</td><td>2020-07-19</td><td>5</td><td>2023-09-25T20:00:00.000Z</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Albert",
         25,
         55000.5,
         true,
         "2025-08-25",
         6,
         "2025-08-25T15:30:00.000Z"
        ],
        [
         "Baskar",
         30,
         72000.0,
         false,
         "2024-12-31",
         7,
         "2024-12-31T08:45:15.000Z"
        ],
        [
         "Chetan",
         27,
         63000.75,
         true,
         "2023-01-15",
         8,
         "2023-01-15T20:00:00.000Z"
        ],
        [
         "Dravid",
         26,
         66000.5,
         true,
         "2024-06-20",
         9,
         "2024-05-22T15:40:55.000Z"
        ],
        [
         "Nishant",
         32,
         98700.0,
         false,
         "2023-10-21",
         10,
         "2020-10-31T06:45:45.000Z"
        ],
        [
         "David",
         29,
         34512.75,
         true,
         "2023-03-15",
         4,
         "2021-08-28T20:15:55.000Z"
        ],
        [
         "Mohan",
         33,
         34908.5,
         true,
         "2022-04-15",
         12,
         "2019-09-29T19:35:55.000Z"
        ],
        [
         "Niroop",
         35,
         49654.98,
         false,
         "2021-02-18",
         3,
         "2024-05-22T08:45:25.000Z"
        ],
        [
         "Pushpa",
         23,
         44111.99,
         true,
         "2020-07-19",
         5,
         "2023-09-25T20:00:00.000Z"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "is_active",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "join_date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "Exp",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "last_login",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply casting with correct date / timestamp format\n",
    "# df_default_format: date & timestamp are in default format\n",
    "# since date & timestamp are in default format, no need to pass date_format -> cast_dataframe_columns_wo(df, col_type_map, date_format=None)\n",
    "\n",
    "df_casted_wo = cast_dataframe_columns_wo(df_default_format, col_type_map)\n",
    "display(df_casted_wo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4629fe85-386b-4474-a003-8ef844add73e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "| Output schema BEFORE casting |   Output schema AFTER casting |\n",
    "|------------------------------|-------------------------------|\n",
    "|         `name`:string        |         `name`:string         |  \n",
    "|         `age`:`string`       |         `age`:`integer`       |\n",
    "|        `salary`:`string`     |        `salary`:`double`      |\n",
    "|       `is_active`:`string`   |      `is_active`:`boolean`    |\n",
    "|       `join_date`:`string`   |        `join_date`:`date`     |\n",
    "|         `Exp`:`string`       |       `Exp`:`integer`         |\n",
    "|       `last_login`:`string`  |     `last_login`:`timestamp`  |    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65869849-cd76-4629-98fd-81d6b44c6866",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "custom_format"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mDateTimeException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2022302555744395>, line 9\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Apply casting with correct date / timestamp format\u001B[39;00m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# df_cust_format: date & timestamp are in custom format\u001B[39;00m\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# since date & timestamp are in custom format, pass date_format -> cast_dataframe_columns_wo(df, col_type_map, date_format=\"%m-%d-%Y\")\u001B[39;00m\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# date_format=\"%m-%d-%Y\" -> month-day-year\u001B[39;00m\n",
       "\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# date_format=\"%m-%d-%Y %H:%M:%S.%f%z\" -> month-day-year hour:minute:second.microsecond\u001B[39;00m\n",
       "\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# since date_format not passed getting error -> [CAST_INVALID_INPUT] The value '08-25-2025' of the type \"STRING\" cannot be cast to \"DATE\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\u001B[39;00m\n",
       "\u001B[1;32m      8\u001B[0m df_casted_cust \u001B[38;5;241m=\u001B[39m cast_dataframe_columns_wo(df_cust_format, col_type_map)\n",
       "\u001B[0;32m----> 9\u001B[0m display(df_casted_cust)\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display.py:133\u001B[0m, in \u001B[0;36mDisplay.display\u001B[0;34m(self, input, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    131\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n",
       "\u001B[1;32m    132\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cf_helper \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28minput\u001B[39m, ConnectDataFrame):\n",
       "\u001B[0;32m--> 133\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdisplay_connect_table(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m    134\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28minput\u001B[39m, ConnectDataFrame):\n",
       "\u001B[1;32m    135\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39misStreaming:\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display.py:97\u001B[0m, in \u001B[0;36mDisplay.display_connect_table\u001B[0;34m(self, df, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     94\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcf_helper\u001B[38;5;241m.\u001B[39mdisplay_streaming_dataframe(df, config, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstreaming_listener,\n",
       "\u001B[1;32m     95\u001B[0m                                                \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m     96\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m---> 97\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcf_helper\u001B[38;5;241m.\u001B[39mdisplay_dataframe(df, config)\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display_helpers/cloudfetch_helper.py:47\u001B[0m, in \u001B[0;36mCloudFetchDisplayHelper.display_dataframe\u001B[0;34m(self, df, config)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdisplay_dataframe\u001B[39m(\u001B[38;5;28mself\u001B[39m, df: ConnectDataFrame, config: TableDisplayConfig):\n",
       "\u001B[0;32m---> 47\u001B[0m     display_payload \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwrite_to_cloudfetch(df, config)\n",
       "\u001B[1;32m     48\u001B[0m     ip_display({\n",
       "\u001B[1;32m     49\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mapplication/vnd.databricks.connect.display\u001B[39m\u001B[38;5;124m\"\u001B[39m: display_payload,\n",
       "\u001B[1;32m     50\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext/plain\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDatabricks Spark Connect Table\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m     51\u001B[0m     },\n",
       "\u001B[1;32m     52\u001B[0m                raw\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display_helpers/cloudfetch_helper.py:133\u001B[0m, in \u001B[0;36mCloudFetchDisplayHelper.write_to_cloudfetch\u001B[0;34m(self, connectDataFrame, config)\u001B[0m\n",
       "\u001B[1;32m    131\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
       "\u001B[1;32m    132\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m PySparkException \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[0;32m--> 133\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
       "\u001B[1;32m    134\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    135\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(\n",
       "\u001B[1;32m    136\u001B[0m         e\n",
       "\u001B[1;32m    137\u001B[0m     )(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEncountered an unexpected error when displaying table, please contact Databricks support\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    138\u001B[0m       ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display_helpers/cloudfetch_helper.py:102\u001B[0m, in \u001B[0;36mCloudFetchDisplayHelper.write_to_cloudfetch\u001B[0;34m(self, connectDataFrame, config)\u001B[0m\n",
       "\u001B[1;32m     99\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m rowLimit \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m    100\u001B[0m     connectDataFrame \u001B[38;5;241m=\u001B[39m cast(ConnectDataFrame, connectDataFrame\u001B[38;5;241m.\u001B[39mlimit(rowLimit \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m))\n",
       "\u001B[1;32m    101\u001B[0m results: Tuple[Optional[StructType], List[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCloudFetchResult\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n",
       "\u001B[0;32m--> 102\u001B[0m                List[\u001B[38;5;28mbool\u001B[39m]] \u001B[38;5;241m=\u001B[39m connectDataFrame\u001B[38;5;241m.\u001B[39m_to_cloudfetch_with_limits_and_file_paths(\n",
       "\u001B[1;32m    103\u001B[0m                    \u001B[38;5;28mformat\u001B[39m\u001B[38;5;241m=\u001B[39mstr_format,\n",
       "\u001B[1;32m    104\u001B[0m                    compression\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n",
       "\u001B[1;32m    105\u001B[0m                    row_limit\u001B[38;5;241m=\u001B[39mrowLimit,\n",
       "\u001B[1;32m    106\u001B[0m                    byte_limit\u001B[38;5;241m=\u001B[39mbyteLimit)\n",
       "\u001B[1;32m    107\u001B[0m pyspark_struct, cloudfetch_results, batch_truncation_results \u001B[38;5;241m=\u001B[39m results\n",
       "\u001B[1;32m    109\u001B[0m schema \u001B[38;5;241m=\u001B[39m CloudFetchDisplayHelper\u001B[38;5;241m.\u001B[39mget_display_schema_from_pyspark_struct(pyspark_struct)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:1988\u001B[0m, in \u001B[0;36mDataFrame._to_cloudfetch_with_limits_and_file_paths\u001B[0;34m(self, format, compression, row_limit, byte_limit)\u001B[0m\n",
       "\u001B[1;32m   1967\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m   1968\u001B[0m \u001B[38;5;124;03mThis is an experimental method to support generating results in the form of pre-signed\u001B[39;00m\n",
       "\u001B[1;32m   1969\u001B[0m \u001B[38;5;124;03mURLs that can be used by the client to fetch the results directly from cloud storage.\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   1985\u001B[0m \n",
       "\u001B[1;32m   1986\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m   1987\u001B[0m query \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan\u001B[38;5;241m.\u001B[39mto_proto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient)\n",
       "\u001B[0;32m-> 1988\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mexperimental_to_cloudfetch(\n",
       "\u001B[1;32m   1989\u001B[0m     query, \u001B[38;5;28mformat\u001B[39m, compression, row_limit, byte_limit, \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m   1990\u001B[0m )\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1241\u001B[0m, in \u001B[0;36mSparkConnectClient.experimental_to_cloudfetch\u001B[0;34m(self, plan, format, compression, row_limit, byte_limit, with_internal_file_path)\u001B[0m\n",
       "\u001B[1;32m   1238\u001B[0m \u001B[38;5;66;03m# Execute the request and iterate over the streaming results. We capture the schema\u001B[39;00m\n",
       "\u001B[1;32m   1239\u001B[0m \u001B[38;5;66;03m# response and unpack the CloudResultBatch responses.\u001B[39;00m\n",
       "\u001B[1;32m   1240\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n",
       "\u001B[0;32m-> 1241\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(req, {}, [], progress\u001B[38;5;241m=\u001B[39mprogress):\n",
       "\u001B[1;32m   1242\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n",
       "\u001B[1;32m   1243\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1949\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n",
       "\u001B[1;32m   1947\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n",
       "\u001B[1;32m   1948\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n",
       "\u001B[0;32m-> 1949\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2269\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n",
       "\u001B[1;32m   2267\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m   2268\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n",
       "\u001B[0;32m-> 2269\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n",
       "\u001B[1;32m   2270\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, \u001B[38;5;167;01mValueError\u001B[39;00m):\n",
       "\u001B[1;32m   2271\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke RPC\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error):\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2352\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n",
       "\u001B[1;32m   2348\u001B[0m             logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived ErrorInfo: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minfo\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m   2350\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(info, status\u001B[38;5;241m.\u001B[39mmessage, status_code)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[0;32m-> 2352\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n",
       "\u001B[1;32m   2353\u001B[0m                 info,\n",
       "\u001B[1;32m   2354\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2355\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n",
       "\u001B[1;32m   2356\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n",
       "\u001B[1;32m   2357\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2359\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n",
       "\u001B[1;32m   2360\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2361\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mErrorCode\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[1;32m   2362\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2363\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\n",
       "\u001B[0;31mDateTimeException\u001B[0m: [CAST_INVALID_INPUT] The value '08-25-2025' of the type \"STRING\" cannot be cast to \"DATE\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n",
       "\n",
       "JVM stacktrace:\n",
       "org.apache.spark.SparkDateTimeException\n",
       "\tat org.apache.spark.sql.errors.ExecutionErrors.invalidInputInCastToDatetimeErrorInternal(ExecutionErrors.scala:119)\n",
       "\tat org.apache.spark.sql.errors.ExecutionErrors.invalidInputInCastToDatetimeErrorInternal$(ExecutionErrors.scala:106)\n",
       "\tat org.apache.spark.sql.errors.ExecutionErrors$.invalidInputInCastToDatetimeErrorInternal(ExecutionErrors.scala:276)\n",
       "\tat org.apache.spark.sql.errors.ExecutionErrors.invalidInputInCastToDatetimeError(ExecutionErrors.scala:96)\n",
       "\tat org.apache.spark.sql.errors.ExecutionErrors.invalidInputInCastToDatetimeError$(ExecutionErrors.scala:92)\n",
       "\tat org.apache.spark.sql.errors.ExecutionErrors$.invalidInputInCastToDatetimeError(ExecutionErrors.scala:276)\n",
       "\tat org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.$anonfun$stringToDateAnsi$1(SparkDateTimeUtils.scala:464)\n",
       "\tat scala.Option.getOrElse(Option.scala:201)\n",
       "\tat org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.stringToDateAnsi(SparkDateTimeUtils.scala:464)\n",
       "\tat org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.stringToDateAnsi$(SparkDateTimeUtils.scala:462)\n",
       "\tat org.apache.spark.sql.catalyst.util.DateTimeUtils$.stringToDateAnsi(DateTimeUtils.scala:41)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.Cast.$anonfun$castToDate$2(Cast.scala:896)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.Cast.$anonfun$castToDate$2$adapted(Cast.scala:896)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.Cast.buildCast(Cast.scala:718)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.Cast.$anonfun$castToDate$1(Cast.scala:896)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.Cast.nullSafeEval(Cast.scala:1442)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.eval(Expression.scala:777)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.Alias.eval(namedExpressions.scala:184)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.InterpretedMutableProjection.apply(InterpretedMutableProjection.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.InterpretedMutableProjection.apply(InterpretedMutableProjection.scala:34)\n",
       "\tat org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$$anonfun$$nestedInanonfun$apply$44$1.$anonfun$applyOrElse$113(Optimizer.scala:3392)\n",
       "\tat scala.collection.immutable.ArraySeq.map(ArraySeq.scala:75)\n",
       "\tat scala.collection.immutable.ArraySeq.map(ArraySeq.scala:35)\n",
       "\tat org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$$anonfun$$nestedInanonfun$apply$44$1.$anonfun$applyOrElse$112(Optimizer.scala:3390)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$$anonfun$$nestedInanonfun$apply$44$1.applyOrElse(Optimizer.scala:3386)\n",
       "\tat org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$$anonfun$$nestedInanonfun$apply$44$1.applyOrElse(Optimizer.scala:3379)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$4(TreeNode.scala:587)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:587)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformUpWithPruning(LogicalPlan.scala:45)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformUpWithPruning(AnalysisHelper.scala:374)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformUpWithPruning$(AnalysisHelper.scala:369)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformUpWithPruning(LogicalPlan.scala:45)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformUpWithPruning(LogicalPlan.scala:45)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$1(TreeNode.scala:580)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1340)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1339)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:580)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformUpWithPruning(LogicalPlan.scala:45)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformUpWithPruning(AnalysisHelper.scala:374)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformUpWithPruning$(AnalysisHelper.scala:369)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformUpWithPruning(LogicalPlan.scala:45)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformUpWithPruning(LogicalPlan.scala:45)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$1(TreeNode.scala:580)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1340)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1339)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:580)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformUpWithPruning(LogicalPlan.scala:45)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformUpWithPruning(AnalysisHelper.scala:374)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformUpWithPruning$(AnalysisHelper.scala:369)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformUpWithPruning(LogicalPlan.scala:45)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformUpWithPruning(LogicalPlan.scala:45)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$1(TreeNode.scala:580)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1340)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1339)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LocalLimit.mapChildren(basicLogicalOperators.scala:2345)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:580)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformUpWithPruning(LogicalPlan.scala:45)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformUpWithPruning(AnalysisHelper.scala:374)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformUpWithPruning$(AnalysisHelper.scala:369)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformUpWithPruning(LogicalPlan.scala:45)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformUpWithPruning(LogicalPlan.scala:45)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$1(TreeNode.scala:580)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1340)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1339)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.GlobalLimit.mapChildren(basicLogicalOperators.scala:2309)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:580)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformUpWithPruning(LogicalPlan.scala:45)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformUpWithPruning(AnalysisHelper.scala:374)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformUpWithPruning$(AnalysisHelper.scala:369)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformUpWithPruning(LogicalPlan.scala:45)\n",
       "\tat org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$.$anonfun$apply$44(Optimizer.scala:3379)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$.apply(Optimizer.scala:3375)\n",
       "\tat org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$.apply(Optimizer.scala:3369)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:503)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:657)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:641)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:503)\n",
       "\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:352)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:501)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:500)\n",
       "\tat scala.collection.immutable.ArraySeq.foldLeft(ArraySeq.scala:222)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:492)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:466)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:613)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:613)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:613)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:359)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:347)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:267)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:347)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyOptimizedPlan$5(QueryExecution.scala:678)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:697)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:832)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:158)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n",
       "\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n",
       "\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n",
       "\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n",
       "\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n",
       "\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:115)\n",
       "\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:139)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:832)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1478)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:825)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:822)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:822)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:821)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:809)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:820)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:819)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyOptimizedPlan$4(QueryExecution.scala:674)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyOptimizedPlan$3(QueryExecution.scala:674)\n",
       "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyOptimizedPlan$1(QueryExecution.scala:671)\n",
       "\tat scala.util.Try$.apply(Try.scala:217)\n",
       "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)\n",
       "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)\n",
       "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:695)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:697)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyExecutedPlan$1(QueryExecution.scala:719)\n",
       "\tat scala.util.Try$.apply(Try.scala:217)\n",
       "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)\n",
       "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)\n",
       "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:753)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:930)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:1003)\n",
       "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1748)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:753)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution._executedPlan(QueryExecution.scala:756)\n",
       "\tat com.databricks.sql.logging.SQLUsageLogging$.logUsage(SQLUsageLogging.scala:188)\n",
       "\tat com.databricks.sql.logging.SQLUsageLogging$.$anonfun$logUsageWrapper$1(SQLUsageLogging.scala:732)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)\n",
       "\tat com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)\n",
       "\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
       "\tat java.lang.Thread.run(Thread.java:840)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "DateTimeException",
        "evalue": "[CAST_INVALID_INPUT] The value '08-25-2025' of the type \"STRING\" cannot be cast to \"DATE\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n\nJVM stacktrace:\norg.apache.spark.SparkDateTimeException\n\tat org.apache.spark.sql.errors.ExecutionErrors.invalidInputInCastToDatetimeErrorInternal(ExecutionErrors.scala:119)\n\tat org.apache.spark.sql.errors.ExecutionErrors.invalidInputInCastToDatetimeErrorInternal$(ExecutionErrors.scala:106)\n\tat org.apache.spark.sql.errors.ExecutionErrors$.invalidInputInCastToDatetimeErrorInternal(ExecutionErrors.scala:276)\n\tat org.apache.spark.sql.errors.ExecutionErrors.invalidInputInCastToDatetimeError(ExecutionErrors.scala:96)\n\tat org.apache.spark.sql.errors.ExecutionErrors.invalidInputInCastToDatetimeError$(ExecutionErrors.scala:92)\n\tat org.apache.spark.sql.errors.ExecutionErrors$.invalidInputInCastToDatetimeError(ExecutionErrors.scala:276)\n\tat org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.$anonfun$stringToDateAnsi$1(SparkDateTimeUtils.scala:464)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.stringToDateAnsi(SparkDateTimeUtils.scala:464)\n\tat org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.stringToDateAnsi$(SparkDateTimeUtils.scala:462)\n\tat org.apache.spark.sql.catalyst.util.DateTimeUtils$.stringToDateAnsi(DateTimeUtils.scala:41)\n\tat org.apache.spark.sql.catalyst.expressions.Cast.$anonfun$castToDate$2(Cast.scala:896)\n\tat org.apache.spark.sql.catalyst.expressions.Cast.$anonfun$castToDate$2$adapted(Cast.scala:896)\n\tat org.apache.spark.sql.catalyst.expressions.Cast.buildCast(Cast.scala:718)\n\tat org.apache.spark.sql.catalyst.expressions.Cast.$anonfun$castToDate$1(Cast.scala:896)\n\tat org.apache.spark.sql.catalyst.expressions.Cast.nullSafeEval(Cast.scala:1442)\n\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.eval(Expression.scala:777)\n\tat org.apache.spark.sql.catalyst.expressions.Alias.eval(namedExpressions.scala:184)\n\tat org.apache.spark.sql.catalyst.expressions.InterpretedMutableProjection.apply(InterpretedMutableProjection.scala:94)\n\tat org.apache.spark.sql.catalyst.expressions.InterpretedMutableProjection.apply(InterpretedMutableProjection.scala:34)\n\tat org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$$anonfun$$nestedInanonfun$apply$44$1.$anonfun$applyOrElse$113(Optimizer.scala:3392)\n\tat scala.collection.immutable.ArraySeq.map(ArraySeq.scala:75)\n\tat scala.collection.immutable.ArraySeq.map(ArraySeq.scala:35)\n\tat org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$$anonfun$$nestedInanonfun$apply$44$1.$anonfun$applyOrElse$112(Optimizer.scala:3390)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$$anonfun$$nestedInanonfun$apply$44$1.applyOrElse(Optimizer.scala:3386)\n\tat org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$$anonfun$$nestedInanonfun$apply$44$1.applyOrElse(Optimizer.scala:3379)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$4(TreeNode.scala:587)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:587)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformUpWithPruning(AnalysisHelper.scala:374)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformUpWithPruning$(AnalysisHelper.scala:369)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$1(TreeNode.scala:580)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1340)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1339)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:580)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformUpWithPruning(AnalysisHelper.scala:374)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformUpWithPruning$(AnalysisHelper.scala:369)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$1(TreeNode.scala:580)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1340)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1339)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:580)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformUpWithPruning(AnalysisHelper.scala:374)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformUpWithPruning$(AnalysisHelper.scala:369)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$1(TreeNode.scala:580)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1340)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1339)\n\tat org.apache.spark.sql.catalyst.plans.logical.LocalLimit.mapChildren(basicLogicalOperators.scala:2345)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:580)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformUpWithPruning(AnalysisHelper.scala:374)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformUpWithPruning$(AnalysisHelper.scala:369)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$1(TreeNode.scala:580)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1340)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1339)\n\tat org.apache.spark.sql.catalyst.plans.logical.GlobalLimit.mapChildren(basicLogicalOperators.scala:2309)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:580)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformUpWithPruning(AnalysisHelper.scala:374)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformUpWithPruning$(AnalysisHelper.scala:369)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$.$anonfun$apply$44(Optimizer.scala:3379)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$.apply(Optimizer.scala:3375)\n\tat org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$.apply(Optimizer.scala:3369)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:503)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:657)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:641)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:137)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:503)\n\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:352)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:501)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:500)\n\tat scala.collection.immutable.ArraySeq.foldLeft(ArraySeq.scala:222)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:492)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:466)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:613)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:613)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:613)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:359)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:347)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:267)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:347)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyOptimizedPlan$5(QueryExecution.scala:678)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:697)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:832)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:158)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:115)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:139)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:832)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1478)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:825)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:822)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:822)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:821)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:809)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:820)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:819)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyOptimizedPlan$4(QueryExecution.scala:674)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyOptimizedPlan$3(QueryExecution.scala:674)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyOptimizedPlan$1(QueryExecution.scala:671)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:695)\n\tat org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:697)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyExecutedPlan$1(QueryExecution.scala:719)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:753)\n\tat org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:930)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:1003)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1748)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:753)\n\tat org.apache.spark.sql.execution.QueryExecution._executedPlan(QueryExecution.scala:756)\n\tat com.databricks.sql.logging.SQLUsageLogging$.logUsage(SQLUsageLogging.scala:188)\n\tat com.databricks.sql.logging.SQLUsageLogging$.$anonfun$logUsageWrapper$1(SQLUsageLogging.scala:732)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)\n\tat com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)\n\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.lang.Thread.run(Thread.java:840)"
       },
       "metadata": {
        "errorSummary": "[CAST_INVALID_INPUT] The value '08-25-2025' of the type \"STRING\" cannot be cast to \"DATE\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018"
       },
       "removedWidgets": [],
       "sqlProps": {
        "breakingChangeInfo": null,
        "errorClass": "CAST_INVALID_INPUT",
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "pysparkSummary": "",
        "sqlState": "22018",
        "stackTrace": "org.apache.spark.SparkDateTimeException\n\tat org.apache.spark.sql.errors.ExecutionErrors.invalidInputInCastToDatetimeErrorInternal(ExecutionErrors.scala:119)\n\tat org.apache.spark.sql.errors.ExecutionErrors.invalidInputInCastToDatetimeErrorInternal$(ExecutionErrors.scala:106)\n\tat org.apache.spark.sql.errors.ExecutionErrors$.invalidInputInCastToDatetimeErrorInternal(ExecutionErrors.scala:276)\n\tat org.apache.spark.sql.errors.ExecutionErrors.invalidInputInCastToDatetimeError(ExecutionErrors.scala:96)\n\tat org.apache.spark.sql.errors.ExecutionErrors.invalidInputInCastToDatetimeError$(ExecutionErrors.scala:92)\n\tat org.apache.spark.sql.errors.ExecutionErrors$.invalidInputInCastToDatetimeError(ExecutionErrors.scala:276)\n\tat org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.$anonfun$stringToDateAnsi$1(SparkDateTimeUtils.scala:464)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.stringToDateAnsi(SparkDateTimeUtils.scala:464)\n\tat org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.stringToDateAnsi$(SparkDateTimeUtils.scala:462)\n\tat org.apache.spark.sql.catalyst.util.DateTimeUtils$.stringToDateAnsi(DateTimeUtils.scala:41)\n\tat org.apache.spark.sql.catalyst.expressions.Cast.$anonfun$castToDate$2(Cast.scala:896)\n\tat org.apache.spark.sql.catalyst.expressions.Cast.$anonfun$castToDate$2$adapted(Cast.scala:896)\n\tat org.apache.spark.sql.catalyst.expressions.Cast.buildCast(Cast.scala:718)\n\tat org.apache.spark.sql.catalyst.expressions.Cast.$anonfun$castToDate$1(Cast.scala:896)\n\tat org.apache.spark.sql.catalyst.expressions.Cast.nullSafeEval(Cast.scala:1442)\n\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.eval(Expression.scala:777)\n\tat org.apache.spark.sql.catalyst.expressions.Alias.eval(namedExpressions.scala:184)\n\tat org.apache.spark.sql.catalyst.expressions.InterpretedMutableProjection.apply(InterpretedMutableProjection.scala:94)\n\tat org.apache.spark.sql.catalyst.expressions.InterpretedMutableProjection.apply(InterpretedMutableProjection.scala:34)\n\tat org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$$anonfun$$nestedInanonfun$apply$44$1.$anonfun$applyOrElse$113(Optimizer.scala:3392)\n\tat scala.collection.immutable.ArraySeq.map(ArraySeq.scala:75)\n\tat scala.collection.immutable.ArraySeq.map(ArraySeq.scala:35)\n\tat org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$$anonfun$$nestedInanonfun$apply$44$1.$anonfun$applyOrElse$112(Optimizer.scala:3390)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$$anonfun$$nestedInanonfun$apply$44$1.applyOrElse(Optimizer.scala:3386)\n\tat org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$$anonfun$$nestedInanonfun$apply$44$1.applyOrElse(Optimizer.scala:3379)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$4(TreeNode.scala:587)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:587)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformUpWithPruning(AnalysisHelper.scala:374)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformUpWithPruning$(AnalysisHelper.scala:369)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$1(TreeNode.scala:580)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1340)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1339)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:580)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformUpWithPruning(AnalysisHelper.scala:374)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformUpWithPruning$(AnalysisHelper.scala:369)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$1(TreeNode.scala:580)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1340)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1339)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:580)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformUpWithPruning(AnalysisHelper.scala:374)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformUpWithPruning$(AnalysisHelper.scala:369)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$1(TreeNode.scala:580)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1340)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1339)\n\tat org.apache.spark.sql.catalyst.plans.logical.LocalLimit.mapChildren(basicLogicalOperators.scala:2345)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:580)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformUpWithPruning(AnalysisHelper.scala:374)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformUpWithPruning$(AnalysisHelper.scala:369)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$1(TreeNode.scala:580)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1340)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1339)\n\tat org.apache.spark.sql.catalyst.plans.logical.GlobalLimit.mapChildren(basicLogicalOperators.scala:2309)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:580)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformUpWithPruning(AnalysisHelper.scala:374)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformUpWithPruning$(AnalysisHelper.scala:369)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$.$anonfun$apply$44(Optimizer.scala:3379)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$.apply(Optimizer.scala:3375)\n\tat org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$.apply(Optimizer.scala:3369)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:503)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:657)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:641)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:137)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:503)\n\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:352)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:501)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:500)\n\tat scala.collection.immutable.ArraySeq.foldLeft(ArraySeq.scala:222)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:492)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:466)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:613)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:613)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:613)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:359)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:347)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:267)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:347)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyOptimizedPlan$5(QueryExecution.scala:678)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:697)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:832)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:158)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:115)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:139)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:832)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1478)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:825)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:822)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:822)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:821)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:809)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:820)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:819)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyOptimizedPlan$4(QueryExecution.scala:674)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyOptimizedPlan$3(QueryExecution.scala:674)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyOptimizedPlan$1(QueryExecution.scala:671)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:695)\n\tat org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:697)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyExecutedPlan$1(QueryExecution.scala:719)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:753)\n\tat org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:930)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:1003)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1748)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:753)\n\tat org.apache.spark.sql.execution.QueryExecution._executedPlan(QueryExecution.scala:756)\n\tat com.databricks.sql.logging.SQLUsageLogging$.logUsage(SQLUsageLogging.scala:188)\n\tat com.databricks.sql.logging.SQLUsageLogging$.$anonfun$logUsageWrapper$1(SQLUsageLogging.scala:732)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)\n\tat com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)\n\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.lang.Thread.run(Thread.java:840)",
        "startIndex": -1,
        "stopIndex": -1
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mDateTimeException\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m<command-2022302555744395>, line 9\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Apply casting with correct date / timestamp format\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# df_cust_format: date & timestamp are in custom format\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# since date & timestamp are in custom format, pass date_format -> cast_dataframe_columns_wo(df, col_type_map, date_format=\"%m-%d-%Y\")\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# date_format=\"%m-%d-%Y\" -> month-day-year\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# date_format=\"%m-%d-%Y %H:%M:%S.%f%z\" -> month-day-year hour:minute:second.microsecond\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# since date_format not passed getting error -> [CAST_INVALID_INPUT] The value '08-25-2025' of the type \"STRING\" cannot be cast to \"DATE\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\u001B[39;00m\n\u001B[1;32m      8\u001B[0m df_casted_cust \u001B[38;5;241m=\u001B[39m cast_dataframe_columns_wo(df_cust_format, col_type_map)\n\u001B[0;32m----> 9\u001B[0m display(df_casted_cust)\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display.py:133\u001B[0m, in \u001B[0;36mDisplay.display\u001B[0;34m(self, input, *args, **kwargs)\u001B[0m\n\u001B[1;32m    131\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[1;32m    132\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cf_helper \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28minput\u001B[39m, ConnectDataFrame):\n\u001B[0;32m--> 133\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdisplay_connect_table(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    134\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28minput\u001B[39m, ConnectDataFrame):\n\u001B[1;32m    135\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39misStreaming:\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display.py:97\u001B[0m, in \u001B[0;36mDisplay.display_connect_table\u001B[0;34m(self, df, **kwargs)\u001B[0m\n\u001B[1;32m     94\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcf_helper\u001B[38;5;241m.\u001B[39mdisplay_streaming_dataframe(df, config, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstreaming_listener,\n\u001B[1;32m     95\u001B[0m                                                \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     96\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 97\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcf_helper\u001B[38;5;241m.\u001B[39mdisplay_dataframe(df, config)\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display_helpers/cloudfetch_helper.py:47\u001B[0m, in \u001B[0;36mCloudFetchDisplayHelper.display_dataframe\u001B[0;34m(self, df, config)\u001B[0m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdisplay_dataframe\u001B[39m(\u001B[38;5;28mself\u001B[39m, df: ConnectDataFrame, config: TableDisplayConfig):\n\u001B[0;32m---> 47\u001B[0m     display_payload \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwrite_to_cloudfetch(df, config)\n\u001B[1;32m     48\u001B[0m     ip_display({\n\u001B[1;32m     49\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mapplication/vnd.databricks.connect.display\u001B[39m\u001B[38;5;124m\"\u001B[39m: display_payload,\n\u001B[1;32m     50\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext/plain\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDatabricks Spark Connect Table\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     51\u001B[0m     },\n\u001B[1;32m     52\u001B[0m                raw\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display_helpers/cloudfetch_helper.py:133\u001B[0m, in \u001B[0;36mCloudFetchDisplayHelper.write_to_cloudfetch\u001B[0;34m(self, connectDataFrame, config)\u001B[0m\n\u001B[1;32m    131\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    132\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m PySparkException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m--> 133\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    134\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    135\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(\n\u001B[1;32m    136\u001B[0m         e\n\u001B[1;32m    137\u001B[0m     )(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEncountered an unexpected error when displaying table, please contact Databricks support\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    138\u001B[0m       ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display_helpers/cloudfetch_helper.py:102\u001B[0m, in \u001B[0;36mCloudFetchDisplayHelper.write_to_cloudfetch\u001B[0;34m(self, connectDataFrame, config)\u001B[0m\n\u001B[1;32m     99\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m rowLimit \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    100\u001B[0m     connectDataFrame \u001B[38;5;241m=\u001B[39m cast(ConnectDataFrame, connectDataFrame\u001B[38;5;241m.\u001B[39mlimit(rowLimit \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m))\n\u001B[1;32m    101\u001B[0m results: Tuple[Optional[StructType], List[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCloudFetchResult\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m--> 102\u001B[0m                List[\u001B[38;5;28mbool\u001B[39m]] \u001B[38;5;241m=\u001B[39m connectDataFrame\u001B[38;5;241m.\u001B[39m_to_cloudfetch_with_limits_and_file_paths(\n\u001B[1;32m    103\u001B[0m                    \u001B[38;5;28mformat\u001B[39m\u001B[38;5;241m=\u001B[39mstr_format,\n\u001B[1;32m    104\u001B[0m                    compression\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    105\u001B[0m                    row_limit\u001B[38;5;241m=\u001B[39mrowLimit,\n\u001B[1;32m    106\u001B[0m                    byte_limit\u001B[38;5;241m=\u001B[39mbyteLimit)\n\u001B[1;32m    107\u001B[0m pyspark_struct, cloudfetch_results, batch_truncation_results \u001B[38;5;241m=\u001B[39m results\n\u001B[1;32m    109\u001B[0m schema \u001B[38;5;241m=\u001B[39m CloudFetchDisplayHelper\u001B[38;5;241m.\u001B[39mget_display_schema_from_pyspark_struct(pyspark_struct)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:1988\u001B[0m, in \u001B[0;36mDataFrame._to_cloudfetch_with_limits_and_file_paths\u001B[0;34m(self, format, compression, row_limit, byte_limit)\u001B[0m\n\u001B[1;32m   1967\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1968\u001B[0m \u001B[38;5;124;03mThis is an experimental method to support generating results in the form of pre-signed\u001B[39;00m\n\u001B[1;32m   1969\u001B[0m \u001B[38;5;124;03mURLs that can be used by the client to fetch the results directly from cloud storage.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1985\u001B[0m \n\u001B[1;32m   1986\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1987\u001B[0m query \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan\u001B[38;5;241m.\u001B[39mto_proto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient)\n\u001B[0;32m-> 1988\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mexperimental_to_cloudfetch(\n\u001B[1;32m   1989\u001B[0m     query, \u001B[38;5;28mformat\u001B[39m, compression, row_limit, byte_limit, \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   1990\u001B[0m )\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1241\u001B[0m, in \u001B[0;36mSparkConnectClient.experimental_to_cloudfetch\u001B[0;34m(self, plan, format, compression, row_limit, byte_limit, with_internal_file_path)\u001B[0m\n\u001B[1;32m   1238\u001B[0m \u001B[38;5;66;03m# Execute the request and iterate over the streaming results. We capture the schema\u001B[39;00m\n\u001B[1;32m   1239\u001B[0m \u001B[38;5;66;03m# response and unpack the CloudResultBatch responses.\u001B[39;00m\n\u001B[1;32m   1240\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n\u001B[0;32m-> 1241\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(req, {}, [], progress\u001B[38;5;241m=\u001B[39mprogress):\n\u001B[1;32m   1242\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n\u001B[1;32m   1243\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1949\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n\u001B[1;32m   1947\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n\u001B[1;32m   1948\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m-> 1949\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2269\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n\u001B[1;32m   2267\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   2268\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n\u001B[0;32m-> 2269\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n\u001B[1;32m   2270\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, \u001B[38;5;167;01mValueError\u001B[39;00m):\n\u001B[1;32m   2271\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke RPC\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error):\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2352\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n\u001B[1;32m   2348\u001B[0m             logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived ErrorInfo: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minfo\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   2350\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(info, status\u001B[38;5;241m.\u001B[39mmessage, status_code)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[0;32m-> 2352\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n\u001B[1;32m   2353\u001B[0m                 info,\n\u001B[1;32m   2354\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2355\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n\u001B[1;32m   2356\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n\u001B[1;32m   2357\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2359\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n\u001B[1;32m   2360\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2361\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mErrorCode\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[1;32m   2362\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2363\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
        "\u001B[0;31mDateTimeException\u001B[0m: [CAST_INVALID_INPUT] The value '08-25-2025' of the type \"STRING\" cannot be cast to \"DATE\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n\nJVM stacktrace:\norg.apache.spark.SparkDateTimeException\n\tat org.apache.spark.sql.errors.ExecutionErrors.invalidInputInCastToDatetimeErrorInternal(ExecutionErrors.scala:119)\n\tat org.apache.spark.sql.errors.ExecutionErrors.invalidInputInCastToDatetimeErrorInternal$(ExecutionErrors.scala:106)\n\tat org.apache.spark.sql.errors.ExecutionErrors$.invalidInputInCastToDatetimeErrorInternal(ExecutionErrors.scala:276)\n\tat org.apache.spark.sql.errors.ExecutionErrors.invalidInputInCastToDatetimeError(ExecutionErrors.scala:96)\n\tat org.apache.spark.sql.errors.ExecutionErrors.invalidInputInCastToDatetimeError$(ExecutionErrors.scala:92)\n\tat org.apache.spark.sql.errors.ExecutionErrors$.invalidInputInCastToDatetimeError(ExecutionErrors.scala:276)\n\tat org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.$anonfun$stringToDateAnsi$1(SparkDateTimeUtils.scala:464)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.stringToDateAnsi(SparkDateTimeUtils.scala:464)\n\tat org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.stringToDateAnsi$(SparkDateTimeUtils.scala:462)\n\tat org.apache.spark.sql.catalyst.util.DateTimeUtils$.stringToDateAnsi(DateTimeUtils.scala:41)\n\tat org.apache.spark.sql.catalyst.expressions.Cast.$anonfun$castToDate$2(Cast.scala:896)\n\tat org.apache.spark.sql.catalyst.expressions.Cast.$anonfun$castToDate$2$adapted(Cast.scala:896)\n\tat org.apache.spark.sql.catalyst.expressions.Cast.buildCast(Cast.scala:718)\n\tat org.apache.spark.sql.catalyst.expressions.Cast.$anonfun$castToDate$1(Cast.scala:896)\n\tat org.apache.spark.sql.catalyst.expressions.Cast.nullSafeEval(Cast.scala:1442)\n\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.eval(Expression.scala:777)\n\tat org.apache.spark.sql.catalyst.expressions.Alias.eval(namedExpressions.scala:184)\n\tat org.apache.spark.sql.catalyst.expressions.InterpretedMutableProjection.apply(InterpretedMutableProjection.scala:94)\n\tat org.apache.spark.sql.catalyst.expressions.InterpretedMutableProjection.apply(InterpretedMutableProjection.scala:34)\n\tat org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$$anonfun$$nestedInanonfun$apply$44$1.$anonfun$applyOrElse$113(Optimizer.scala:3392)\n\tat scala.collection.immutable.ArraySeq.map(ArraySeq.scala:75)\n\tat scala.collection.immutable.ArraySeq.map(ArraySeq.scala:35)\n\tat org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$$anonfun$$nestedInanonfun$apply$44$1.$anonfun$applyOrElse$112(Optimizer.scala:3390)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$$anonfun$$nestedInanonfun$apply$44$1.applyOrElse(Optimizer.scala:3386)\n\tat org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$$anonfun$$nestedInanonfun$apply$44$1.applyOrElse(Optimizer.scala:3379)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$4(TreeNode.scala:587)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:587)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformUpWithPruning(AnalysisHelper.scala:374)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformUpWithPruning$(AnalysisHelper.scala:369)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$1(TreeNode.scala:580)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1340)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1339)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:580)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformUpWithPruning(AnalysisHelper.scala:374)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformUpWithPruning$(AnalysisHelper.scala:369)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$1(TreeNode.scala:580)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1340)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1339)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:580)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformUpWithPruning(AnalysisHelper.scala:374)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformUpWithPruning$(AnalysisHelper.scala:369)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$1(TreeNode.scala:580)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1340)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1339)\n\tat org.apache.spark.sql.catalyst.plans.logical.LocalLimit.mapChildren(basicLogicalOperators.scala:2345)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:580)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformUpWithPruning(AnalysisHelper.scala:374)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformUpWithPruning$(AnalysisHelper.scala:369)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$1(TreeNode.scala:580)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1340)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1339)\n\tat org.apache.spark.sql.catalyst.plans.logical.GlobalLimit.mapChildren(basicLogicalOperators.scala:2309)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:580)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformUpWithPruning(AnalysisHelper.scala:374)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformUpWithPruning$(AnalysisHelper.scala:369)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$.$anonfun$apply$44(Optimizer.scala:3379)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$.apply(Optimizer.scala:3375)\n\tat org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$.apply(Optimizer.scala:3369)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:503)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:657)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:641)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:137)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:503)\n\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:352)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:501)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:500)\n\tat scala.collection.immutable.ArraySeq.foldLeft(ArraySeq.scala:222)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:492)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:466)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:613)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:613)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:613)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:359)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:347)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:267)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:347)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyOptimizedPlan$5(QueryExecution.scala:678)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:697)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:832)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:158)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:115)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:139)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:832)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1478)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:825)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:822)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:822)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:821)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:809)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:820)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:819)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyOptimizedPlan$4(QueryExecution.scala:674)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyOptimizedPlan$3(QueryExecution.scala:674)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyOptimizedPlan$1(QueryExecution.scala:671)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:695)\n\tat org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:697)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyExecutedPlan$1(QueryExecution.scala:719)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:753)\n\tat org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:930)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:1003)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1748)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:753)\n\tat org.apache.spark.sql.execution.QueryExecution._executedPlan(QueryExecution.scala:756)\n\tat com.databricks.sql.logging.SQLUsageLogging$.logUsage(SQLUsageLogging.scala:188)\n\tat com.databricks.sql.logging.SQLUsageLogging$.$anonfun$logUsageWrapper$1(SQLUsageLogging.scala:732)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)\n\tat com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)\n\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.lang.Thread.run(Thread.java:840)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply casting with correct date / timestamp format\n",
    "# df_cust_format: date & timestamp are in custom format\n",
    "# since date & timestamp are in custom format, pass date_format -> cast_dataframe_columns_wo(df, col_type_map, date_format=\"%m-%d-%Y\")\n",
    "# date_format=\"%m-%d-%Y\" -> month-day-year\n",
    "# date_format=\"%m-%d-%Y %H:%M:%S.%f%z\" -> month-day-year hour:minute:second.microsecond\n",
    "# since date_format not passed getting error -> [CAST_INVALID_INPUT] The value '08-25-2025' of the type \"STRING\" cannot be cast to \"DATE\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n",
    "\n",
    "df_casted_cust = cast_dataframe_columns_wo(df_cust_format, col_type_map)\n",
    "display(df_casted_cust)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2cf3ba96-012f-495c-aea8-aaa4f63f3138",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**date_format** has a **default value** of **None**.\n",
    "\n",
    "      def cast_dataframe_columns_wo(df, col_type_map, date_format=None):\n",
    "\n",
    "- If the user **does not** provide a **date_format**, the function **still works**, it will just treat **date_format** as **None**.\n",
    "   - Spark uses its **default date** `parsing rules`, which expect the input to be in the **standard format**  \n",
    "\n",
    "         # No format passed  date_format is None\n",
    "         cast_dataframe_columns_wo(df, {\"join_date\": \"date\"})\n",
    "\n",
    "         -> yyyy-MM-dd (for date)\n",
    "         -> yyyy-MM-dd HH:mm:ss (for timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f4a09c7-a5fd-41cb-9fa2-b13e74f8b7ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Scenario 02\n",
    "- date = \"MM-dd-yyyy\" -> custom format\n",
    "- date_format = \"yyyy-MM-dd'T'HH:mm:ss.SSSXXX\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26408ab6-0cca-4466-ab45-a9f807ee5281",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"last_login\":211},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763230517635}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "date cols: custom format"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>age</th><th>salary</th><th>is_active</th><th>join_date</th><th>Exp</th><th>last_login</th></tr></thead><tbody><tr><td>Albert</td><td>25</td><td>55000.50</td><td>true</td><td>08-25-2025</td><td>6</td><td>08-25-2025T15:30:00.000+00:00</td></tr><tr><td>Baskar</td><td>30</td><td>72000.00</td><td>false</td><td>12-30-2024</td><td>7</td><td>12-31-2024T08:45:15.000+00:00</td></tr><tr><td>Chetan</td><td>27</td><td>63000.75</td><td>true</td><td>01-15-2023</td><td>8</td><td>01-15-2023T20:00:00.000+00:00</td></tr><tr><td>Dravid</td><td>26</td><td>66000.50</td><td>true</td><td>06-20-2024</td><td>9</td><td>05-22-2024T15:40:55.000+00:00</td></tr><tr><td>Nishant</td><td>32</td><td>98700.00</td><td>false</td><td>10-21-2023</td><td>10</td><td>10-31-2020T06:45:45.000+00:00</td></tr><tr><td>David</td><td>29</td><td>34512.75</td><td>true</td><td>03-15-2023</td><td>4</td><td>08-28-2021T20:15:55.000+00:00</td></tr><tr><td>Mohan</td><td>33</td><td>34908.50</td><td>true</td><td>04-15-2022</td><td>12</td><td>09-29-2019T19:35:55.000+00:00</td></tr><tr><td>Niroop</td><td>35</td><td>49654.98</td><td>false</td><td>02-18-2021</td><td>3</td><td>05-22-2024T08:45:25.000+00:00</td></tr><tr><td>Pushpa</td><td>23</td><td>44111.99</td><td>true</td><td>07-19-2020</td><td>5</td><td>09-25-2023T20:00:00.000+00:00</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Albert",
         "25",
         "55000.50",
         "true",
         "08-25-2025",
         "6",
         "08-25-2025T15:30:00.000+00:00"
        ],
        [
         "Baskar",
         "30",
         "72000.00",
         "false",
         "12-30-2024",
         "7",
         "12-31-2024T08:45:15.000+00:00"
        ],
        [
         "Chetan",
         "27",
         "63000.75",
         "true",
         "01-15-2023",
         "8",
         "01-15-2023T20:00:00.000+00:00"
        ],
        [
         "Dravid",
         "26",
         "66000.50",
         "true",
         "06-20-2024",
         "9",
         "05-22-2024T15:40:55.000+00:00"
        ],
        [
         "Nishant",
         "32",
         "98700.00",
         "false",
         "10-21-2023",
         "10",
         "10-31-2020T06:45:45.000+00:00"
        ],
        [
         "David",
         "29",
         "34512.75",
         "true",
         "03-15-2023",
         "4",
         "08-28-2021T20:15:55.000+00:00"
        ],
        [
         "Mohan",
         "33",
         "34908.50",
         "true",
         "04-15-2022",
         "12",
         "09-29-2019T19:35:55.000+00:00"
        ],
        [
         "Niroop",
         "35",
         "49654.98",
         "false",
         "02-18-2021",
         "3",
         "05-22-2024T08:45:25.000+00:00"
        ],
        [
         "Pushpa",
         "23",
         "44111.99",
         "true",
         "07-19-2020",
         "5",
         "09-25-2023T20:00:00.000+00:00"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "is_active",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "join_date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Exp",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "last_login",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# All columns are in string type\n",
    "# All rows of date & timestamp are in custom format\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (\"Albert\", \"25\", \"55000.50\", \"true\", \"08-25-2025\", \"6\", \"08-25-2025T15:30:00.000+00:00\"),     # All rows of date & timestamp are in default format\n",
    "    (\"Baskar\", \"30\", \"72000.00\", \"false\", \"12-30-2024\", \"7\", \"12-31-2024T08:45:15.000+00:00\"),\n",
    "    (\"Chetan\", \"27\", \"63000.75\", \"true\", \"01-15-2023\", \"8\", \"01-15-2023T20:00:00.000+00:00\"),\n",
    "    (\"Dravid\", \"26\", \"66000.50\", \"true\", \"06-20-2024\", \"9\", \"05-22-2024T15:40:55.000+00:00\"),\n",
    "    (\"Nishant\", \"32\", \"98700.00\", \"false\", \"10-21-2023\", \"10\", \"10-31-2020T06:45:45.000+00:00\"),\n",
    "    (\"David\", \"29\", \"34512.75\", \"true\", \"03-15-2023\", \"4\", \"08-28-2021T20:15:55.000+00:00\"),\n",
    "    (\"Mohan\", \"33\", \"34908.50\", \"true\", \"04-15-2022\", \"12\", \"09-29-2019T19:35:55.000+00:00\"),\n",
    "    (\"Niroop\", \"35\", \"49654.98\", \"false\", \"02-18-2021\", \"3\", \"05-22-2024T08:45:25.000+00:00\"),\n",
    "    (\"Pushpa\", \"23\", \"44111.99\", \"true\", \"07-19-2020\", \"5\", \"09-25-2023T20:00:00.000+00:00\")\n",
    "]\n",
    "\n",
    "# Define schema\n",
    "columns = [\"name\", \"age\", \"salary\", \"is_active\", \"join_date\", \"Exp\" ,\"last_login\"]\n",
    "\n",
    "df_cust_format = spark.createDataFrame(data, columns)\n",
    "display(df_cust_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0550bac1-0693-43ed-bbfb-8c7e9100579d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def cast_dataframe_columns_tformat(df, col_type_map, date_format=None):\n",
    "    for col_name, data_type in col_type_map.items():\n",
    "        data_type = data_type.lower()\n",
    "\n",
    "        if data_type == \"string\":\n",
    "            df = df.withColumn(col_name, F.col(col_name).cast(\"string\"))\n",
    "\n",
    "        elif data_type in [\"int\", \"Int\", \"integer\"]:\n",
    "            df = df.withColumn(col_name, F.col(col_name).cast(\"int\"))\n",
    "\n",
    "        elif data_type in [\"double\", \"float\"]:\n",
    "            df = df.withColumn(col_name, F.col(col_name).cast(\"double\"))\n",
    "\n",
    "        elif data_type == \"boolean\":\n",
    "            df = df.withColumn(col_name, F.col(col_name).cast(\"boolean\"))\n",
    "\n",
    "        elif data_type == \"date\":\n",
    "            # Handle custom date format \"08-25-2025\" -> \"MM-dd-yyyy\"\n",
    "            df = df.withColumn(col_name, F.to_date(F.col(col_name), \"MM-dd-yyyy\"))\n",
    "\n",
    "        elif data_type == \"timestamp\":\n",
    "            df = df.withColumn(col_name, F.to_timestamp(F.col(col_name), date_format))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b37469e4-25f7-4c33-8ec2-efa6bc2d8f81",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"last_login\":197},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763796222997}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>age</th><th>salary</th><th>is_active</th><th>join_date</th><th>Exp</th><th>last_login</th></tr></thead><tbody><tr><td>Albert</td><td>25</td><td>55000.5</td><td>true</td><td>2025-08-25</td><td>6</td><td>2025-08-25T15:30:00.000Z</td></tr><tr><td>Baskar</td><td>30</td><td>72000.0</td><td>false</td><td>2024-12-30</td><td>7</td><td>2024-12-31T08:45:15.000Z</td></tr><tr><td>Chetan</td><td>27</td><td>63000.75</td><td>true</td><td>2023-01-15</td><td>8</td><td>2023-01-15T20:00:00.000Z</td></tr><tr><td>Dravid</td><td>26</td><td>66000.5</td><td>true</td><td>2024-06-20</td><td>9</td><td>2024-05-22T15:40:55.000Z</td></tr><tr><td>Nishant</td><td>32</td><td>98700.0</td><td>false</td><td>2023-10-21</td><td>10</td><td>2020-10-31T06:45:45.000Z</td></tr><tr><td>David</td><td>29</td><td>34512.75</td><td>true</td><td>2023-03-15</td><td>4</td><td>2021-08-28T20:15:55.000Z</td></tr><tr><td>Mohan</td><td>33</td><td>34908.5</td><td>true</td><td>2022-04-15</td><td>12</td><td>2019-09-29T19:35:55.000Z</td></tr><tr><td>Niroop</td><td>35</td><td>49654.98</td><td>false</td><td>2021-02-18</td><td>3</td><td>2024-05-22T08:45:25.000Z</td></tr><tr><td>Pushpa</td><td>23</td><td>44111.99</td><td>true</td><td>2020-07-19</td><td>5</td><td>2023-09-25T20:00:00.000Z</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Albert",
         25,
         55000.5,
         true,
         "2025-08-25",
         6,
         "2025-08-25T15:30:00.000Z"
        ],
        [
         "Baskar",
         30,
         72000.0,
         false,
         "2024-12-30",
         7,
         "2024-12-31T08:45:15.000Z"
        ],
        [
         "Chetan",
         27,
         63000.75,
         true,
         "2023-01-15",
         8,
         "2023-01-15T20:00:00.000Z"
        ],
        [
         "Dravid",
         26,
         66000.5,
         true,
         "2024-06-20",
         9,
         "2024-05-22T15:40:55.000Z"
        ],
        [
         "Nishant",
         32,
         98700.0,
         false,
         "2023-10-21",
         10,
         "2020-10-31T06:45:45.000Z"
        ],
        [
         "David",
         29,
         34512.75,
         true,
         "2023-03-15",
         4,
         "2021-08-28T20:15:55.000Z"
        ],
        [
         "Mohan",
         33,
         34908.5,
         true,
         "2022-04-15",
         12,
         "2019-09-29T19:35:55.000Z"
        ],
        [
         "Niroop",
         35,
         49654.98,
         false,
         "2021-02-18",
         3,
         "2024-05-22T08:45:25.000Z"
        ],
        [
         "Pushpa",
         23,
         44111.99,
         true,
         "2020-07-19",
         5,
         "2023-09-25T20:00:00.000Z"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "is_active",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "join_date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "Exp",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "last_login",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply casting with correct date / timestamp format\n",
    "df_casted_tformat = cast_dataframe_columns_tformat(df_cust_format, col_type_map, \"MM-dd-yyyy'T'HH:mm:ss.SSSXXX\")\n",
    "display(df_casted_tformat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e571041c-03b2-4840-bdc5-95e877570f19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Scenario 03\n",
    "- date_format_map=**None**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ee78ca1-f81c-4f40-af2f-4c0a43afee91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def cast_dataframe_columns_map_wo(df, col_type_map, date_format_map=None):\n",
    "    for col_name, data_type in col_type_map.items():\n",
    "        data_type = data_type.lower()\n",
    "\n",
    "        if data_type == \"string\":\n",
    "            df = df.withColumn(col_name, F.col(col_name).cast(\"string\"))\n",
    "\n",
    "        elif data_type in [\"int\", \"Int\", \"integer\"]:\n",
    "            df = df.withColumn(col_name, F.col(col_name).cast(\"int\"))\n",
    "\n",
    "        elif data_type in [\"double\", \"float\"]:\n",
    "            df = df.withColumn(col_name, F.col(col_name).cast(\"double\"))\n",
    "\n",
    "        elif data_type == \"boolean\":\n",
    "            df = df.withColumn(col_name, F.col(col_name).cast(\"boolean\"))\n",
    "\n",
    "        elif data_type == \"date\":\n",
    "            df = df.withColumn(col_name, F.to_date(F.col(col_name)))\n",
    "\n",
    "        elif data_type == \"timestamp\":\n",
    "            df = df.withColumn(col_name, F.to_timestamp(F.col(col_name)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da1ba1f7-41ba-4a57-87ba-5a7b2e3e0a81",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"last_login\":197},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763796834863}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>age</th><th>salary</th><th>is_active</th><th>join_date</th><th>Exp</th><th>last_login</th></tr></thead><tbody><tr><td>Albert</td><td>25</td><td>55000.5</td><td>true</td><td>2025-08-25</td><td>6</td><td>2025-08-25T15:30:00.000Z</td></tr><tr><td>Baskar</td><td>30</td><td>72000.0</td><td>false</td><td>2024-12-31</td><td>7</td><td>2024-12-31T08:45:15.000Z</td></tr><tr><td>Chetan</td><td>27</td><td>63000.75</td><td>true</td><td>2023-01-15</td><td>8</td><td>2023-01-15T20:00:00.000Z</td></tr><tr><td>Dravid</td><td>26</td><td>66000.5</td><td>true</td><td>2024-06-20</td><td>9</td><td>2024-05-22T15:40:55.000Z</td></tr><tr><td>Nishant</td><td>32</td><td>98700.0</td><td>false</td><td>2023-10-21</td><td>10</td><td>2020-10-31T06:45:45.000Z</td></tr><tr><td>David</td><td>29</td><td>34512.75</td><td>true</td><td>2023-03-15</td><td>4</td><td>2021-08-28T20:15:55.000Z</td></tr><tr><td>Mohan</td><td>33</td><td>34908.5</td><td>true</td><td>2022-04-15</td><td>12</td><td>2019-09-29T19:35:55.000Z</td></tr><tr><td>Niroop</td><td>35</td><td>49654.98</td><td>false</td><td>2021-02-18</td><td>3</td><td>2024-05-22T08:45:25.000Z</td></tr><tr><td>Pushpa</td><td>23</td><td>44111.99</td><td>true</td><td>2020-07-19</td><td>5</td><td>2023-09-25T20:00:00.000Z</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Albert",
         25,
         55000.5,
         true,
         "2025-08-25",
         6,
         "2025-08-25T15:30:00.000Z"
        ],
        [
         "Baskar",
         30,
         72000.0,
         false,
         "2024-12-31",
         7,
         "2024-12-31T08:45:15.000Z"
        ],
        [
         "Chetan",
         27,
         63000.75,
         true,
         "2023-01-15",
         8,
         "2023-01-15T20:00:00.000Z"
        ],
        [
         "Dravid",
         26,
         66000.5,
         true,
         "2024-06-20",
         9,
         "2024-05-22T15:40:55.000Z"
        ],
        [
         "Nishant",
         32,
         98700.0,
         false,
         "2023-10-21",
         10,
         "2020-10-31T06:45:45.000Z"
        ],
        [
         "David",
         29,
         34512.75,
         true,
         "2023-03-15",
         4,
         "2021-08-28T20:15:55.000Z"
        ],
        [
         "Mohan",
         33,
         34908.5,
         true,
         "2022-04-15",
         12,
         "2019-09-29T19:35:55.000Z"
        ],
        [
         "Niroop",
         35,
         49654.98,
         false,
         "2021-02-18",
         3,
         "2024-05-22T08:45:25.000Z"
        ],
        [
         "Pushpa",
         23,
         44111.99,
         true,
         "2020-07-19",
         5,
         "2023-09-25T20:00:00.000Z"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "is_active",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "join_date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "Exp",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "last_login",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_casted_map_wo = cast_dataframe_columns_map_wo(df_default_format, col_type_map)\n",
    "display(df_casted_map_wo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea5b8114-af81-49c7-87b9-d907ecf9d6bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Scenario 04\n",
    "- date_format_map=**None**\n",
    "- date_format_map = **{\"date\": \"MM-dd-yyyy\", \"timestamp\": \"MM-dd-yyyy'T'HH:mm:ss.SSSXXX\"}**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94bbeeb1-b44f-491b-85df-5c30b9579930",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def cast_dataframe_columns_map(df, col_type_map, date_format_map=None):\n",
    "    for col_name, data_type in col_type_map.items():\n",
    "        data_type = data_type.lower()\n",
    "\n",
    "        if data_type == \"string\":\n",
    "            df = df.withColumn(col_name, F.col(col_name).cast(\"string\"))\n",
    "\n",
    "        elif data_type in [\"int\", \"Int\", \"integer\"]:\n",
    "            df = df.withColumn(col_name, F.col(col_name).cast(\"int\"))\n",
    "\n",
    "        elif data_type in [\"double\", \"float\"]:\n",
    "            df = df.withColumn(col_name, F.col(col_name).cast(\"double\"))\n",
    "\n",
    "        elif data_type == \"boolean\":\n",
    "            df = df.withColumn(col_name, F.col(col_name).cast(\"boolean\"))\n",
    "\n",
    "        elif data_type == \"date\":\n",
    "            df = df.withColumn(col_name, F.to_date(F.col(col_name), date_format_map.get(\"date\")))\n",
    "\n",
    "        elif data_type == \"timestamp\":\n",
    "            df = df.withColumn(col_name, F.to_timestamp(F.col(col_name), date_format_map.get(\"timestamp\")))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74d40dd1-01d7-4d05-86ae-45a88559be08",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"last_login\":202},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763797010094}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>age</th><th>salary</th><th>is_active</th><th>join_date</th><th>Exp</th><th>last_login</th></tr></thead><tbody><tr><td>Albert</td><td>25</td><td>55000.5</td><td>true</td><td>2025-08-25</td><td>6</td><td>2025-08-25T15:30:00.000Z</td></tr><tr><td>Baskar</td><td>30</td><td>72000.0</td><td>false</td><td>2024-12-30</td><td>7</td><td>2024-12-31T08:45:15.000Z</td></tr><tr><td>Chetan</td><td>27</td><td>63000.75</td><td>true</td><td>2023-01-15</td><td>8</td><td>2023-01-15T20:00:00.000Z</td></tr><tr><td>Dravid</td><td>26</td><td>66000.5</td><td>true</td><td>2024-06-20</td><td>9</td><td>2024-05-22T15:40:55.000Z</td></tr><tr><td>Nishant</td><td>32</td><td>98700.0</td><td>false</td><td>2023-10-21</td><td>10</td><td>2020-10-31T06:45:45.000Z</td></tr><tr><td>David</td><td>29</td><td>34512.75</td><td>true</td><td>2023-03-15</td><td>4</td><td>2021-08-28T20:15:55.000Z</td></tr><tr><td>Mohan</td><td>33</td><td>34908.5</td><td>true</td><td>2022-04-15</td><td>12</td><td>2019-09-29T19:35:55.000Z</td></tr><tr><td>Niroop</td><td>35</td><td>49654.98</td><td>false</td><td>2021-02-18</td><td>3</td><td>2024-05-22T08:45:25.000Z</td></tr><tr><td>Pushpa</td><td>23</td><td>44111.99</td><td>true</td><td>2020-07-19</td><td>5</td><td>2023-09-25T20:00:00.000Z</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Albert",
         25,
         55000.5,
         true,
         "2025-08-25",
         6,
         "2025-08-25T15:30:00.000Z"
        ],
        [
         "Baskar",
         30,
         72000.0,
         false,
         "2024-12-30",
         7,
         "2024-12-31T08:45:15.000Z"
        ],
        [
         "Chetan",
         27,
         63000.75,
         true,
         "2023-01-15",
         8,
         "2023-01-15T20:00:00.000Z"
        ],
        [
         "Dravid",
         26,
         66000.5,
         true,
         "2024-06-20",
         9,
         "2024-05-22T15:40:55.000Z"
        ],
        [
         "Nishant",
         32,
         98700.0,
         false,
         "2023-10-21",
         10,
         "2020-10-31T06:45:45.000Z"
        ],
        [
         "David",
         29,
         34512.75,
         true,
         "2023-03-15",
         4,
         "2021-08-28T20:15:55.000Z"
        ],
        [
         "Mohan",
         33,
         34908.5,
         true,
         "2022-04-15",
         12,
         "2019-09-29T19:35:55.000Z"
        ],
        [
         "Niroop",
         35,
         49654.98,
         false,
         "2021-02-18",
         3,
         "2024-05-22T08:45:25.000Z"
        ],
        [
         "Pushpa",
         23,
         44111.99,
         true,
         "2020-07-19",
         5,
         "2023-09-25T20:00:00.000Z"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "is_active",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "join_date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "Exp",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "last_login",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_casted_cfrmt = cast_dataframe_columns_map(\n",
    "    df_cust_format,\n",
    "    col_type_map,\n",
    "    {\"date\": \"MM-dd-yyyy\", \"timestamp\": \"MM-dd-yyyy'T'HH:mm:ss.SSSXXX\"}\n",
    ")\n",
    "display(df_casted_cfrmt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b19ec3b9-c217-4ce3-b8cd-2871697b4ae0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Scenario 05\n",
    "- date_format_map=**None**\n",
    "- date_format_map = **{\"my_date\": \"yyyy-MM-dd\", \"my_timestamp\": \"yyyy-MM-dd'T'HH:mm:ss.SSSXXX\"}**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fef96808-1f07-4bf0-9daa-f56d16e4e90d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def cast_dataframe_columns_dtformat(df, col_type_map, date_format_map=None):\n",
    "    for col_name, target_type in col_type_map.items():\n",
    "        target_type = target_type.lower()\n",
    "        fmt = None if date_format_map is None else date_format_map.get(col_name)\n",
    "\n",
    "        if target_type == \"string\":\n",
    "            df = df.withColumn(col_name, F.col(col_name).cast(\"string\"))\n",
    "\n",
    "        elif target_type in [\"int\", \"integer\"]:\n",
    "            df = df.withColumn(col_name, F.col(col_name).cast(\"int\"))\n",
    "\n",
    "        elif target_type in [\"double\", \"float\"]:\n",
    "            df = df.withColumn(col_name, F.col(col_name).cast(\"double\"))\n",
    "\n",
    "        elif target_type == \"boolean\":\n",
    "            df = df.withColumn(col_name, F.col(col_name).cast(\"boolean\"))\n",
    "\n",
    "        elif target_type == \"date\":\n",
    "            if fmt:\n",
    "                df = df.withColumn(col_name, F.to_date(F.col(col_name), fmt))\n",
    "            else:\n",
    "                df = df.withColumn(col_name, F.to_date(F.col(col_name)))\n",
    "\n",
    "        # For timestamp, apply both to_date and to_timestamp as new columns\n",
    "        if target_type == \"timestamp\":\n",
    "            if fmt:\n",
    "                df = df.withColumn(f\"{col_name}_date\", F.to_date(F.col(col_name), fmt))\n",
    "                df = df.withColumn(f\"{col_name}_ts\", F.to_timestamp(F.col(col_name), fmt))\n",
    "            else:\n",
    "                df = df.withColumn(f\"{col_name}_date\", F.to_date(F.col(col_name)))\n",
    "                df = df.withColumn(f\"{col_name}_ts\", F.to_timestamp(F.col(col_name)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de466fce-7cf1-43c8-93d3-347203ad0cbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "date_format_map = {\n",
    "    \"my_date\": \"yyyy-MM-dd\",\n",
    "    \"my_timestamp\": \"yyyy-MM-dd'T'HH:mm:ss.SSSXXX\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93f606b3-d403-4610-8004-d87307f48815",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"last_login\":203,\"last_login_ts\":210},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763798209530}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>age</th><th>salary</th><th>is_active</th><th>join_date</th><th>Exp</th><th>last_login</th><th>last_login_date</th><th>last_login_ts</th></tr></thead><tbody><tr><td>Albert</td><td>25</td><td>55000.5</td><td>true</td><td>2025-08-25</td><td>6</td><td>2025-08-25T15:30:00.000+00:00</td><td>2025-08-25</td><td>2025-08-25T15:30:00.000Z</td></tr><tr><td>Baskar</td><td>30</td><td>72000.0</td><td>false</td><td>2024-12-31</td><td>7</td><td>2024-12-31T08:45:15.000+00:00</td><td>2024-12-31</td><td>2024-12-31T08:45:15.000Z</td></tr><tr><td>Chetan</td><td>27</td><td>63000.75</td><td>true</td><td>2023-01-15</td><td>8</td><td>2023-01-15T20:00:00.000+00:00</td><td>2023-01-15</td><td>2023-01-15T20:00:00.000Z</td></tr><tr><td>Dravid</td><td>26</td><td>66000.5</td><td>true</td><td>2024-06-20</td><td>9</td><td>2024-05-22T15:40:55.000+00:00</td><td>2024-05-22</td><td>2024-05-22T15:40:55.000Z</td></tr><tr><td>Nishant</td><td>32</td><td>98700.0</td><td>false</td><td>2023-10-21</td><td>10</td><td>2020-10-31T06:45:45.000+00:00</td><td>2020-10-31</td><td>2020-10-31T06:45:45.000Z</td></tr><tr><td>David</td><td>29</td><td>34512.75</td><td>true</td><td>2023-03-15</td><td>4</td><td>2021-08-28T20:15:55.000+00:00</td><td>2021-08-28</td><td>2021-08-28T20:15:55.000Z</td></tr><tr><td>Mohan</td><td>33</td><td>34908.5</td><td>true</td><td>2022-04-15</td><td>12</td><td>2019-09-29T19:35:55.000+00:00</td><td>2019-09-29</td><td>2019-09-29T19:35:55.000Z</td></tr><tr><td>Niroop</td><td>35</td><td>49654.98</td><td>false</td><td>2021-02-18</td><td>3</td><td>2024-05-22T08:45:25.000+00:00</td><td>2024-05-22</td><td>2024-05-22T08:45:25.000Z</td></tr><tr><td>Pushpa</td><td>23</td><td>44111.99</td><td>true</td><td>2020-07-19</td><td>5</td><td>2023-09-25T20:00:00.000+00:00</td><td>2023-09-25</td><td>2023-09-25T20:00:00.000Z</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Albert",
         25,
         55000.5,
         true,
         "2025-08-25",
         6,
         "2025-08-25T15:30:00.000+00:00",
         "2025-08-25",
         "2025-08-25T15:30:00.000Z"
        ],
        [
         "Baskar",
         30,
         72000.0,
         false,
         "2024-12-31",
         7,
         "2024-12-31T08:45:15.000+00:00",
         "2024-12-31",
         "2024-12-31T08:45:15.000Z"
        ],
        [
         "Chetan",
         27,
         63000.75,
         true,
         "2023-01-15",
         8,
         "2023-01-15T20:00:00.000+00:00",
         "2023-01-15",
         "2023-01-15T20:00:00.000Z"
        ],
        [
         "Dravid",
         26,
         66000.5,
         true,
         "2024-06-20",
         9,
         "2024-05-22T15:40:55.000+00:00",
         "2024-05-22",
         "2024-05-22T15:40:55.000Z"
        ],
        [
         "Nishant",
         32,
         98700.0,
         false,
         "2023-10-21",
         10,
         "2020-10-31T06:45:45.000+00:00",
         "2020-10-31",
         "2020-10-31T06:45:45.000Z"
        ],
        [
         "David",
         29,
         34512.75,
         true,
         "2023-03-15",
         4,
         "2021-08-28T20:15:55.000+00:00",
         "2021-08-28",
         "2021-08-28T20:15:55.000Z"
        ],
        [
         "Mohan",
         33,
         34908.5,
         true,
         "2022-04-15",
         12,
         "2019-09-29T19:35:55.000+00:00",
         "2019-09-29",
         "2019-09-29T19:35:55.000Z"
        ],
        [
         "Niroop",
         35,
         49654.98,
         false,
         "2021-02-18",
         3,
         "2024-05-22T08:45:25.000+00:00",
         "2024-05-22",
         "2024-05-22T08:45:25.000Z"
        ],
        [
         "Pushpa",
         23,
         44111.99,
         true,
         "2020-07-19",
         5,
         "2023-09-25T20:00:00.000+00:00",
         "2023-09-25",
         "2023-09-25T20:00:00.000Z"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "is_active",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "join_date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "Exp",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "last_login",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "last_login_date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "last_login_ts",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply casting with correct timestamp format\n",
    "df_casted_dtformat = cast_dataframe_columns_dtformat(df_default_format, col_type_map, date_format_map)\n",
    "display(df_casted_dtformat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02cf55b8-db6c-45a7-9192-9b74542c5265",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### How Spark handles it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8c07520-2d87-4ea7-99ce-dcab23f9fbd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "If you call:\n",
    "\n",
    "        F.to_date(F.col(\"start_date\"))\n",
    "- Spark uses its **default parser (yyyy-MM-dd)**.\n",
    "\n",
    "If you call:\n",
    "\n",
    "      F.to_date(F.col(\"start_date\"), \"yyyy-MM-dd'T'HH:mm:ss.SSSXXX\")\n",
    "\n",
    "- Spark will use your **custom format**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c09bf06-bc9d-4848-b144-f8cc0ee6b08b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 2) Why set None?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d68e3a78-8043-4d30-95a3-ecd52e44e153",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- **Without =None**, the argument becomes **mandatory**.\n",
    "\n",
    "      def convert_dataframe_columns(df, col_type_map, date_format):\n",
    "          ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5d3f11b-91d9-4a95-a8a5-0723ae46fd00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Now you **must** always call:\n",
    "\n",
    "      convert_dataframe_columns(df, {\"start_date\": \"date\"}, \"yyyy-MM-dd'T'HH:mm:ss.SSSXXX\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "427e9133-99a4-49d6-95c0-438f6c9f9b2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- If you **dont** pass a **format**, Python will raise:\n",
    "\n",
    "      TypeError: missing 1 required positional argument: 'date_format'\n",
    "\n",
    "- By making it **None**, you can handle **two cases**:\n",
    "  - User **provided a format**  use it.\n",
    "  - User **didnt provide one**  either fall back to default Spark behavior or skip formatting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5540d525-177a-4890-ba0b-fc956bc9ccf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Scenario 06\n",
    "- date_format\n",
    "- date_format = **\"yyyy-MM-dd'T'HH:mm:ss.SSSXXX\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a8d0ab0-dcc7-428c-ad88-0f2a2295bb04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def cast_dataframe_columns_none(df, col_type_map, date_format):\n",
    "    for col_name, data_type in col_type_map.items():\n",
    "        data_type = data_type.lower()\n",
    "\n",
    "        if data_type == \"string\":\n",
    "            df = df.withColumn(col_name, F.col(col_name).cast(\"string\"))\n",
    "\n",
    "        elif data_type in [\"int\", \"integer\"]:\n",
    "            df = df.withColumn(col_name, F.col(col_name).cast(\"int\"))\n",
    "\n",
    "        elif data_type in [\"double\", \"float\"]:\n",
    "            df = df.withColumn(col_name, F.col(col_name).cast(\"double\"))\n",
    "\n",
    "        elif data_type == \"boolean\":\n",
    "            df = df.withColumn(col_name, F.col(col_name).cast(\"boolean\"))\n",
    "\n",
    "        elif data_type == \"date\":\n",
    "            df = df.withColumn(col_name, F.to_date(F.col(col_name)))\n",
    "\n",
    "        elif data_type == \"timestamp\":\n",
    "            df = df.withColumn(col_name, F.to_timestamp(F.col(col_name)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ae36479-6684-4829-8753-90b8b9179834",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2022302555744407>, line 2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Apply casting with correct timestamp format\u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m df_casted_none \u001B[38;5;241m=\u001B[39m cast_dataframe_columns_none(df_default_format, col_type_map)\n",
       "\u001B[1;32m      3\u001B[0m display(df_casted_none)\n",
       "\n",
       "\u001B[0;31mTypeError\u001B[0m: cast_dataframe_columns_none() missing 1 required positional argument: 'date_format'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "TypeError",
        "evalue": "cast_dataframe_columns_none() missing 1 required positional argument: 'date_format'"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>TypeError</span>: cast_dataframe_columns_none() missing 1 required positional argument: 'date_format'"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
        "File \u001B[0;32m<command-2022302555744407>, line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Apply casting with correct timestamp format\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m df_casted_none \u001B[38;5;241m=\u001B[39m cast_dataframe_columns_none(df_default_format, col_type_map)\n\u001B[1;32m      3\u001B[0m display(df_casted_none)\n",
        "\u001B[0;31mTypeError\u001B[0m: cast_dataframe_columns_none() missing 1 required positional argument: 'date_format'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply casting with correct timestamp format\n",
    "df_casted_none = cast_dataframe_columns_none(df_default_format, col_type_map)\n",
    "display(df_casted_none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5ab155e-7968-4875-81ca-ef81cf14a114",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"last_login\":198},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763798499367}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>age</th><th>salary</th><th>is_active</th><th>join_date</th><th>Exp</th><th>last_login</th></tr></thead><tbody><tr><td>Albert</td><td>25</td><td>55000.5</td><td>true</td><td>2025-08-25</td><td>6</td><td>2025-08-25T15:30:00.000Z</td></tr><tr><td>Baskar</td><td>30</td><td>72000.0</td><td>false</td><td>2024-12-31</td><td>7</td><td>2024-12-31T08:45:15.000Z</td></tr><tr><td>Chetan</td><td>27</td><td>63000.75</td><td>true</td><td>2023-01-15</td><td>8</td><td>2023-01-15T20:00:00.000Z</td></tr><tr><td>Dravid</td><td>26</td><td>66000.5</td><td>true</td><td>2024-06-20</td><td>9</td><td>2024-05-22T15:40:55.000Z</td></tr><tr><td>Nishant</td><td>32</td><td>98700.0</td><td>false</td><td>2023-10-21</td><td>10</td><td>2020-10-31T06:45:45.000Z</td></tr><tr><td>David</td><td>29</td><td>34512.75</td><td>true</td><td>2023-03-15</td><td>4</td><td>2021-08-28T20:15:55.000Z</td></tr><tr><td>Mohan</td><td>33</td><td>34908.5</td><td>true</td><td>2022-04-15</td><td>12</td><td>2019-09-29T19:35:55.000Z</td></tr><tr><td>Niroop</td><td>35</td><td>49654.98</td><td>false</td><td>2021-02-18</td><td>3</td><td>2024-05-22T08:45:25.000Z</td></tr><tr><td>Pushpa</td><td>23</td><td>44111.99</td><td>true</td><td>2020-07-19</td><td>5</td><td>2023-09-25T20:00:00.000Z</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Albert",
         25,
         55000.5,
         true,
         "2025-08-25",
         6,
         "2025-08-25T15:30:00.000Z"
        ],
        [
         "Baskar",
         30,
         72000.0,
         false,
         "2024-12-31",
         7,
         "2024-12-31T08:45:15.000Z"
        ],
        [
         "Chetan",
         27,
         63000.75,
         true,
         "2023-01-15",
         8,
         "2023-01-15T20:00:00.000Z"
        ],
        [
         "Dravid",
         26,
         66000.5,
         true,
         "2024-06-20",
         9,
         "2024-05-22T15:40:55.000Z"
        ],
        [
         "Nishant",
         32,
         98700.0,
         false,
         "2023-10-21",
         10,
         "2020-10-31T06:45:45.000Z"
        ],
        [
         "David",
         29,
         34512.75,
         true,
         "2023-03-15",
         4,
         "2021-08-28T20:15:55.000Z"
        ],
        [
         "Mohan",
         33,
         34908.5,
         true,
         "2022-04-15",
         12,
         "2019-09-29T19:35:55.000Z"
        ],
        [
         "Niroop",
         35,
         49654.98,
         false,
         "2021-02-18",
         3,
         "2024-05-22T08:45:25.000Z"
        ],
        [
         "Pushpa",
         23,
         44111.99,
         true,
         "2020-07-19",
         5,
         "2023-09-25T20:00:00.000Z"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "is_active",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "join_date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "Exp",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "last_login",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply casting with correct timestamp format\n",
    "df_casted_format_none = cast_dataframe_columns_none(df_default_format, col_type_map, \"yyyy-MM-dd'T'HH:mm:ss.SSSXXX\")\n",
    "display(df_casted_format_none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90d34352-0c22-493b-9dea-6d9d540abbac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def cast_dataframe_columns_none_cust(df, col_type_map, date_format_map):\n",
    "    for col_name, data_type in col_type_map.items():\n",
    "        data_type = data_type.lower()\n",
    "\n",
    "        if data_type == \"string\":\n",
    "            df = df.withColumn(col_name, F.col(col_name).cast(\"string\"))\n",
    "\n",
    "        elif data_type in [\"int\", \"integer\"]:\n",
    "            df = df.withColumn(col_name, F.col(col_name).cast(\"int\"))\n",
    "\n",
    "        elif data_type in [\"double\", \"float\"]:\n",
    "            df = df.withColumn(col_name, F.col(col_name).cast(\"double\"))\n",
    "\n",
    "        elif data_type == \"boolean\":\n",
    "            df = df.withColumn(col_name, F.col(col_name).cast(\"boolean\"))\n",
    "\n",
    "        elif data_type == \"date\":\n",
    "            df = df.withColumn(col_name, F.to_date(F.col(col_name), date_format_map.get(\"date\")))\n",
    "\n",
    "        elif data_type == \"timestamp\":\n",
    "            df = df.withColumn(col_name, F.to_timestamp(F.col(col_name), date_format_map.get(\"timestamp\")))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9069a030-1e8b-4122-a45f-e945e3b798d7",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"last_login\":206},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763798832910}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>age</th><th>salary</th><th>is_active</th><th>join_date</th><th>Exp</th><th>last_login</th></tr></thead><tbody><tr><td>Albert</td><td>25</td><td>55000.5</td><td>true</td><td>2025-08-25</td><td>6</td><td>2025-08-25T15:30:00.000Z</td></tr><tr><td>Baskar</td><td>30</td><td>72000.0</td><td>false</td><td>2024-12-30</td><td>7</td><td>2024-12-31T08:45:15.000Z</td></tr><tr><td>Chetan</td><td>27</td><td>63000.75</td><td>true</td><td>2023-01-15</td><td>8</td><td>2023-01-15T20:00:00.000Z</td></tr><tr><td>Dravid</td><td>26</td><td>66000.5</td><td>true</td><td>2024-06-20</td><td>9</td><td>2024-05-22T15:40:55.000Z</td></tr><tr><td>Nishant</td><td>32</td><td>98700.0</td><td>false</td><td>2023-10-21</td><td>10</td><td>2020-10-31T06:45:45.000Z</td></tr><tr><td>David</td><td>29</td><td>34512.75</td><td>true</td><td>2023-03-15</td><td>4</td><td>2021-08-28T20:15:55.000Z</td></tr><tr><td>Mohan</td><td>33</td><td>34908.5</td><td>true</td><td>2022-04-15</td><td>12</td><td>2019-09-29T19:35:55.000Z</td></tr><tr><td>Niroop</td><td>35</td><td>49654.98</td><td>false</td><td>2021-02-18</td><td>3</td><td>2024-05-22T08:45:25.000Z</td></tr><tr><td>Pushpa</td><td>23</td><td>44111.99</td><td>true</td><td>2020-07-19</td><td>5</td><td>2023-09-25T20:00:00.000Z</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Albert",
         25,
         55000.5,
         true,
         "2025-08-25",
         6,
         "2025-08-25T15:30:00.000Z"
        ],
        [
         "Baskar",
         30,
         72000.0,
         false,
         "2024-12-30",
         7,
         "2024-12-31T08:45:15.000Z"
        ],
        [
         "Chetan",
         27,
         63000.75,
         true,
         "2023-01-15",
         8,
         "2023-01-15T20:00:00.000Z"
        ],
        [
         "Dravid",
         26,
         66000.5,
         true,
         "2024-06-20",
         9,
         "2024-05-22T15:40:55.000Z"
        ],
        [
         "Nishant",
         32,
         98700.0,
         false,
         "2023-10-21",
         10,
         "2020-10-31T06:45:45.000Z"
        ],
        [
         "David",
         29,
         34512.75,
         true,
         "2023-03-15",
         4,
         "2021-08-28T20:15:55.000Z"
        ],
        [
         "Mohan",
         33,
         34908.5,
         true,
         "2022-04-15",
         12,
         "2019-09-29T19:35:55.000Z"
        ],
        [
         "Niroop",
         35,
         49654.98,
         false,
         "2021-02-18",
         3,
         "2024-05-22T08:45:25.000Z"
        ],
        [
         "Pushpa",
         23,
         44111.99,
         true,
         "2020-07-19",
         5,
         "2023-09-25T20:00:00.000Z"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "is_active",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "join_date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "Exp",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "last_login",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_casted_cfrmt_none = cast_dataframe_columns_none_cust(\n",
    "    df_cust_format,\n",
    "    col_type_map,\n",
    "    {\"date\": \"MM-dd-yyyy\", \"timestamp\": \"MM-dd-yyyy'T'HH:mm:ss.SSSXXX\"}\n",
    ")\n",
    "display(df_casted_cfrmt_none)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "23_why passing argument date_format=None in function",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}