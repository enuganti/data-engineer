{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df277574-4018-45a2-baa4-c5268c3685ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### NULLIF()\n",
    "- The **NULLIF()** function returns **NULL** if **two expressions are equal**, otherwise it returns the **first expression**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc73380b-8e3e-40c9-8152-c34102304c62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Syntax\n",
    "\n",
    "     NULLIF(expr1, expr2)\n",
    "\n",
    "     NULL if expr1 = expr2\n",
    "     expr1 if expr1 ≠ expr2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5804f002-4b23-46c7-a043-587b6b953e8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 1) Basic Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f552f07-b719-46a6-9b46-b86287858b00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>result1</th><th>result2</th><th>result3</th><th>result4</th><th>result5</th></tr></thead><tbody><tr><td>null</td><td>10</td><td>null</td><td>Hello</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         null,
         10,
         null,
         "Hello",
         null
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "result1",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "result2",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "result3",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "result4",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "result5",
            "nullable": true,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 2
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "result1",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "result2",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "result3",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "result4",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "result5",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "-- Compare two expressions\n",
    "SELECT NULLIF(10, 10) AS result1,                          -- returns NULL  (since 10 = 10)\n",
    "       NULLIF(10, 20) AS result2,                          -- returns 10    (since 10 ≠ 20)\n",
    "       NULLIF('Hello', 'Hello') AS result3,                -- returns NULL  (since Hello = Hello)\n",
    "       NULLIF('Hello', 'world') AS result4,                -- returns Hello (since Hello ≠ world)\n",
    "       NULLIF('2017-08-25', '2017-08-25') AS result5;      -- returns NULL  (since 2017-08-25 = 2017-08-25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39da73bb-6ac6-4c4a-a856-b35a7aa7811a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 2) Avoiding Divide by Zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8851b6f-cb6b-4678-9a7e-f14bd3075049",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>EmpID</th><th>Name</th><th>Salary</th><th>YearsWorked</th><th>department</th><th>bonus</th></tr></thead><tbody><tr><td>1</td><td>Albert</td><td>50000</td><td>5</td><td>HR</td><td>5</td></tr><tr><td>2</td><td>Bobby</td><td>60000</td><td>0</td><td>ADMIN</td><td>0</td></tr><tr><td>3</td><td>Charan</td><td>45000</td><td>9</td><td>FINACE</td><td>6</td></tr><tr><td>4</td><td>David</td><td>30000</td><td>null</td><td>HR</td><td>9</td></tr><tr><td>5</td><td>Eshwar</td><td>75000</td><td>15</td><td>ACCOUNTS</td><td>4</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Albert",
         50000,
         5,
         "HR",
         5
        ],
        [
         2,
         "Bobby",
         60000,
         0,
         "ADMIN",
         0
        ],
        [
         3,
         "Charan",
         45000,
         9,
         "FINACE",
         6
        ],
        [
         4,
         "David",
         30000,
         null,
         "HR",
         9
        ],
        [
         5,
         "Eshwar",
         75000,
         15,
         "ACCOUNTS",
         4
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "EmpID",
            "nullable": false,
            "type": "integer"
           },
           {
            "metadata": {
             "__CHAR_VARCHAR_TYPE_STRING": "varchar(50)"
            },
            "name": "Name",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "Salary",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "YearsWorked",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {
             "__CHAR_VARCHAR_TYPE_STRING": "varchar(10)"
            },
            "name": "department",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "bonus",
            "nullable": true,
            "type": "integer"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 33
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "EmpID",
         "type": "\"integer\""
        },
        {
         "metadata": "{\"__CHAR_VARCHAR_TYPE_STRING\": \"varchar(50)\"}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Salary",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "YearsWorked",
         "type": "\"integer\""
        },
        {
         "metadata": "{\"__CHAR_VARCHAR_TYPE_STRING\": \"varchar(10)\"}",
         "name": "department",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "bonus",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "DROP TABLE IF EXISTS tblEmployeesNullIF;\n",
    "CREATE TABLE tblEmployeesNullIF (\n",
    "    EmpID INT PRIMARY KEY,\n",
    "    Name VARCHAR(50),\n",
    "    Salary INT,\n",
    "    YearsWorked INT,\n",
    "    department VARCHAR(10),\n",
    "    bonus INT\n",
    ");\n",
    "\n",
    "INSERT INTO tblEmployeesNullIF (EmpID, Name, Salary, YearsWorked, department, bonus)\n",
    "VALUES\n",
    "(1, 'Albert', 50000, 5, 'HR', 5),     -- Normal case\n",
    "(2, 'Bobby', 60000, 0, 'ADMIN', 0),       -- Divide by zero avoided\n",
    "(3, 'Charan', 45000, 9, 'FINACE', 6),   -- Normal case\n",
    "(4, 'David', 30000, NULL, 'HR', 9),  -- NULL YearsWorked\n",
    "(5, 'Eshwar', 75000, 15, 'ACCOUNTS', 4);      -- Normal case\n",
    "\n",
    "SELECT * FROM tblEmployeesNullIF;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03e725e2-4207-447d-a62a-deb9465880c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mArithmeticException\u001B[0m                       Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-5511119600097141>, line 1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m get_ipython()\u001B[38;5;241m.\u001B[39mrun_cell_magic(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msql\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSELECT Salary / YearsWorked AS AvgPerYear\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mFROM tblEmployeesNullIF;\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2541\u001B[0m, in \u001B[0;36mInteractiveShell.run_cell_magic\u001B[0;34m(self, magic_name, line, cell)\u001B[0m\n",
       "\u001B[1;32m   2539\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuiltin_trap:\n",
       "\u001B[1;32m   2540\u001B[0m     args \u001B[38;5;241m=\u001B[39m (magic_arg_s, cell)\n",
       "\u001B[0;32m-> 2541\u001B[0m     result \u001B[38;5;241m=\u001B[39m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m   2543\u001B[0m \u001B[38;5;66;03m# The code below prevents the output from being displayed\u001B[39;00m\n",
       "\u001B[1;32m   2544\u001B[0m \u001B[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001B[39;00m\n",
       "\u001B[1;32m   2545\u001B[0m \u001B[38;5;66;03m# when the last Python token in the expression is a ';'.\u001B[39;00m\n",
       "\u001B[1;32m   2546\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(fn, magic\u001B[38;5;241m.\u001B[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001B[38;5;28;01mFalse\u001B[39;00m):\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:147\u001B[0m, in \u001B[0;36mSqlMagic.sql\u001B[0;34m(self, line, cell)\u001B[0m\n",
       "\u001B[1;32m    143\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\n",
       "\u001B[1;32m    144\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot run \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124mql command because spark connect initialization failed and no stacktrace was available.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    145\u001B[0m     )\n",
       "\u001B[1;32m    146\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muse_py4j:\n",
       "\u001B[0;32m--> 147\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexecute_via_py4j(cell)\n",
       "\u001B[1;32m    149\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexecute_via_sql_comm_handler(cell)\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:231\u001B[0m, in \u001B[0;36mSqlMagic.execute_via_py4j\u001B[0;34m(self, cell)\u001B[0m\n",
       "\u001B[1;32m    228\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    229\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdriver_activity_logger\u001B[38;5;241m.\u001B[39mlogExecuteCommandEvent(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSQL_MAGIC_PY4J_FAILED\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    230\u001B[0m                                                        json\u001B[38;5;241m.\u001B[39mdumps(get_debug_info(e)))\n",
       "\u001B[0;32m--> 231\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
       "\u001B[1;32m    232\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdriver_activity_logger\u001B[38;5;241m.\u001B[39mlogExecuteCommandEvent(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSQL_MAGIC_PY4J_SUCCEEDED\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m    233\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:227\u001B[0m, in \u001B[0;36mSqlMagic.execute_via_py4j\u001B[0;34m(self, cell)\u001B[0m\n",
       "\u001B[1;32m    225\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExecuting subquery: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquery_text\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, file\u001B[38;5;241m=\u001B[39msys\u001B[38;5;241m.\u001B[39m__stdout__)  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n",
       "\u001B[1;32m    226\u001B[0m         sql_directive \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mentry_point\u001B[38;5;241m.\u001B[39mgetSqlDirective(query_text)\n",
       "\u001B[0;32m--> 227\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandle_sql_directive(sql_directive, i \u001B[38;5;241m==\u001B[39m number_of_sub_queries \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m)\n",
       "\u001B[1;32m    228\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    229\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdriver_activity_logger\u001B[38;5;241m.\u001B[39mlogExecuteCommandEvent(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSQL_MAGIC_PY4J_FAILED\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    230\u001B[0m                                                        json\u001B[38;5;241m.\u001B[39mdumps(get_debug_info(e)))\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:305\u001B[0m, in \u001B[0;36mSqlMagic.handle_sql_directive\u001B[0;34m(self, sql_directive, is_last_query)\u001B[0m\n",
       "\u001B[1;32m    302\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m DriverException(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdirective_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is unsupported\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m    304\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_last_query \u001B[38;5;129;01mand\u001B[39;00m df \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(df\u001B[38;5;241m.\u001B[39mschema) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
       "\u001B[0;32m--> 305\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdisplay(df)\n",
       "\u001B[1;32m    306\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muser_ns[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_sqldf\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m df\n",
       "\u001B[1;32m    307\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_is_temp_view_creation_for_implicit_df_enabled:\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display.py:142\u001B[0m, in \u001B[0;36mDisplay.display\u001B[0;34m(self, input, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    140\u001B[0m \u001B[38;5;66;03m# This version is for Serverless + Spark Connect dogfooding.\u001B[39;00m\n",
       "\u001B[1;32m    141\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspark_connect_enabled \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28minput\u001B[39m, ConnectDataFrame):\n",
       "\u001B[0;32m--> 142\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdisplay_connect_table(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m    143\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28minput\u001B[39m, ConnectDataFrame):\n",
       "\u001B[1;32m    144\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39misStreaming:\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display.py:107\u001B[0m, in \u001B[0;36mDisplay.display_connect_table\u001B[0;34m(self, df, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    104\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcf_helper\u001B[38;5;241m.\u001B[39mdisplay_streaming_dataframe(df, config, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstreaming_listener,\n",
       "\u001B[1;32m    105\u001B[0m                                                \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m    106\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m--> 107\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcf_helper\u001B[38;5;241m.\u001B[39mdisplay_dataframe(df, config)\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display_helpers/cloudfetch_helper.py:72\u001B[0m, in \u001B[0;36mCloudFetchDisplayHelper.display_dataframe\u001B[0;34m(self, df, config)\u001B[0m\n",
       "\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdisplay_dataframe\u001B[39m(\u001B[38;5;28mself\u001B[39m, df: ConnectDataFrame, config: TableDisplayConfig):\n",
       "\u001B[0;32m---> 72\u001B[0m     display_payload \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwrite_to_cloudfetch(df, config)\n",
       "\u001B[1;32m     73\u001B[0m     ip_display({\n",
       "\u001B[1;32m     74\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mapplication/vnd.databricks.connect.display\u001B[39m\u001B[38;5;124m\"\u001B[39m: display_payload,\n",
       "\u001B[1;32m     75\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext/plain\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDatabricks Spark Connect Table\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m     76\u001B[0m     },\n",
       "\u001B[1;32m     77\u001B[0m                raw\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display_helpers/cloudfetch_helper.py:168\u001B[0m, in \u001B[0;36mCloudFetchDisplayHelper.write_to_cloudfetch\u001B[0;34m(self, connectDataFrame, config)\u001B[0m\n",
       "\u001B[1;32m    166\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
       "\u001B[1;32m    167\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m PySparkException \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[0;32m--> 168\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
       "\u001B[1;32m    169\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    170\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(\n",
       "\u001B[1;32m    171\u001B[0m         e\n",
       "\u001B[1;32m    172\u001B[0m     )(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEncountered an unexpected error when displaying table, please contact Databricks support\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    173\u001B[0m       ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display_helpers/cloudfetch_helper.py:132\u001B[0m, in \u001B[0;36mCloudFetchDisplayHelper.write_to_cloudfetch\u001B[0;34m(self, connectDataFrame, config)\u001B[0m\n",
       "\u001B[1;32m    128\u001B[0m \u001B[38;5;66;03m# Check if the new cloudfetch API is available on the dataframe\u001B[39;00m\n",
       "\u001B[1;32m    129\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m CloudFetchDisplayHelper\u001B[38;5;241m.\u001B[39muse_pyspark_cloudfetch_api \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\n",
       "\u001B[1;32m    130\u001B[0m         connectDataFrame, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_to_cloudfetch_with_limits_and_file_paths\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\u001B[1;32m    131\u001B[0m     results: Tuple[Optional[StructType], List[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCloudFetchResult\u001B[39m\u001B[38;5;124m\"\u001B[39m], List[\n",
       "\u001B[0;32m--> 132\u001B[0m         \u001B[38;5;28mbool\u001B[39m]] \u001B[38;5;241m=\u001B[39m connectDataFrame\u001B[38;5;241m.\u001B[39m_to_cloudfetch_with_limits_and_file_paths(\n",
       "\u001B[1;32m    133\u001B[0m             \u001B[38;5;28mformat\u001B[39m\u001B[38;5;241m=\u001B[39mstr_format,\n",
       "\u001B[1;32m    134\u001B[0m             compression\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n",
       "\u001B[1;32m    135\u001B[0m             row_limit\u001B[38;5;241m=\u001B[39mrowLimit,\n",
       "\u001B[1;32m    136\u001B[0m             byte_limit\u001B[38;5;241m=\u001B[39mbyteLimit)\n",
       "\u001B[1;32m    137\u001B[0m     pyspark_struct, cloudfetch_results, batch_truncation_results \u001B[38;5;241m=\u001B[39m results\n",
       "\u001B[1;32m    139\u001B[0m     schema \u001B[38;5;241m=\u001B[39m CloudFetchDisplayHelper\u001B[38;5;241m.\u001B[39mget_display_schema_from_pyspark_struct(\n",
       "\u001B[1;32m    140\u001B[0m         pyspark_struct)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/dataframe.py:1963\u001B[0m, in \u001B[0;36mDataFrame._to_cloudfetch_with_limits_and_file_paths\u001B[0;34m(self, format, compression, row_limit, byte_limit)\u001B[0m\n",
       "\u001B[1;32m   1942\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m   1943\u001B[0m \u001B[38;5;124;03mThis is an experimental method to support generating results in the form of pre-signed\u001B[39;00m\n",
       "\u001B[1;32m   1944\u001B[0m \u001B[38;5;124;03mURLs that can be used by the client to fetch the results directly from cloud storage.\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   1960\u001B[0m \n",
       "\u001B[1;32m   1961\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m   1962\u001B[0m query \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan\u001B[38;5;241m.\u001B[39mto_proto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient)\n",
       "\u001B[0;32m-> 1963\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mexperimental_to_cloudfetch(\n",
       "\u001B[1;32m   1964\u001B[0m     query, \u001B[38;5;28mformat\u001B[39m, compression, row_limit, byte_limit, \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m   1965\u001B[0m )\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1083\u001B[0m, in \u001B[0;36mSparkConnectClient.experimental_to_cloudfetch\u001B[0;34m(self, plan, format, compression, row_limit, byte_limit, with_internal_file_path)\u001B[0m\n",
       "\u001B[1;32m   1080\u001B[0m \u001B[38;5;66;03m# Execute the request and iterate over the streaming results. We capture the schema\u001B[39;00m\n",
       "\u001B[1;32m   1081\u001B[0m \u001B[38;5;66;03m# response and unpack the CloudResultBatch responses.\u001B[39;00m\n",
       "\u001B[1;32m   1082\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n",
       "\u001B[0;32m-> 1083\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(req, {}, [], progress\u001B[38;5;241m=\u001B[39mprogress):\n",
       "\u001B[1;32m   1084\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n",
       "\u001B[1;32m   1085\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1737\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n",
       "\u001B[1;32m   1735\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n",
       "\u001B[1;32m   1736\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n",
       "\u001B[0;32m-> 1737\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2053\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n",
       "\u001B[1;32m   2051\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m   2052\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n",
       "\u001B[0;32m-> 2053\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n",
       "\u001B[1;32m   2054\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, \u001B[38;5;167;01mValueError\u001B[39;00m):\n",
       "\u001B[1;32m   2055\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke RPC\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error):\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2132\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n",
       "\u001B[1;32m   2128\u001B[0m             d\u001B[38;5;241m.\u001B[39mUnpack(info)\n",
       "\u001B[1;32m   2130\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(status, info)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[0;32m-> 2132\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n",
       "\u001B[1;32m   2133\u001B[0m                 info,\n",
       "\u001B[1;32m   2134\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2135\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n",
       "\u001B[1;32m   2136\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n",
       "\u001B[1;32m   2137\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2139\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n",
       "\u001B[1;32m   2140\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2141\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mSparkConnectGrpcException\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[1;32m   2142\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2143\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\n",
       "\u001B[0;31mArithmeticException\u001B[0m: [DIVIDE_BY_ZERO] Division by zero. Use `try_divide` to tolerate divisor being 0 and return NULL instead. If necessary set \"spark.sql.ansi.enabled\" to \"false\" to bypass this error. SQLSTATE: 22012\n",
       "== SQL (line 1, position 8) ==\n",
       "SELECT Salary / YearsWorked AS AvgPerYear\n",
       "       ^^^^^^^^^^^^^^^^^^^^\n",
       "\n",
       "\n",
       "JVM stacktrace:\n",
       "org.apache.spark.SparkArithmeticException\n",
       "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.divideByZeroError(QueryExecutionErrors.scala:234)\n",
       "\tat com.databricks.photon.PhotonException$.getSparkException(PhotonException.scala:164)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$12(SQLExecution.scala:532)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:817)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:372)\n",
       "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:372)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:846)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:371)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:237)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:770)\n",
       "\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2701)\n",
       "\tat org.apache.spark.sql.classic.Dataset.collectToHybridCloudStoreResults(Dataset.scala:1780)\n",
       "\tat org.apache.spark.sql.connect.execution.SparkConnectPlanExecution.processAsRemoteBatches(SparkConnectPlanExecution.scala:490)\n",
       "\tat org.apache.spark.sql.connect.execution.SparkConnectPlanExecution.handlePlan(SparkConnectPlanExecution.scala:144)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handlePlan(ExecuteThreadRunner.scala:410)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:317)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:237)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:466)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:466)\n",
       "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:465)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:237)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:140)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:620)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
       "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n",
       "\tat scala.util.Using$.resource(Using.scala:296)\n",
       "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:620)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "ArithmeticException",
        "evalue": "[DIVIDE_BY_ZERO] Division by zero. Use `try_divide` to tolerate divisor being 0 and return NULL instead. If necessary set \"spark.sql.ansi.enabled\" to \"false\" to bypass this error. SQLSTATE: 22012\n== SQL (line 1, position 8) ==\nSELECT Salary / YearsWorked AS AvgPerYear\n       ^^^^^^^^^^^^^^^^^^^^\n\n\nJVM stacktrace:\norg.apache.spark.SparkArithmeticException\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.divideByZeroError(QueryExecutionErrors.scala:234)\n\tat com.databricks.photon.PhotonException$.getSparkException(PhotonException.scala:164)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$12(SQLExecution.scala:532)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:817)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:372)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:372)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:846)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:371)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:237)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:770)\n\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2701)\n\tat org.apache.spark.sql.classic.Dataset.collectToHybridCloudStoreResults(Dataset.scala:1780)\n\tat org.apache.spark.sql.connect.execution.SparkConnectPlanExecution.processAsRemoteBatches(SparkConnectPlanExecution.scala:490)\n\tat org.apache.spark.sql.connect.execution.SparkConnectPlanExecution.handlePlan(SparkConnectPlanExecution.scala:144)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handlePlan(ExecuteThreadRunner.scala:410)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:317)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:466)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:466)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:465)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:140)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:620)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:620)"
       },
       "metadata": {
        "errorSummary": "[DIVIDE_BY_ZERO] Division by zero. Use `try_divide` to tolerate divisor being 0 and return NULL instead. If necessary set \"spark.sql.ansi.enabled\" to \"false\" to bypass this error. SQLSTATE: 22012"
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "DIVIDE_BY_ZERO",
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "pysparkSummary": null,
        "sqlState": "22012",
        "stackTrace": "org.apache.spark.SparkArithmeticException\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.divideByZeroError(QueryExecutionErrors.scala:234)\n\tat com.databricks.photon.PhotonException$.getSparkException(PhotonException.scala:164)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$12(SQLExecution.scala:532)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:817)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:372)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:372)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:846)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:371)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:237)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:770)\n\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2701)\n\tat org.apache.spark.sql.classic.Dataset.collectToHybridCloudStoreResults(Dataset.scala:1780)\n\tat org.apache.spark.sql.connect.execution.SparkConnectPlanExecution.processAsRemoteBatches(SparkConnectPlanExecution.scala:490)\n\tat org.apache.spark.sql.connect.execution.SparkConnectPlanExecution.handlePlan(SparkConnectPlanExecution.scala:144)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handlePlan(ExecuteThreadRunner.scala:410)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:317)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:466)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:466)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:465)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:140)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:620)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:620)",
        "startIndex": 7,
        "stopIndex": 26
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mArithmeticException\u001B[0m                       Traceback (most recent call last)",
        "File \u001B[0;32m<command-5511119600097141>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m get_ipython()\u001B[38;5;241m.\u001B[39mrun_cell_magic(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msql\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSELECT Salary / YearsWorked AS AvgPerYear\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mFROM tblEmployeesNullIF;\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2541\u001B[0m, in \u001B[0;36mInteractiveShell.run_cell_magic\u001B[0;34m(self, magic_name, line, cell)\u001B[0m\n\u001B[1;32m   2539\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuiltin_trap:\n\u001B[1;32m   2540\u001B[0m     args \u001B[38;5;241m=\u001B[39m (magic_arg_s, cell)\n\u001B[0;32m-> 2541\u001B[0m     result \u001B[38;5;241m=\u001B[39m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   2543\u001B[0m \u001B[38;5;66;03m# The code below prevents the output from being displayed\u001B[39;00m\n\u001B[1;32m   2544\u001B[0m \u001B[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001B[39;00m\n\u001B[1;32m   2545\u001B[0m \u001B[38;5;66;03m# when the last Python token in the expression is a ';'.\u001B[39;00m\n\u001B[1;32m   2546\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(fn, magic\u001B[38;5;241m.\u001B[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001B[38;5;28;01mFalse\u001B[39;00m):\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:147\u001B[0m, in \u001B[0;36mSqlMagic.sql\u001B[0;34m(self, line, cell)\u001B[0m\n\u001B[1;32m    143\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\n\u001B[1;32m    144\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot run \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124mql command because spark connect initialization failed and no stacktrace was available.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    145\u001B[0m     )\n\u001B[1;32m    146\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muse_py4j:\n\u001B[0;32m--> 147\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexecute_via_py4j(cell)\n\u001B[1;32m    149\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexecute_via_sql_comm_handler(cell)\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:231\u001B[0m, in \u001B[0;36mSqlMagic.execute_via_py4j\u001B[0;34m(self, cell)\u001B[0m\n\u001B[1;32m    228\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    229\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdriver_activity_logger\u001B[38;5;241m.\u001B[39mlogExecuteCommandEvent(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSQL_MAGIC_PY4J_FAILED\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    230\u001B[0m                                                        json\u001B[38;5;241m.\u001B[39mdumps(get_debug_info(e)))\n\u001B[0;32m--> 231\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    232\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdriver_activity_logger\u001B[38;5;241m.\u001B[39mlogExecuteCommandEvent(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSQL_MAGIC_PY4J_SUCCEEDED\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    233\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:227\u001B[0m, in \u001B[0;36mSqlMagic.execute_via_py4j\u001B[0;34m(self, cell)\u001B[0m\n\u001B[1;32m    225\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExecuting subquery: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquery_text\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, file\u001B[38;5;241m=\u001B[39msys\u001B[38;5;241m.\u001B[39m__stdout__)  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[1;32m    226\u001B[0m         sql_directive \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mentry_point\u001B[38;5;241m.\u001B[39mgetSqlDirective(query_text)\n\u001B[0;32m--> 227\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandle_sql_directive(sql_directive, i \u001B[38;5;241m==\u001B[39m number_of_sub_queries \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    228\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    229\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdriver_activity_logger\u001B[38;5;241m.\u001B[39mlogExecuteCommandEvent(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSQL_MAGIC_PY4J_FAILED\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    230\u001B[0m                                                        json\u001B[38;5;241m.\u001B[39mdumps(get_debug_info(e)))\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:305\u001B[0m, in \u001B[0;36mSqlMagic.handle_sql_directive\u001B[0;34m(self, sql_directive, is_last_query)\u001B[0m\n\u001B[1;32m    302\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m DriverException(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdirective_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is unsupported\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    304\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_last_query \u001B[38;5;129;01mand\u001B[39;00m df \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(df\u001B[38;5;241m.\u001B[39mschema) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m--> 305\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdisplay(df)\n\u001B[1;32m    306\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muser_ns[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_sqldf\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m df\n\u001B[1;32m    307\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_is_temp_view_creation_for_implicit_df_enabled:\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display.py:142\u001B[0m, in \u001B[0;36mDisplay.display\u001B[0;34m(self, input, *args, **kwargs)\u001B[0m\n\u001B[1;32m    140\u001B[0m \u001B[38;5;66;03m# This version is for Serverless + Spark Connect dogfooding.\u001B[39;00m\n\u001B[1;32m    141\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspark_connect_enabled \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28minput\u001B[39m, ConnectDataFrame):\n\u001B[0;32m--> 142\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdisplay_connect_table(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    143\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28minput\u001B[39m, ConnectDataFrame):\n\u001B[1;32m    144\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39misStreaming:\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display.py:107\u001B[0m, in \u001B[0;36mDisplay.display_connect_table\u001B[0;34m(self, df, **kwargs)\u001B[0m\n\u001B[1;32m    104\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcf_helper\u001B[38;5;241m.\u001B[39mdisplay_streaming_dataframe(df, config, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstreaming_listener,\n\u001B[1;32m    105\u001B[0m                                                \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    106\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 107\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcf_helper\u001B[38;5;241m.\u001B[39mdisplay_dataframe(df, config)\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display_helpers/cloudfetch_helper.py:72\u001B[0m, in \u001B[0;36mCloudFetchDisplayHelper.display_dataframe\u001B[0;34m(self, df, config)\u001B[0m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdisplay_dataframe\u001B[39m(\u001B[38;5;28mself\u001B[39m, df: ConnectDataFrame, config: TableDisplayConfig):\n\u001B[0;32m---> 72\u001B[0m     display_payload \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwrite_to_cloudfetch(df, config)\n\u001B[1;32m     73\u001B[0m     ip_display({\n\u001B[1;32m     74\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mapplication/vnd.databricks.connect.display\u001B[39m\u001B[38;5;124m\"\u001B[39m: display_payload,\n\u001B[1;32m     75\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext/plain\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDatabricks Spark Connect Table\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     76\u001B[0m     },\n\u001B[1;32m     77\u001B[0m                raw\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display_helpers/cloudfetch_helper.py:168\u001B[0m, in \u001B[0;36mCloudFetchDisplayHelper.write_to_cloudfetch\u001B[0;34m(self, connectDataFrame, config)\u001B[0m\n\u001B[1;32m    166\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    167\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m PySparkException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m--> 168\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    169\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    170\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(\n\u001B[1;32m    171\u001B[0m         e\n\u001B[1;32m    172\u001B[0m     )(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEncountered an unexpected error when displaying table, please contact Databricks support\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    173\u001B[0m       ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display_helpers/cloudfetch_helper.py:132\u001B[0m, in \u001B[0;36mCloudFetchDisplayHelper.write_to_cloudfetch\u001B[0;34m(self, connectDataFrame, config)\u001B[0m\n\u001B[1;32m    128\u001B[0m \u001B[38;5;66;03m# Check if the new cloudfetch API is available on the dataframe\u001B[39;00m\n\u001B[1;32m    129\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m CloudFetchDisplayHelper\u001B[38;5;241m.\u001B[39muse_pyspark_cloudfetch_api \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\n\u001B[1;32m    130\u001B[0m         connectDataFrame, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_to_cloudfetch_with_limits_and_file_paths\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    131\u001B[0m     results: Tuple[Optional[StructType], List[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCloudFetchResult\u001B[39m\u001B[38;5;124m\"\u001B[39m], List[\n\u001B[0;32m--> 132\u001B[0m         \u001B[38;5;28mbool\u001B[39m]] \u001B[38;5;241m=\u001B[39m connectDataFrame\u001B[38;5;241m.\u001B[39m_to_cloudfetch_with_limits_and_file_paths(\n\u001B[1;32m    133\u001B[0m             \u001B[38;5;28mformat\u001B[39m\u001B[38;5;241m=\u001B[39mstr_format,\n\u001B[1;32m    134\u001B[0m             compression\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    135\u001B[0m             row_limit\u001B[38;5;241m=\u001B[39mrowLimit,\n\u001B[1;32m    136\u001B[0m             byte_limit\u001B[38;5;241m=\u001B[39mbyteLimit)\n\u001B[1;32m    137\u001B[0m     pyspark_struct, cloudfetch_results, batch_truncation_results \u001B[38;5;241m=\u001B[39m results\n\u001B[1;32m    139\u001B[0m     schema \u001B[38;5;241m=\u001B[39m CloudFetchDisplayHelper\u001B[38;5;241m.\u001B[39mget_display_schema_from_pyspark_struct(\n\u001B[1;32m    140\u001B[0m         pyspark_struct)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/dataframe.py:1963\u001B[0m, in \u001B[0;36mDataFrame._to_cloudfetch_with_limits_and_file_paths\u001B[0;34m(self, format, compression, row_limit, byte_limit)\u001B[0m\n\u001B[1;32m   1942\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1943\u001B[0m \u001B[38;5;124;03mThis is an experimental method to support generating results in the form of pre-signed\u001B[39;00m\n\u001B[1;32m   1944\u001B[0m \u001B[38;5;124;03mURLs that can be used by the client to fetch the results directly from cloud storage.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1960\u001B[0m \n\u001B[1;32m   1961\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1962\u001B[0m query \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan\u001B[38;5;241m.\u001B[39mto_proto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient)\n\u001B[0;32m-> 1963\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mexperimental_to_cloudfetch(\n\u001B[1;32m   1964\u001B[0m     query, \u001B[38;5;28mformat\u001B[39m, compression, row_limit, byte_limit, \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   1965\u001B[0m )\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1083\u001B[0m, in \u001B[0;36mSparkConnectClient.experimental_to_cloudfetch\u001B[0;34m(self, plan, format, compression, row_limit, byte_limit, with_internal_file_path)\u001B[0m\n\u001B[1;32m   1080\u001B[0m \u001B[38;5;66;03m# Execute the request and iterate over the streaming results. We capture the schema\u001B[39;00m\n\u001B[1;32m   1081\u001B[0m \u001B[38;5;66;03m# response and unpack the CloudResultBatch responses.\u001B[39;00m\n\u001B[1;32m   1082\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n\u001B[0;32m-> 1083\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(req, {}, [], progress\u001B[38;5;241m=\u001B[39mprogress):\n\u001B[1;32m   1084\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n\u001B[1;32m   1085\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1737\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n\u001B[1;32m   1735\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n\u001B[1;32m   1736\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m-> 1737\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2053\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n\u001B[1;32m   2051\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   2052\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n\u001B[0;32m-> 2053\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n\u001B[1;32m   2054\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, \u001B[38;5;167;01mValueError\u001B[39;00m):\n\u001B[1;32m   2055\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke RPC\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error):\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2132\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n\u001B[1;32m   2128\u001B[0m             d\u001B[38;5;241m.\u001B[39mUnpack(info)\n\u001B[1;32m   2130\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(status, info)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[0;32m-> 2132\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n\u001B[1;32m   2133\u001B[0m                 info,\n\u001B[1;32m   2134\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2135\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n\u001B[1;32m   2136\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n\u001B[1;32m   2137\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2139\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n\u001B[1;32m   2140\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2141\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mSparkConnectGrpcException\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[1;32m   2142\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2143\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
        "\u001B[0;31mArithmeticException\u001B[0m: [DIVIDE_BY_ZERO] Division by zero. Use `try_divide` to tolerate divisor being 0 and return NULL instead. If necessary set \"spark.sql.ansi.enabled\" to \"false\" to bypass this error. SQLSTATE: 22012\n== SQL (line 1, position 8) ==\nSELECT Salary / YearsWorked AS AvgPerYear\n       ^^^^^^^^^^^^^^^^^^^^\n\n\nJVM stacktrace:\norg.apache.spark.SparkArithmeticException\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.divideByZeroError(QueryExecutionErrors.scala:234)\n\tat com.databricks.photon.PhotonException$.getSparkException(PhotonException.scala:164)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$12(SQLExecution.scala:532)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:817)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:372)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:372)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:846)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:371)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:237)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:770)\n\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2701)\n\tat org.apache.spark.sql.classic.Dataset.collectToHybridCloudStoreResults(Dataset.scala:1780)\n\tat org.apache.spark.sql.connect.execution.SparkConnectPlanExecution.processAsRemoteBatches(SparkConnectPlanExecution.scala:490)\n\tat org.apache.spark.sql.connect.execution.SparkConnectPlanExecution.handlePlan(SparkConnectPlanExecution.scala:144)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handlePlan(ExecuteThreadRunner.scala:410)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:317)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:466)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:466)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:465)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:140)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:620)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:620)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT Salary / YearsWorked AS AvgPerYear\n",
    "FROM tblEmployeesNullIF;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10789c9f-9a18-42b7-ac09-1981aa6c99b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- If **YearsWorked = 0**, this causes an **error**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7638dfc7-f2aa-46a1-a3ff-0c4b923c313e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Salary</th><th>YearsWorked</th><th>AvgPerYear</th></tr></thead><tbody><tr><td>50000</td><td>5</td><td>10000.0</td></tr><tr><td>60000</td><td>0</td><td>null</td></tr><tr><td>45000</td><td>9</td><td>5000.0</td></tr><tr><td>30000</td><td>null</td><td>null</td></tr><tr><td>75000</td><td>15</td><td>5000.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         50000,
         5,
         10000.0
        ],
        [
         60000,
         0,
         null
        ],
        [
         45000,
         9,
         5000.0
        ],
        [
         30000,
         null,
         null
        ],
        [
         75000,
         15,
         5000.0
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "Salary",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "YearsWorked",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "AvgPerYear",
            "nullable": true,
            "type": "double"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 24
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Salary",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "YearsWorked",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "AvgPerYear",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT Salary, YearsWorked,\n",
    "       Salary / NULLIF(YearsWorked, 0) AS AvgPerYear\n",
    "FROM tblEmployeesNullIF;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4bb16a0-240a-4385-9688-71bef87a3148",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- If **YearsWorked = 0**, **NULLIF(YearsWorked, 0) = NULL**, so the **result = NULL instead of error**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2405e8e6-f7f5-4a89-9d3c-5e31127dc75f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 3) Removing Duplicate Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5aa3566-9ad2-4991-9d2b-250dba92d18f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>EmpID</th><th>Name</th><th>Department</th><th>DeptCheck</th></tr></thead><tbody><tr><td>1</td><td>Albert</td><td>HR</td><td>null</td></tr><tr><td>2</td><td>Bobby</td><td>ADMIN</td><td>ADMIN</td></tr><tr><td>3</td><td>Charan</td><td>FINACE</td><td>FINACE</td></tr><tr><td>4</td><td>David</td><td>HR</td><td>null</td></tr><tr><td>5</td><td>Eshwar</td><td>ACCOUNTS</td><td>ACCOUNTS</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Albert",
         "HR",
         null
        ],
        [
         2,
         "Bobby",
         "ADMIN",
         "ADMIN"
        ],
        [
         3,
         "Charan",
         "FINACE",
         "FINACE"
        ],
        [
         4,
         "David",
         "HR",
         null
        ],
        [
         5,
         "Eshwar",
         "ACCOUNTS",
         "ACCOUNTS"
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "EmpID",
            "nullable": false,
            "type": "integer"
           },
           {
            "metadata": {
             "__CHAR_VARCHAR_TYPE_STRING": "varchar(50)"
            },
            "name": "Name",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {
             "__CHAR_VARCHAR_TYPE_STRING": "varchar(10)"
            },
            "name": "Department",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "DeptCheck",
            "nullable": true,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 31
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "EmpID",
         "type": "\"integer\""
        },
        {
         "metadata": "{\"__CHAR_VARCHAR_TYPE_STRING\": \"varchar(50)\"}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{\"__CHAR_VARCHAR_TYPE_STRING\": \"varchar(10)\"}",
         "name": "Department",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DeptCheck",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT EmpID, Name, Department,\n",
    "       NULLIF(Department, 'HR') AS DeptCheck\n",
    "FROM tblEmployeesNullIF;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1aac7b4a-e156-4901-a174-61cde7faaf16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- If **Department = 'HR'**, returns **NULL**.\n",
    "- Otherwise, returns **actual Department**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78a4cdb8-a7e2-4a7c-a224-97262cb87517",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 4) Using in CASE Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db719b14-bc7d-434e-83b2-ba5aa69a3ae6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>EmpID</th><th>Salary</th><th>bonus</th><th>BonusStatus</th></tr></thead><tbody><tr><td>1</td><td>50000</td><td>5</td><td>Has Bonus</td></tr><tr><td>2</td><td>60000</td><td>0</td><td>No Bonus</td></tr><tr><td>3</td><td>45000</td><td>6</td><td>Has Bonus</td></tr><tr><td>4</td><td>30000</td><td>9</td><td>Has Bonus</td></tr><tr><td>5</td><td>75000</td><td>4</td><td>Has Bonus</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         50000,
         5,
         "Has Bonus"
        ],
        [
         2,
         60000,
         0,
         "No Bonus"
        ],
        [
         3,
         45000,
         6,
         "Has Bonus"
        ],
        [
         4,
         30000,
         9,
         "Has Bonus"
        ],
        [
         5,
         75000,
         4,
         "Has Bonus"
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "EmpID",
            "nullable": false,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "Salary",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "bonus",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "BonusStatus",
            "nullable": false,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 36
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "EmpID",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "Salary",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "bonus",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "BonusStatus",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT EmpID, Salary, bonus,\n",
    "       CASE \n",
    "            WHEN NULLIF(bonus, 0) IS NULL THEN 'No Bonus'\n",
    "            ELSE 'Has Bonus'\n",
    "       END AS BonusStatus\n",
    "FROM tblEmployeesNullIF;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2ce5b3b-5b06-4453-bb0b-f532d067fdc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- If Bonus = 0, **NULLIF(Bonus,0) = NULL** → \"No Bonus\".\n",
    "- Otherwise, **\"Has Bonus\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c29014e-48a7-40e4-80c1-fa21ac80b6e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 5) Comparing Two Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77e2f0f1-9d97-4bd5-b3aa-7f02b923646f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Salary</th><th>bonus</th><th>SalaryCheck</th></tr></thead><tbody><tr><td>50000</td><td>5</td><td>50000</td></tr><tr><td>60000</td><td>0</td><td>60000</td></tr><tr><td>45000</td><td>6</td><td>45000</td></tr><tr><td>30000</td><td>9</td><td>30000</td></tr><tr><td>75000</td><td>4</td><td>75000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         50000,
         5,
         50000
        ],
        [
         60000,
         0,
         60000
        ],
        [
         45000,
         6,
         45000
        ],
        [
         30000,
         9,
         30000
        ],
        [
         75000,
         4,
         75000
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "Salary",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "bonus",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "SalaryCheck",
            "nullable": true,
            "type": "integer"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 40
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Salary",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "bonus",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "SalaryCheck",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT Salary, bonus,\n",
    "       NULLIF(Salary, bonus) AS SalaryCheck\n",
    "FROM tblEmployeesNullIF;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22226b70-38a5-4f2a-b74c-3344c2537f28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- If **Salary = Bonus** → returns **NULL**.\n",
    "- Else → returns **Salary**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9754a8d7-2fb0-46eb-9191-4f44bb4ef325",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 6) Filtering Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f20bbefb-2522-4206-a4c4-dc2ed744bf72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Name</th><th>Department</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {
             "__CHAR_VARCHAR_TYPE_STRING": "varchar(50)"
            },
            "name": "Name",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {
             "__CHAR_VARCHAR_TYPE_STRING": "varchar(10)"
            },
            "name": "Department",
            "nullable": true,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 43
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"__CHAR_VARCHAR_TYPE_STRING\": \"varchar(50)\"}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{\"__CHAR_VARCHAR_TYPE_STRING\": \"varchar(10)\"}",
         "name": "Department",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT Name, Department\n",
    "FROM tblEmployeesNullIF\n",
    "WHERE NULLIF(Name, Department) IS NULL;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "623d4c4d-1337-4bcb-87c7-ad0edca2a303",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- This finds rows where **Name = Department**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f234e294-89fe-4c2e-b22b-9264efbb60ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 7) NULLIF() with COALESCE & AVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b712e0a9-f1bd-4407-9d05-e26d195c548c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>dept</th><th>current_year</th><th>previous_year</th></tr></thead><tbody><tr><td>1</td><td>100000.00</td><td>150000.00</td></tr><tr><td>5</td><td>300000.00</td><td>300000.00</td></tr><tr><td>3</td><td>0.00</td><td>100000.00</td></tr><tr><td>2</td><td>null</td><td>300000.00</td></tr><tr><td>4</td><td>null</td><td>150000.00</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "100000.00",
         "150000.00"
        ],
        [
         5,
         "300000.00",
         "300000.00"
        ],
        [
         3,
         "0.00",
         "100000.00"
        ],
        [
         2,
         null,
         "300000.00"
        ],
        [
         4,
         null,
         "150000.00"
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "dept",
            "nullable": true,
            "type": "byte"
           },
           {
            "metadata": {},
            "name": "current_year",
            "nullable": true,
            "type": "decimal(10,2)"
           },
           {
            "metadata": {},
            "name": "previous_year",
            "nullable": true,
            "type": "decimal(10,2)"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 12
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "dept",
         "type": "\"byte\""
        },
        {
         "metadata": "{}",
         "name": "current_year",
         "type": "\"decimal(10,2)\""
        },
        {
         "metadata": "{}",
         "name": "previous_year",
         "type": "\"decimal(10,2)\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "DROP TABLE IF EXISTS tblBudgetsNullIf;\n",
    "CREATE TABLE tblBudgetsNullIf (  \n",
    "   dept           TINYINT,  \n",
    "   current_year   DECIMAL(10,2),  \n",
    "   previous_year  DECIMAL(10,2)  \n",
    ");  \n",
    "  \n",
    "INSERT INTO tblBudgetsNullIf VALUES(1, 100000, 150000);  \n",
    "INSERT INTO tblBudgetsNullIf VALUES(2, NULL, 300000);  \n",
    "INSERT INTO tblBudgetsNullIf VALUES(3, 0, 100000);  \n",
    "INSERT INTO tblBudgetsNullIf VALUES(4, NULL, 150000);  \n",
    "INSERT INTO tblBudgetsNullIf VALUES(5, 300000, 300000);\n",
    "\n",
    "SELECT * FROM tblBudgetsNullIf;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e577253c-3141-4870-b3ba-a774a81ba31c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>current_year</th><th>previous_year</th><th>LastBudget</th></tr></thead><tbody><tr><td>100000.00</td><td>150000.00</td><td>100000.00</td></tr><tr><td>300000.00</td><td>300000.00</td><td>null</td></tr><tr><td>0.00</td><td>100000.00</td><td>0.00</td></tr><tr><td>null</td><td>300000.00</td><td>null</td></tr><tr><td>null</td><td>150000.00</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "100000.00",
         "150000.00",
         "100000.00"
        ],
        [
         "300000.00",
         "300000.00",
         null
        ],
        [
         "0.00",
         "100000.00",
         "0.00"
        ],
        [
         null,
         "300000.00",
         null
        ],
        [
         null,
         "150000.00",
         null
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "current_year",
            "nullable": true,
            "type": "decimal(10,2)"
           },
           {
            "metadata": {},
            "name": "previous_year",
            "nullable": true,
            "type": "decimal(10,2)"
           },
           {
            "metadata": {},
            "name": "LastBudget",
            "nullable": true,
            "type": "decimal(10,2)"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 26
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "current_year",
         "type": "\"decimal(10,2)\""
        },
        {
         "metadata": "{}",
         "name": "previous_year",
         "type": "\"decimal(10,2)\""
        },
        {
         "metadata": "{}",
         "name": "LastBudget",
         "type": "\"decimal(10,2)\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT current_year, previous_year,\n",
    "       NULLIF(current_year, previous_year) AS LastBudget  \n",
    "FROM tblBudgetsNullIf;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bb020a3-87f1-4ec4-8e9d-d61c33c45fbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>current_year</th><th>previous_year</th><th>Colsc_TotalBudget</th><th>NullIF_LastBudget</th></tr></thead><tbody><tr><td>100000.00</td><td>150000.00</td><td>100000.00</td><td>100000.00</td></tr><tr><td>300000.00</td><td>300000.00</td><td>300000.00</td><td>300000.00</td></tr><tr><td>0.00</td><td>100000.00</td><td>0.00</td><td>null</td></tr><tr><td>null</td><td>300000.00</td><td>300000.00</td><td>300000.00</td></tr><tr><td>null</td><td>150000.00</td><td>150000.00</td><td>150000.00</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "100000.00",
         "150000.00",
         "100000.00",
         "100000.00"
        ],
        [
         "300000.00",
         "300000.00",
         "300000.00",
         "300000.00"
        ],
        [
         "0.00",
         "100000.00",
         "0.00",
         null
        ],
        [
         null,
         "300000.00",
         "300000.00",
         "300000.00"
        ],
        [
         null,
         "150000.00",
         "150000.00",
         "150000.00"
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "current_year",
            "nullable": true,
            "type": "decimal(10,2)"
           },
           {
            "metadata": {},
            "name": "previous_year",
            "nullable": true,
            "type": "decimal(10,2)"
           },
           {
            "metadata": {},
            "name": "Colsc_TotalBudget",
            "nullable": true,
            "type": "decimal(10,2)"
           },
           {
            "metadata": {},
            "name": "NullIF_LastBudget",
            "nullable": true,
            "type": "decimal(10,2)"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 25
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "current_year",
         "type": "\"decimal(10,2)\""
        },
        {
         "metadata": "{}",
         "name": "previous_year",
         "type": "\"decimal(10,2)\""
        },
        {
         "metadata": "{}",
         "name": "Colsc_TotalBudget",
         "type": "\"decimal(10,2)\""
        },
        {
         "metadata": "{}",
         "name": "NullIF_LastBudget",
         "type": "\"decimal(10,2)\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT current_year, previous_year,\n",
    "       COALESCE(current_year, previous_year) AS Colsc_TotalBudget,\n",
    "       NULLIF(COALESCE(current_year, previous_year), 0.00) AS NullIF_LastBudget  \n",
    "FROM tblBudgetsNullIf;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7f5cd82-92b4-4874-90ed-03627d7c34ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Average_Budget</th></tr></thead><tbody><tr><td>212500.000000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "212500.000000"
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "Average_Budget",
            "nullable": true,
            "type": "decimal(14,6)"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 18
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Average_Budget",
         "type": "\"decimal(14,6)\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT AVG(NULLIF(COALESCE(current_year, previous_year), 0.00)) AS Average_Budget\n",
    "FROM tblBudgetsNullIf;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5511119600097155,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "21_NULLIF() | SQL NULL FUNCTIONS",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}