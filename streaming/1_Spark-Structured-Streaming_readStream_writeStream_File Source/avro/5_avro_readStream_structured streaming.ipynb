{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d0e59ff3-8643-4ce7-9be7-edcab38b4a47",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read\\\n",
    "          .format(\"avro\")\\\n",
    "          .load(\"/FileStore/tables/Streaming/Stream_readStream/avro/multiline_nested_avro.avro\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8db2a399-5033-4fde-8f7a-12e8f98f1b72",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Create folder in DBFS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e773ea1-833c-484f-a74d-0ad1d44dd8bf",
     "showTitle": true,
     "title": "Create Directories"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.mkdirs(\"/FileStore/tables/Streaming/Stream_checkpoint/csv\")\n",
    "dbutils.fs.mkdirs(\"/FileStore/tables/Streaming/Stream_checkpoint/json\")\n",
    "dbutils.fs.mkdirs(\"/FileStore/tables/Streaming/Stream_checkpoint/parquet\")\n",
    "dbutils.fs.mkdirs(\"/FileStore/tables/Streaming/Stream_checkpoint/orc\")\n",
    "dbutils.fs.mkdirs(\"/FileStore/tables/Streaming/Stream_checkpoint/avro\")\n",
    "\n",
    "dbutils.fs.mkdirs(\"/FileStore/tables/Streaming/Stream_readStream/csv/\")\n",
    "dbutils.fs.mkdirs(\"/FileStore/tables/Streaming/Stream_readStream/json/\")\n",
    "dbutils.fs.mkdirs(\"/FileStore/tables/Streaming/Stream_readStream/parquet/\")\n",
    "dbutils.fs.mkdirs(\"/FileStore/tables/Streaming/Stream_readStream/orc/\")\n",
    "dbutils.fs.mkdirs(\"/FileStore/tables/Streaming/Stream_readStream/avro/\")\n",
    "\n",
    "dbutils.fs.mkdirs(\"/FileStore/tables/Streaming/Stream_writeStream/csv/\")\n",
    "dbutils.fs.mkdirs(\"/FileStore/tables/Streaming/Stream_writeStream/json/\")\n",
    "dbutils.fs.mkdirs(\"/FileStore/tables/Streaming/Stream_writeStream/parquet/\")\n",
    "dbutils.fs.mkdirs(\"/FileStore/tables/Streaming/Stream_writeStream/orc/\")\n",
    "dbutils.fs.mkdirs(\"/FileStore/tables/Streaming/Stream_writeStream/avro/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0b2aae0-1709-497c-9f8e-d0be174cad50",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Delete folder in DBFS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4228e6b8-87aa-4c97-a53f-82ab2ad387d1",
     "showTitle": true,
     "title": "Remove Directories"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.rm(\"/FileStore/tables/Streaming/Stream_checkpoint/csv\", True)\n",
    "dbutils.fs.rm(\"/FileStore/tables/Streaming/Stream_checkpoint/json\", True)\n",
    "dbutils.fs.rm(\"/FileStore/tables/Streaming/Stream_checkpoint/parquet\", True)\n",
    "dbutils.fs.rm(\"/FileStore/tables/Streaming/Stream_checkpoint/orc\", True)\n",
    "dbutils.fs.rm(\"/FileStore/tables/Streaming/Stream_checkpoint/avro\", True)\n",
    "\n",
    "dbutils.fs.rm(\"/FileStore/tables/Streaming/Stream_readStream/csv\", True)\n",
    "dbutils.fs.rm(\"/FileStore/tables/Streaming/Stream_readStream/json\", True)\n",
    "dbutils.fs.rm(\"/FileStore/tables/Streaming/Stream_readStream/parquet\", True)\n",
    "dbutils.fs.rm(\"/FileStore/tables/Streaming/Stream_readStream/orc\", True)\n",
    "dbutils.fs.rm(\"/FileStore/tables/Streaming/Stream_readStream/avro\", True)\n",
    "\n",
    "dbutils.fs.rm(\"/FileStore/tables/Streaming/Stream_writeStream/csv\", True)\n",
    "dbutils.fs.rm(\"/FileStore/tables/Streaming/Stream_writeStream/json\", True)\n",
    "dbutils.fs.rm(\"/FileStore/tables/Streaming/Stream_writeStream/parquet\", True)\n",
    "dbutils.fs.rm(\"/FileStore/tables/Streaming/Stream_writeStream/orc\", True)\n",
    "dbutils.fs.rm(\"/FileStore/tables/Streaming/Stream_writeStream/avro\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3589528-7926-4a05-b7e3-b01ca6f03230",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Define schema for input JSON file**\n",
    "- schema must be specified when creating a streaming source dataframe, otherwise it will through error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86b1983c-e989-4fb3-a20e-e801ddf00dab",
     "showTitle": true,
     "title": "schema defined wrt JSON"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, IntegerType, StringType, LongType\n",
    "\n",
    "# Define the main schema including the nested structure\n",
    "schema_avro = StructType([StructField('source', StringType(), False),\n",
    "                          StructField('description', StringType(), False),\n",
    "                          StructField('input_timestamp', LongType(), False),\n",
    "                          StructField('last_update_timestamp', LongType(), False),\n",
    "                          StructField('country', StringType(), False),\n",
    "                          StructField('user', StringType(), False),\n",
    "                          StructField('Location', StringType(), False),\n",
    "                          StructField('Zipcode', StringType(), False)]\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb10bcdc-4a1f-4357-84d3-7b33424e2022",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Infer schema from a static DataFrame\n",
    "# static_df = spark.read\\\n",
    "#                  .format(\"avro\")\\\n",
    "#                  .load(\"/FileStore/tables/Streaming/Stream_readStream/avro/multiline_nested_avro.avro\")\n",
    "# schema_avro1 = static_df.schema\n",
    "# schema_avro1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e38d96d-36a4-4a74-9837-854ca66cad0e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### **1) readStream**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4f82a77-3c67-4f23-ad03-6de8bff39e69",
     "showTitle": true,
     "title": "Read JSON files from input folder"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\nroot\n |-- source: string (nullable = true)\n |-- description: string (nullable = true)\n |-- input_timestamp: long (nullable = true)\n |-- last_update_timestamp: long (nullable = true)\n |-- country: string (nullable = true)\n |-- user: string (nullable = true)\n |-- Location: string (nullable = true)\n |-- Zipcode: string (nullable = true)\n\nNone\n"
     ]
    }
   ],
   "source": [
    "stream_avro = spark.readStream\\\n",
    "                   .format(\"avro\")\\\n",
    "                   .schema(schema_avro)\\\n",
    "                   .load(\"/FileStore/tables/Streaming/Stream_readStream/avro/\")\n",
    "\n",
    "print(stream_avro.isStreaming)\n",
    "print(stream_avro.printSchema()) \n",
    "                        \n",
    "display(stream_avro)\n",
    "\n",
    "# stream_avro.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "caf24a47-9033-42fa-b409-cf22e17c5d6c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### **2) writeStream**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4fd23e89-e5e8-43a4-b389-bfe474320ec4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**format('parquet')**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acccffc3-27e1-45be-84cc-7651c83d710c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "check_point = \"/FileStore/tables/Streaming/Stream_checkpoint/avro\"\n",
    "\n",
    "stream_avro.writeStream\\\n",
    "           .format('parquet')\\\n",
    "           .outputMode('append')\\\n",
    "           .option(\"path\", \"/FileStore/tables/Streaming/Stream_writeStream/avro/\")\\\n",
    "           .option(\"checkpointLocation\", check_point)\\\n",
    "           .start()\n",
    "\n",
    "display(stream_avro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1979e11-37a0-4363-b52d-03d0a7ee6ede",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**verify the written stream data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e04dbd3-7ec8-44bb-b3a6-d2f594a18dad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.read.format(\"parquet\").load(\"/FileStore/tables/Streaming/Stream_writeStream/avro/*.parquet\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4046965238523516,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "5_avro_readStream_structured streaming",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
